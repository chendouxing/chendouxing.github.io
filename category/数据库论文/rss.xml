<?xml version="1.0"?>
<rss version="2.0">
    <channel>
        <title>蟹堡王海星 • Posts by &#34;数据库论文&#34; category</title>
        <link>https://chendouxing.github.io</link>
        <description>花有重开日，人无再少年</description>
        <language>zh-CN</language>
        <pubDate>Mon, 09 Oct 2023 14:26:24 +0800</pubDate>
        <lastBuildDate>Mon, 09 Oct 2023 14:26:24 +0800</lastBuildDate>
        <category>语言</category>
        <category>SQL</category>
        <category>分布式</category>
        <category>方法</category>
        <category>论文</category>
        <category>计算机等级考试</category>
        <category>数学</category>
        <category>课堂学习</category>
        <item>
            <guid isPermalink="true">https://chendouxing.github.io/2023/10/09/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%BA%E6%96%87/</guid>
            <title>分布式论文</title>
            <link>https://chendouxing.github.io/2023/10/09/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%BA%E6%96%87/</link>
            <category>分布式</category>
            <category>论文</category>
            <pubDate>Mon, 09 Oct 2023 14:26:24 +0800</pubDate>
            <description><![CDATA[ &lt;link rel=&#34;stylesheet&#34; class=&#34;aplayer-secondary-style-marker&#34; href=&#34;\assets\css\APlayer.min.css&#34;&gt;&lt;script src=&#34;\assets\js\APlayer.min.js&#34; class=&#34;aplayer-secondary-script-marker&#34;&gt;&lt;/script&gt;&lt;h1 id=&#34;the-google-file-system&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#the-google-file-system&#34;&gt;#&lt;/a&gt; 《The Google File System》&lt;/h1&gt;
&lt;p&gt;（谷歌文件系统 GFS）&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/33944479&#34;&gt;Google File System 论文详析 - 知乎 (zhihu.com)&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;简介&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#简介&#34;&gt;#&lt;/a&gt; 简介&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;组件故障是常态而不是例外。持续监控、错误检测、容错和自动恢复必须成为系统的组成部分。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;按照传统的分布式文件系统，文件的大小是难以管理的，必须重新考虑设计假设和参数，如 I/O 操作和块大小。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;共同设计应用程序和文件系统 API 通过提高灵活性使整个系统受益。&lt;strong&gt;引入原子追加操作，这样多个客户端就可以并发地向一个文件追加内容，而无需在它们之间进行额外的同步。&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GFS 具有快照和记录追加操作。快照以低成本创建文件或目录树的副本。&lt;strong&gt;记录追加允许多个客户端并发地将数据追加到同一文件，同时保证每个客户端追加的原子性。&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;假设&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#假设&#34;&gt;#&lt;/a&gt; 假设&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;该系统由许多经常失效的廉价商品组件构建而成。它必须不断地自我监控，并定期检测、容忍组件故障并迅速恢复。&lt;/li&gt;
&lt;li&gt;系统存储适量的大型文件。&lt;/li&gt;
&lt;li&gt;工作负载主要包括两种读取：&lt;strong&gt;大的流读取和小的随机读取&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;工作负载还具有许多将数据追加到文件的大型顺序写入。一旦写好，文件很少再修改。支持在文件中任意位置的小写入，不需要是高效的。&lt;/li&gt;
&lt;li&gt;系统为并发附加到同一文件的多个客户端实现良好定义的语义（多个用户写同一个文件）。每台机器运行一个生产者，将并发地附加到一个文件中。&lt;/li&gt;
&lt;li&gt;高持续带宽比低延迟更重要。大多数目标应用程序都非常重视以高速率批量处理数据，而很少有应用程序对单个读取或写入有严格的响应时间要求。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;体系架构&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#体系架构&#34;&gt;#&lt;/a&gt; 体系架构&lt;/h2&gt;
&lt;p&gt;一个 GFS 集群由一个 master 和多个 chunkserver 组成，并由多个客户端访问，如图 1 所示。其中的每一个通常都是运行用户级服务器进程的商用 Linux 机器。文件被分成固定大小的块。每个 chunk 由一个不可变的、全局唯一的 64 位 chunk handle（块句柄），该 handle 由 master 在 chunk 创建时分配。Chunkserver 将块存储在本地磁盘上作为 Linux 文件，并读取或写入由块句柄和字节范围指定的块数据。为了可靠性，每个块都在多个块服务器上复制。默认情况下，我们存储三个副本，尽管用户可以为文件命名空间的不同区域指定不同的复制级别。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/image-20231009153731477.png&#34; alt=&#34;image-20231009153731477&#34;&gt;&lt;/p&gt;
&lt;p&gt;主服务器维护文件系统所有的元数据，包括命名空间、访问控制信息、从文件到块的映射以及块的当前位置。它还控制系统范围的活动，如块租用管理、孤立块的垃圾收集以及块服务器之间的块迁移。主服务器定期在&lt;strong&gt;心跳消息&lt;/strong&gt;中与每个块服务器通信，以向其提供指令并收集其状态。&lt;/p&gt;
&lt;p&gt;链接到每个应用程序的 GFS 客户端代码实现文件系统 API，并与主服务器和块服务器通信。客户端与主服务器交互进行元数据操作，但所有数据承载通信都直接到块服务器。&lt;/p&gt;
&lt;p&gt;客户端和块服务器都不缓存文件数据。因为大多数应用程序都流经大量的文件，或者工作集太大而无法缓存。不使用它们可以消除缓存一致性问题，从而简简化客户端和整个系统。(客户端会缓存元数据)。块服务器不需要缓存文件数据，因为块存储在本地文件中，因此 LINUX 的 buﬀer 缓存已经将频繁访问的数据保存在内存中。&lt;/p&gt;
&lt;h3 id=&#34;简单阅读的交互&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#简单阅读的交互&#34;&gt;#&lt;/a&gt; 简单阅读的交互&lt;/h3&gt;
&lt;p&gt;首先，使用固定的块大小，客户端将应用程序指定的文件名和字节偏移量转换为文件中的块指数。然后，它向主服务器发送包含文件名和块索引的请求。主服务器用相应的块句柄和副本的位置进行回复，客户端使用文件名和块指数作为键来缓存这些信息。（主服务器告诉客户端目标文件在哪个块服务器）&lt;/p&gt;
&lt;p&gt;然后客户端向其中一个副本（在块服务器中）发送请求，很可能是最近的副本。该请求指定块句柄和该块内的字节范围，在缓存信息过期或文件重新打开之前，对同一块的进一步读取不需要更多的客户端 - 主服务器交互。事实上，客户端通常在同一请求中请求多个块，并且主服务器还可以包括紧接在所请求的那些块之后的块的信息，这些额外的信息避开了几个未来的客户端 - 主服务器交互。&lt;/p&gt;
&lt;h3 id=&#34;块大小&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#块大小&#34;&gt;#&lt;/a&gt; 块大小&lt;/h3&gt;
&lt;p&gt;每个区块副本都作为普通的 Linux 文件存储在块服务器上，并且仅在需要时进行扩展，惰性空间分配避免了由于内部碎片而浪费空间。&lt;u&gt;（惰性空间分配时，空间的物理分配会尽可能延迟，直到累积了块大小大小的数据。换句话说，在磁盘上分配新块之前的决策过程在很大程度上受到要写入的数据大小的影响，即 64 MB 块的未使用部分最小化。【尽可能地填满前面的块，才开始创建写入后面的块】）&lt;/u&gt;&lt;/p&gt;
&lt;p&gt;大块的优势：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;减少了客户端与主服务器交互的需求，因为对同一块的读写只需要向主服务器发出一个初始请求，以获取块的位置信息。这种减少对于我们的工作负载尤其重要，因为应用程序大多按顺序读取和写入大型文件。即使对于小的随机读取，客户端也可以轻松地缓存多 TB 工作集的所有块位置信息（原因与优势 3 一致，主要由于元数据会更小，更加便于存储到内存中。）&lt;/li&gt;
&lt;li&gt;由于在大块上，客户端更有可能对给定的块执行许多操作，因此可以通过在延长的时间段内保持与块服务器的持久网络连接来减少网络开销。&lt;/li&gt;
&lt;li&gt;减小了存储在主服务器上的元数据的大小，这允许我们将元数据保存在内存中，这又带来了其他优势。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;大块的缺点：&lt;/p&gt;
&lt;p&gt;（使用惰性空间分配）&lt;/p&gt;
&lt;p&gt;一个小文件可能由少量块组成，可能只有一个块。如果许多客户端正在访问同一个文件，存储这些块的块服务器可能会成为热点。在实践中，热点并不是一个主要问题，因为我们的应用程序大多顺序读取大型多块文件。（热点是常见问题）&lt;/p&gt;
&lt;p&gt;当 GFS 首次被批处理队列系统使用时，热点确实出现了：一个可执行文件作为一个单一的块文件被写入 GFS，然后同时在数百台机器上启动，此时存储可执行文件的少数块服务器被数百个同时请求超载。&lt;strong&gt;解决方法：&lt;strong&gt;我们通过使用更高的复制因子存储这样的可执行文件，并通过使批处理队列系统错开应用程序的启动时间来解决这个问题。一个潜在的&lt;/strong&gt;长期解决方案&lt;/strong&gt;是允许客户端在这种情况下从其他客户端读取数据。&lt;/p&gt;
&lt;h3 id=&#34;元数据&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#元数据&#34;&gt;#&lt;/a&gt; 元数据&lt;/h3&gt;
&lt;p&gt;主服务器存储三种主要类型的元数据：&lt;strong&gt;文件和块命名空间&lt;/strong&gt;、&lt;strong&gt;从文件到块的映射&lt;/strong&gt;以及&lt;strong&gt;每个块的副本的位置&lt;/strong&gt;。所有元数据都保存在主存储器中。前两种类型（命名空间和文件到块的映射）通过将增量记录到存储在主机本地磁盘上的操作日志中并在远程机器上复制来保持持久性，使用日志使得我们可以简单、可靠地更新主服务器状态，并且在主服务器崩溃时不会有不一致的风险。&lt;strong&gt;主服务器不永久存储块位置信息&lt;/strong&gt;，&lt;strong&gt;相反，它会在主服务器启动时以及每当块服务器加入集群时询问每个块服务器的块位置信息&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&#34;一致性模型&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#一致性模型&#34;&gt;#&lt;/a&gt; 一致性模型&lt;/h3&gt;
&lt;h2 id=&#34;系统交互&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#系统交互&#34;&gt;#&lt;/a&gt; 系统交互&lt;/h2&gt;
&lt;p&gt;（描述客户端、主服务器和块服务器如何交互来实现数据变化、原子记录附加和快照。）&lt;/p&gt;
&lt;h3 id=&#34;租约和修改顺序&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#租约和修改顺序&#34;&gt;#&lt;/a&gt; 租约和修改顺序&lt;/h3&gt;
&lt;p&gt;变化是一种改变块的内容或元数据的操作，如写入或追加操作。&lt;strong&gt;每个变化在所有块的副本处执行&lt;/strong&gt;，使用租约来保持其一致的变化顺序。主节点向其中一个副本节点授予块租约，我们称之为原始节点（primary）。原始节点为块的所有变化选择一个连续的顺序。应用变化时，所有副本都遵循此顺序。因此，全局变化顺序首先由主服务器（master）选择的租约授予顺序来定义，并且在租约内由原始节点分配的序列号来定义。&lt;/p&gt;
&lt;p&gt;租约机制旨在最小化主服务器处的管理开销。只要块发生变化，原始节点就可以无限期地请求并通常从主服务器接收扩展。这些扩展请求和授予被捎带在主服务器和所有块服务器之间定期交换的心跳消息上。主服务器有时可能试图在租约到期之前撤销租约（例如，当主服务器想要禁用正在被重命名的文件上的变化时）。即使主服务器失去了与原始节点的通信，它也可以在旧租约到期后安全地将新租约授予另一个副本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;写的控制流程&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;客户端询问主服务器哪个块服务器持有块的当前租约以及其他副本的位置。如果没有块服务器具有租约，则主服务器将租约授予它选择的副本。&lt;/li&gt;
&lt;li&gt;主服务器回复原始节点的标识和其他（辅助）副本的位置。客户端缓存该数据以用于将来的变化。只有当原始节点变得不可访问或答复它不再持有租约时，它才需要再次与主服务器联系。&lt;/li&gt;
&lt;li&gt;客户端将数据推送到所有副本，客户端可以按任何顺序执行此操作。每个块服务器将数据存储在内部 LRU 缓冲区缓存中，直到数据被使用或失效。通过将数据流与控制流解耦，我们可以基于网络拓扑来调度昂贵的数据流来提高性能，而不管哪个块服务器是主要的（primary）。&lt;/li&gt;
&lt;li&gt;一旦所有副本都确认收到数据，客户端就向原始节点发送写请求。该请求标识先前推送到所有副本的数据。原始节点将连续的序列号分配给它可能从多个客户端接收的所有变化，这提供了必要的序列化。它以序列号顺序将变化应用到它自己的本地状态。&lt;/li&gt;
&lt;li&gt;原始节点将写入请求转发到所有辅助副本。每个辅助副本以由原始节点分配的相同序列号顺序应用增量。&lt;/li&gt;
&lt;li&gt;副本都回复原始节点，表示它们已经完成了操作。&lt;/li&gt;
&lt;li&gt;原始节点回复客户端。在任何副本中遇到的任何错误都将报告给客户端。如果出现错误，则可能在原始节点和辅助副本的任意子集上成功写入 (如果它在原始节点上发生故障，则不会为其分配序列号并转发），客户端请求被认为失败，并且修改的区域保持不一致状态。客户端代码通过重试失败的变化来处理此类错误。它将在步骤（3）到（7）中尝试几次，然后从写开始退回重试。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;/images/image-20231011181559699.png&#34; alt=&#34;image-20231011181559699&#34;&gt;&lt;/p&gt;
&lt;p&gt;如果应用程序的写入操作很大或跨越块边界，GFS 客户端代码将其分解为多个写入操作。它们都遵循上述控制流程，但可能与来自其他客户端的并发操作交织并被其覆盖。因此，共享文件区域可能最终包含来自不同客户端的片段，尽管副本将是相同的，因为各个操作在所有副本上以相同的顺序成功完成，这使文件区域处于一致但未定义的状态。&lt;/p&gt;
&lt;h3 id=&#34;记录追加&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#记录追加&#34;&gt;#&lt;/a&gt; 记录追加&lt;/h3&gt;
&lt;h3 id=&#34;快照&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#快照&#34;&gt;#&lt;/a&gt; 快照&lt;/h3&gt;
&lt;p&gt;使用标准的写时复制技术来实现快照。当主服务器接收到快照请求时，它首先撤销它要快照的文件中块的任何未完成的租约。这确保了对这些块的任何后续写入都将需要与主设备进行交互以找到租用保持器。在租约被撤销或到期后，主服务器将操作记录到磁盘。然后，它通过复制源文件或目录树的元数据，将此日志记录应用于其内存中状态。新创建的快照文件指向与源文件相同的块。&lt;/p&gt;
&lt;p&gt;在快照操作之后，当客户端第一次想要写入块 C 时，它会向主机发送一个请求，以查找当前的租户。主程序注意到块 C 的引用计数大于 1。它推迟对客户端请求的回复，而是选择新的组块句柄 C‘。然后，它要求每个拥有 C 的当前副本的块服务器创建一个名为 C‘的新块。通过在与原始数据块相同的数据块服务器上创建新数据块，我们确保可以在本地复制数据，而不是通过网络。从这一点来看，请求处理与任何块的请求处理没有什么不同：主服务器向其中一个副本授予新块 C‘的租约，并回复客户端，客户端可以正常写入块，而不知道它是从现有块创建的。&lt;/p&gt;
&lt;h2 id=&#34;主服务器操作&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#主服务器操作&#34;&gt;#&lt;/a&gt; 主服务器操作&lt;/h2&gt;
&lt;h3 id=&#34;命名空间管理和锁定&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#命名空间管理和锁定&#34;&gt;#&lt;/a&gt; 命名空间管理和锁定&lt;/h3&gt;
&lt;p&gt;许多主操作可能需要很长时间：例如，快照操作必须撤销快照覆盖的所有区块上的块服务器租约。我们不希望在其他主操作运行时延迟它们。因此，我们允许多个操作处于活动状态，并在命名空间的区域上使用锁来确保正确的序列化。&lt;/p&gt;
&lt;p&gt;GFS 在逻辑上将其命名空间表示为一个查找表，该表将完整路径名映射到元数据。通过前缀压缩，可以在内存中有效地表示该表。命名空间树中的每个节点（绝对文件名或绝对目录名）都有一个关联的读写锁。&lt;strong&gt;每个主操作在运行之前都会获得一组锁&lt;/strong&gt;。通常，如果它涉及 /d1/d2/…/dn/leaf，它将获取目录名 /d1，/d1/d2，…，/d1/d2/…/dn，以及完整路径名 /d1/d2/…/ 上的读锁或写锁。&lt;/p&gt;
&lt;p&gt;这种锁定模式的一个很好的特性是，它允许在同一目录中进行并发变化。例如，多个文件创建可以在同一个目录中同时执行：每个文件创建都在目录名上获得一个读锁定，在文件名上获得一个写锁定。目录名上的读锁定足以防止目录被删除、重命名或快照。对文件名序列化的写锁尝试创建具有相同名称的文件两次。&lt;/p&gt;
&lt;p&gt;由于命名空间可以有许多节点，读写锁对象被延迟分配，一旦不使用就被删除。此外，锁是以一致的总顺序获取的，以防止死锁：它们首先在名称空间树中按级别排序，并在同一级别内按字典顺序排序。&lt;/p&gt;
&lt;h3 id=&#34;副本管理&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#副本管理&#34;&gt;#&lt;/a&gt; 副本管理&lt;/h3&gt;
&lt;p&gt;当 master 创建一个 chunk 时，它会选择在哪里放置最初为空的副本。它考虑了几个因素。(1) 希望将新副本放置在磁盘空间利用率低于平均水平的 chunkserver 上。随着时间的推移，这将均衡块服务器之间的磁盘利用率。(2) 希望限制每个 chunkserver 上 “最近” 创建的数量。虽然创建本身很便宜，但它可靠地预测了即将到来的繁重的写入流量，因为块是在写入需要时创建的，并且在我们的 append-once-read-many 工作负载中，一旦它们被完全写入，它们通常就会变成只读。(3) 如上所述，希望跨机架分布块副本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;增加块副本的优先级&lt;/strong&gt;：一旦可用副本的数量低于用户指定的目标数量，主服务器就会重新复制这些块。这种情况可能有多种原因：chunkserver 变得不可用，它报告其副本可能已损坏，其中一个磁盘因错误而被禁用，或者复制目标增加。每个需要重新复制的块都基于几个因素进行优先级排序。一个是它离复制目标有多远。例如，我们给予一个丢失了两个副本的块比一个只丢失了一个副本的块更高的优先级。此外，我们更倾向于首先重新复制活动文件的块。最后，为了最小化故障对正在运行的应用程序的影响，我们提高了任何阻碍客户端进程的块的优先级。&lt;/p&gt;
&lt;p&gt;主服务器选择最高优先级的块，并通过指示一些块服务器直接从现有的有效副本复制块数据来 “克隆” 它。新副本的目标与创建副本的目标类似：均衡磁盘空间利用率，限制任何单个 chunkserver 上的活动克隆操作，以及将副本分布在机架上。为了防止克隆流量压倒客户端流量，主服务器限制集群和每个 chunkserver 的活动克隆操作数量。此外，每个 chunkserver 通过限制其对源 chunkserver 的读取请求来限制其在每个克隆操作上花费的带宽量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;块副本负载均衡&lt;/strong&gt;：最后，主服务器定期重新平衡副本：它检查当前副本的分布，并移动副本以获得更好的磁盘空间和负载平衡。同样在这个过程中，主服务器会逐渐填满一个新的 chunkserver，而不是立即用新的 chunk 和随之而来的繁重的写流量填满它。新副本的放置标准与上面讨论的类似。此外，主服务器还必须选择要删除的现有副本。一般来说，它倾向于删除那些低于平均可用空间的块服务器，以均衡磁盘空间使用。&lt;/p&gt;
&lt;h3 id=&#34;回收存储空间&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#回收存储空间&#34;&gt;#&lt;/a&gt; 回收存储空间&lt;/h3&gt;
&lt;p&gt;删除文件后，&lt;strong&gt;GFS 不会立即回收可用的物理存储&lt;/strong&gt;。它只在文件和块级别的常规垃圾收集期间才执行回收。这种方法使系统更简单可靠。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;当应用程序删除文件时，主服务器会像记录其他更改一样立即记录删除。但是，不是立即回收资源，而是将文件重命名为包含删除时间戳的隐藏名称&lt;/strong&gt;。在主服务器定期扫描文件系统命名空间时，如果任何此类隐藏文件的存在时间超过三天 (间隔是可配置的)，它会将其删除（自：为了实现高效恢复）。&lt;strong&gt;在此之前，该文件仍然可以使用新的特殊名称读取，并且可以通过将其重命名为 Normal 来恢复&lt;/strong&gt;。当隐藏文件从命名空间中移除时，其内存中元数据将被擦除，这实际上切断了它与所有数据块的连接。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;块服务器删除块&lt;/strong&gt;：在块命名空间的类似的常规扫描中，主服务器标识孤立块（即，那些不能从任何文件到达的块）并擦除那些块的元数据。在与主服务器定期交换的心跳消息中，每个块服务器报告它所拥有的块的子集，并且主服务器将被删除元数据的所有块的标识发送到相应的块服务器，块服务器可以自由删除这些块的副本。&lt;/p&gt;
&lt;p&gt;存储回收的垃圾收集方法的优点。首先，它在组件故障常见的大规模分布式系统中简单可靠。创建块可能会在某些块服务器上成功，但在其他块服务器上却失败，留下了主服务器不知道存在的副本。数据块删除消息可能会丢失，主服务器必须记住在故障时重新发送它们，包括它自己的和块服务器的。垃圾收集提供了一种统一且可靠的方法来清理任何不知道是否有用的副本。其次，它将存储回收合并到主服务器的常规后台活动中，例如命名空间的常规扫描和与块服务器的交互。因此，该过程是分批完成的，成本是摊销的。而且，只有在主服务器相对自由的时候才这样做，这样主服务器可以更快速响应需要及时关注的客户端请求。第三，回收存储的延迟提供了一个防止意外的、不可逆的删除的安全网（使删除可以及时恢复）。&lt;/p&gt;
&lt;h3 id=&#34;陈旧副本检测&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#陈旧副本检测&#34;&gt;#&lt;/a&gt; 陈旧副本检测&lt;/h3&gt;
&lt;p&gt;如果一个块服务器故障失效了，并且在它故障的时候错过了对块副本的修改，那么这些块副本就可能会变得陈旧。对于每个块，主服务器维护一个块版本号，以区分最新的副本和过时的副本。&lt;/p&gt;
&lt;p&gt;每当主服务器在块上授予新的租约时，它会增加块版本号并通知最新副本。主服务器和这些副本服务器都在其持久状态下记录新版本号。这发生在任何客户端被通知之前，因此在它可以开始写入块之前，如果另一个副本当前不可用，则其块版本号将不会被提升。当块服务器重新启动时，主服务器将检测到这个块服务器有一个陈旧的副本，并报告它的一组块及其相关的版本号。如果主服务器看到的版本号大于该副本记录中的版本号，则主服务器会假定它在授予租约时失败，因此会采用最新版本的副本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主服务器在常规垃圾收集中删除过时的副本。在此之前，当它回复客户端对区块信息的请求时，它实际上会认为过时的副本根本不存在&lt;/strong&gt;。作为另一种保护措施，当主服务器通知客户端哪个块服务器持有块的租约时，或者当它指示块服务器在克隆操作中从另一个块服务器读取块时，主服务器包括块版本号。客户端或块服务器在执行操作时验证版本号，以便始终访问最新数据。&lt;/p&gt;
&lt;h2 id=&#34;容错与诊断&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#容错与诊断&#34;&gt;#&lt;/a&gt; 容错与诊断&lt;/h2&gt;
&lt;p&gt;设计系统时最大的挑战之一是处理频繁的组件故障。组件的质量和数量一起使这些问题变得更常见，而不是例外：我们不能完全信任机器，也不能完全信任磁盘。组件故障可能导致系统不可用，或者更糟糕的是，数据损坏。我们讨论如何应对这些挑战，以及我们在系统中内置的工具，以便在问题发生时对其进行诊断。&lt;/p&gt;
&lt;h3 id=&#34;高可用性&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#高可用性&#34;&gt;#&lt;/a&gt; 高可用性&lt;/h3&gt;
&lt;p&gt;在 GFS 集群中的数百台服务器中，有些服务器在任何给定时间都是不可用的。通过两个简单而有效的策略保持整个系统的高度可用性：&lt;strong&gt;快速恢复和复制&lt;/strong&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;快速恢复&lt;/p&gt;
&lt;p&gt;无论它们如何终止，主服务器和块服务器都被设计为恢复其状态并在几秒钟内启动。事实上，我们不区分正常和异常终止；服务器通常只是通过终止进程来关闭。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;块复制&lt;/p&gt;
&lt;p&gt;如前所述，每个块在不同机架上的多个块服务器上复制。用户可以为文件命名空间的不同部分指定不同的复制级别。主服务器根据需要克隆现有的副本，以保持每个块完全复制。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;主服务器复制&lt;/p&gt;
&lt;p&gt;复制主服务器状态以提高可靠性。它的操作日志和检查点在多台机器上复制。只有在日志记录被刷新到磁盘本地和所有主服务器副本上之后，状态的变化才被认为是已提交的。为了简单起见，一个主进程仍然负责所有的变化以及后台活动，如在内部改变系统的垃圾收集。当它失败时，它几乎可以立即重新启动。如果其计算机或磁盘出现故障，GFS 外部的监视基础结构将在其他位置启动一个新的主进程，并使用复制的操作日志。&lt;/p&gt;
&lt;p&gt;此外，即使在主服务器关闭时，“影子” 主机也提供对文件系统的只读访问。它们是影子，而不是镜子，因为它们可能会稍微滞后于主光，通常是几分之一秒。它们增强了未被主动改变的文件或不介意获得稍微陈旧结果的应用程序的读取可用性。事实上，由于文件内容是从 chunkserver 读取的，应用程序不会观察到陈旧的文件内容。&lt;/p&gt;
&lt;p&gt;为了使自己保持知情，影子主服务器读取不断增长的操作日志的副本，并将与主服务器完全相同的更改序列应用于其数据结构。与主服务器一样，它在启动时（此后很少）轮流交互 chunkserver 以定位 chunkreplicas，并与它们频繁交换心跳消息以监视它们的状态。它仅在由主服务器决定创建和删除副本，而导致的副本位置更新方面依赖于主服务器。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;数据完整性&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#数据完整性&#34;&gt;#&lt;/a&gt; 数据完整性&lt;/h3&gt;
&lt;p&gt;每个块服务器使用校验和来检测存储数据的损坏。由于 GFS 集群通常在数百台机器上有数千个磁盘，因此它经常会遇到磁盘故障，导致读取和写入路径上的数据损坏或丢失。我们可以使用其他块副本从损坏中恢复，但是通过比较块服务器上的副本来检测损坏是不切实际的。此外，不同的副本可能是独立的：GFS 变化的语义，特别是前面讨论的原子记录附加，不保证相同的副本。因此，每个块服务器都必须通过维护校验和来独立验证自己副本的完整性。&lt;/p&gt;
&lt;p&gt;每个块都有一个对应的 32 位校验和。与其他元数据一样，校验和保存在内存中，并与日志记录一起持久存储，与用户数据分开。&lt;/p&gt;
&lt;p&gt;对于读取，块服务器在将任何数据返回给请求者（无论是客户端还是另一个块服务器）之前，验证与读取范围重叠的数据块的校验和。因此，块服务器不会将损坏发送到其他服务器或客户端。如果块与记录的校验和不匹配，则块服务器向请求者返回错误，并向主服务器报告不匹配。作为响应，请求者将从其他副本读取，而主服务器将从另一个副本克隆块。在一个有效的新副本就位之后，主服务器指示报告不匹配的块服务器删除它的副本。&lt;/p&gt;
&lt;p&gt;校验和计算针对附加到块末尾的写入（而不是覆盖现有数据的写入）进行了大量优化，因为它们在我们的工作负载中占主导地位。我们只是递增更新最后一个部分校验和，并为任何由增加填充的全新校验和块计算新的校验和。即使最后一个部分校验和块已经损坏，我们现在无法检测到它，新的校验和值也不会与存储的数据匹配，并且在下一次读取块时会像往常一样检测到损坏。&lt;/p&gt;
&lt;p&gt;相反，如果写操作覆盖了块的现有范围，我们必须读取并验证被覆盖范围的第一个和最后一个块，然后执行写操作，最后计算并记录新的校验和。如果我们在部分覆盖前没有验证第一个和最后一个块，新的校验和可能会隐藏未被覆盖的区域中存在的损坏。&lt;/p&gt;
&lt;p&gt;在空闲期间，chunkserver 可以扫描和验证非活动块的内容，这使我们能够检测很少被读取的块中的损坏。一旦检测到损坏，主设备就可以创建新的未损坏的副本并删除损坏的副本。这可以防止一个不活动但已损坏的块副本欺骗主服务器，使其认为它有该块的有效副本。&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;bigtable-a-distributed-storage-system-for-structured-data&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#bigtable-a-distributed-storage-system-for-structured-data&#34;&gt;#&lt;/a&gt; 《Bigtable: A Distributed Storage System for Structured Data》&lt;/h1&gt;
&lt;p&gt;BigTable 支持客户端寻找存储中数据的位置属性，同时，允许客户端动态控制是从内存还是从磁盘选择数据。&lt;/p&gt;
&lt;h2 id=&#34;数据模型&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#数据模型&#34;&gt;#&lt;/a&gt; 数据模型&lt;/h2&gt;
&lt;p&gt;数据被组织成三个维度：行、列和时间戳，&lt;strong&gt;(row：string，column：string，time：int64) → string&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;将行键、列键和时间戳引用的存储称为单元格。将行分组在一起以形成负载平衡单元，将列分组在一起以形成访问控制和资源核算单元。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;行键&lt;/p&gt;
&lt;p&gt;Bigtable 按照行键的字典顺序维护数据。表中的行键是任意字符串。在单个行键下的每次数据读写都是可序列化的（由版本号控制），它使客户端在存在对同一行的并发更新时更容易推断系统的行为。换句话说，&lt;strong&gt;行是 Bigtable 中事务一致性的单位&lt;/strong&gt;，&lt;strong&gt;Bigtable 目前不支持跨行事务&lt;/strong&gt;。（自：只能对单表进行并发更新）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;存储方面&lt;/strong&gt;：具有连续键的磁盘被分组到小块（tablet）中，小块形成分布和负载平衡的单元。因此，短行范围的读取更高效，并且通常仅需要与少量机器通信（减少网络开销）。客户端可以通过选择行键来利用此属性，以便为数据访问获得&lt;strong&gt;良好的局部性&lt;/strong&gt;。例如，在 Webtable 中，通过反转 URL 的主机名组件，同一域中的页面被分组到连续的行中。&lt;a href=&#34;http://xn--maps-965fpjo21fs7k.google.com/index.html%E7%9A%84%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E5%9C%A8com.google.maps/index.html%E8%A1%8C%E4%B8%8B%E9%9D%A2%EF%BC%8C%E5%B0%86%E6%9D%A5%E8%87%AA%E5%90%8C%E4%B8%80%E4%B8%AA%E5%9F%9F%E7%9A%84%E9%A1%B5%E9%9D%A2%E5%AD%98%E5%82%A8%E5%9C%A8%E5%BD%BC%E6%AD%A4%E9%99%84%E8%BF%91%EF%BC%8C%E8%BF%99%E5%8F%AF%E4%BB%A5%E4%BD%BF%E6%9F%90%E4%BA%9B%E4%B8%BB%E6%9C%BA%E5%92%8C%E5%9F%9F%E5%88%86%E6%9E%90%E6%9B%B4%E6%9C%89%E6%95%88%E3%80%82&#34;&gt;我们会将 maps.google.com/index.html 的数据存储在 com.google.maps/index.html 行下面，将来自同一个域的页面存储在彼此附近，这可以使某些主机和域分析更有效。&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/image-20231115140811457.png&#34; alt=&#34;image-20231115140811457&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;列键&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;列键被分组到称为列族的集合中，这些集合构成访问控制的单元&lt;/strong&gt;。存储在列族中的所有数据通常都是相同类型的。创建一个族之后，可以使用族中的任何列键：数据可以存储在这样的列键下，而不会影响表的模式。表中不同列族的数量要小（最多几百个），并且族在操作过程中通常很少改变，这种限制使广泛共享的元数据不会太大（修改的增量和频率小，相关元数据也小）。相反，表可以具有无限数量的列。&lt;/p&gt;
&lt;p&gt;可以通过更改表的模式来删除整个列族，在这种情况下，存储在该族中任何列键下的数据都将被删除。然而，由于 Bigtable 不支持跨多行的事务，因此如果存储在特定列键下的数据驻留在多行中，则无法原子地删除该数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;列键使用以下语法命名：family：qualifier&lt;/strong&gt;（自：主域名：子域名）。family 名称必须是可打印的，但 qualifier 可以是任意字符串。Webtable 的一个示例列族，它存储编写网页时使用的语言。我们只使用一个带有空限定符（empty qualifier）的列键来存储每个网页的 ID（自：第一列列名为空，内容为网页地址 ID）。此表的另一个有用的列族是锚，此族中的每个列键表示单个锚，如图 1 所示。&lt;strong&gt;限定符是引用站点的名称，单元格包含与链接关联的文本&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;时间戳&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;表中的不同单元格可以包含同一数据的多个版本，其中版本按时间戳索引&lt;/strong&gt;。Bigtable 时间戳是 64 位整数。它们可以由 Bigtable 隐式分配，在这种情况下，它们表示以微秒为单位的 “真实的时间”，也可以由客户端应用程序显式分配。需要避免冲突的应用程序必须自己生成唯一的时间戳。&lt;strong&gt;单元格的不同版本按时间戳降序存储&lt;/strong&gt;，&lt;strong&gt;以便可以首先读取最新版本&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;为了使版本数据的管理不那么繁重，Bigtable 支持两个每列族设置，告诉 Bigtable 自动收集版本数据垃圾。客户端可以指定只保留数据的最后 n 个版本，或者只保留足够新的版本。&lt;/p&gt;
&lt;p&gt;在我们的 Webtable 示例中，我们可以将存储在 contents：列中的抓取页面的时间戳设置为实际抓取这些页面版本的时间。上面描述的垃圾收集机制使我们能够告诉 Bigtable 只保留每个页面的最新三个版本。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;api&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#api&#34;&gt;#&lt;/a&gt; API&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Bigtable 支持单行事务，可用于对存储在单行键下的数据执行原子读 - 修改 - 写序列。Bigtable 目前不支持跨行键的一般事务，尽管它提供了一个接口，用于在客户端跨行键进行重复写入。&lt;/li&gt;
&lt;li&gt;Bigtable 允许单元格用作整数计数器。&lt;/li&gt;
&lt;li&gt;Bigtable 支持在服务器的地址空间中执行客户端提供的脚本。这些脚本是用一种名为 Sawzall 的语言编写的，该语言是由 Google 开发的，用于处理数据。目前，我们基于 Sawzall 的 API 不允许客户端脚本写回 Bigtable，但它允许各种形式的数据转换，基于任意表达式的过滤，以及通过各种运算符的汇总。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;bigtable基础设施&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#bigtable基础设施&#34;&gt;#&lt;/a&gt; Bigtable 基础设施&lt;/h2&gt;
&lt;h3 id=&#34;gfs&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#gfs&#34;&gt;#&lt;/a&gt; GFS&lt;/h3&gt;
&lt;p&gt;Bigtable 使用 GFS 存储日志和数据文件。GFS 是一个分布式文件系统，它维护每个文件的多个副本，以提高可靠性和可用性。&lt;/p&gt;
&lt;h3 id=&#34;sstable&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#sstable&#34;&gt;#&lt;/a&gt; SSTable&lt;/h3&gt;
&lt;p&gt;Google SSTable 不可变文件格式在内部用于存储 Bigtable 数据文件。SSTable 提供了一个持久的、有序的、不可变的从键到值的映射，其中键和值都是任意字节字符串。&lt;strong&gt;SSTable 提供的操作用于查找与指定键关联的值&lt;/strong&gt;，&lt;strong&gt;以及遍历指定键范围内的所有键 / 值对&lt;/strong&gt;。在内部，每个 SSTable 都包含一个块序列（默认情况下，每个块的大小为 64 KB，但大小是可配置的）。块索引（存储在 SSTable 的末尾）用于定位块；当 SSTable 打开时，索引被加载到内存中。查找可以通过单个磁盘寻道来执行：我们首先&lt;strong&gt;通过在内存索引中执行二进制搜索来找到适当的块，然后从磁盘读取适当的块&lt;/strong&gt;。SSTable 可以完全映射到内存中，这允许我们在不接触磁盘的情况下执行查找和扫描。&lt;/p&gt;
&lt;h3 id=&#34;chubby&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#chubby&#34;&gt;#&lt;/a&gt; Chubby&lt;/h3&gt;
&lt;p&gt;Bigtable 依赖于一个高可用性和持久性的分布式锁服务，称为 Chubby 。一个 Chubby 服务由五个活动副本组成，其中一个被选为主服务器并主动服务请求。当大多数副本正在运行并且可以相互通信时，服务是活动的。Chubby 使用 Paxos 算法来保持其副本在面对失败时的一致性。Chubby 提供了一个由目录和小文件组成的命名空间。每个目录或文件都可以用作锁，对文件的读取和写入都是原子的。Chubby 客户端库提供 Chubby 文件的一致缓存。每一个 Chubby 客户端都维护一个与 Chubby 服务的会话。如果客户端无法在租约到期时间内续订其会话租约，则该客户端的会话将到期。当客户端的会话过期时，它将丢失所有锁和打开的句柄。Chubby 客户端还可以在 Chubby 文件和目录上注册回调，以通知更改或会话过期。&lt;/p&gt;
&lt;p&gt;Bigtable 使用 Chubby 来完成各种任务：确保任何时候最多有一个活动的主服务器；存储 Bigtable 数据的引导位置；发现 tablet 服务器并确定 tablet 服务器的死亡，以及存储 Bigtable 模式。如果 Chubby 长时间不可用，Bigtable 也将不可用。&lt;/p&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">https://chendouxing.github.io/2023/09/06/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/</guid>
            <title>分布式理论</title>
            <link>https://chendouxing.github.io/2023/09/06/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/</link>
            <category>分布式</category>
            <category>论文</category>
            <pubDate>Wed, 06 Sep 2023 14:38:56 +0800</pubDate>
            <description><![CDATA[ &lt;link rel=&#34;stylesheet&#34; class=&#34;aplayer-secondary-style-marker&#34; href=&#34;\assets\css\APlayer.min.css&#34;&gt;&lt;script src=&#34;\assets\js\APlayer.min.js&#34; class=&#34;aplayer-secondary-script-marker&#34;&gt;&lt;/script&gt;&lt;h1 id=&#34;分布式理论&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#分布式理论&#34;&gt;#&lt;/a&gt; 分布式理论&lt;/h1&gt;
&lt;h2 id=&#34;cap理论&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#cap理论&#34;&gt;#&lt;/a&gt; CAP 理论&lt;/h2&gt;
&lt;h3 id=&#34;理论概述&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#理论概述&#34;&gt;#&lt;/a&gt; 理论概述&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;一个分布式系统最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）这三项中的两项&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&#34;一致性&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#一致性&#34;&gt;#&lt;/a&gt; 一致性&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;对于一致性，可以分为从客户端和服务端两个不同的视角。&lt;/p&gt;
&lt;p&gt;客户端：从客户端来看，一致性主要指的是多并发访问时更新过的数据如何获取的问题。&lt;/p&gt;
&lt;p&gt;服务端：从服务端来看，则是更新如何分布到整个系统，以保证数据最终一致。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对于一致性，可以分为强 / 弱 / 最终一致性三类&lt;/p&gt;
&lt;p&gt;从客户端角度，多进程并发访问时，更新过的数据在不同进程如何获取的不同策略，决定了不同的一致性。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;强一致性：对于关系型数据库，要求更新过的数据能被后续的访问都能看到，这是强一致性。&lt;/li&gt;
&lt;li&gt;弱一致性：如果能容忍后续的部分或者全部访问不到，则是弱一致性。&lt;/li&gt;
&lt;li&gt;最终一致性：如果经过一段时间后要求能访问到更新后的数据，则是最终一致性。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;可用性&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#可用性&#34;&gt;#&lt;/a&gt; 可用性&lt;/h3&gt;
&lt;p&gt;可用性指服务在正常响应时间内一直可用。&lt;/p&gt;
&lt;p&gt;好的可用性主要是指系统能够很好的为用户服务，不出现用户操作失败或者访问超时等用户体验不好的情况。可用性通常情况下与分布式数据冗余，负载均衡等有着很大的关联。&lt;/p&gt;
&lt;h3 id=&#34;分区容错性&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#分区容错性&#34;&gt;#&lt;/a&gt; 分区容错性&lt;/h3&gt;
&lt;p&gt;分区容错性指分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性或可用性的服务。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;base模型&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#base模型&#34;&gt;#&lt;/a&gt; BASE 模型&lt;/h2&gt;
&lt;h3 id=&#34;理论概述-2&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#理论概述-2&#34;&gt;#&lt;/a&gt; 理论概述&lt;/h3&gt;
&lt;p&gt;Base 理论是三要素的缩写：基本可用（Basically Available）、软状态（Soft-state）、最终一致性（Eventually Consistency）。&lt;/p&gt;
&lt;h3 id=&#34;基本可用&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#基本可用&#34;&gt;#&lt;/a&gt; 基本可用&lt;/h3&gt;
&lt;p&gt;“基本可用” 要求系统能够基本运行，一直提供服务，强调的是分布式系统在出现不可预知故障的时候，允许损失部分可用性。&lt;/p&gt;
&lt;h3 id=&#34;软状态&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#软状态&#34;&gt;#&lt;/a&gt; 软状态&lt;/h3&gt;
&lt;p&gt;相对于 ACID 事务中原子性要求（要么做，要么不做），强调的是强制一致性，要求多个节点的数据副本是一致的，强调数据的一致性。这种原子性可以理解为” 硬状态 “。&lt;/p&gt;
&lt;p&gt;软状态则允许系统中的数据存在中间状态，并认为该状态不影响系统的整体可用性，即允许系统在不同节点的数据副本上存在数据延时。&lt;/p&gt;
&lt;h3 id=&#34;最终一致性&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#最终一致性&#34;&gt;#&lt;/a&gt; 最终一致性&lt;/h3&gt;
&lt;p&gt;数据不可能一直处于软状态，必须在一个时间期限后达到各个节点的一致性。在期限过后，应当保证所有副本中的数据保持一致性，也就是达到了数据的最终一致性。&lt;/p&gt;
&lt;p&gt;在系统设计中，最终一致性实现的时间取决于网络延时、系统负载、不同的存储选型，不同数据复制方案设计等因素。也就是说，谁都不保证用户什么时候能看到更新好的数据，但是总会看到的。&lt;/p&gt;
&lt;p&gt;最终一致性作为弱一致性中的特例，强调的是所有数据副本，在经过一段时间的同步后，最终能够到达一致的状态，不需要实时保证系统数据的强一致性，而到达最终一致性。&lt;/p&gt;
&lt;p&gt;根据业务需求的不同，最终一致性中又有很多种情况：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;因果一致性：要求有因果关系的操作顺序得到保证，非因果关系的操作顺序则无所谓。例如微信朋友圈的评论以及对评论的答复所构成的因果关系。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;会话一致性：在操作顺序得到保证的前提下，保证用户在同一个会话里读取数据时保证数据是最新的，如分布式系统 Session 一致性解决方案。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;单调读一致性：用户读取某个数据值后，其后续操作不会读取到该数据更早版本的值。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;单调写一致性：要求数据的所有副本，以相同的顺序执行所有的更新操作，也称为时间轴一致性。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/images/v2-3d4420b4d4eb057f590da858ba6c7523_r.jpg&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;两阶段提交2pc&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#两阶段提交2pc&#34;&gt;#&lt;/a&gt; 两阶段提交（2PC）&lt;/h2&gt;
&lt;p&gt;在分布式系统中，为了让每个节点都能够感知到其他节点的事务执行状况，需要引入一个中心节点来统一处理所有节点的执行逻辑，这个中心节点叫做协调者（coordinator），被中心节点调度的其他业务节点叫做参与者（participant）。&lt;/p&gt;
&lt;p&gt;2PC 将分布式事务分成了两个阶段，两个阶段分别为提交请求（投票）和提交（执行）。协调者根据参与者的响应来决定是否需要真正地执行事务。具体步骤如下：&lt;/p&gt;
&lt;h3 id=&#34;提交请求投票阶段&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#提交请求投票阶段&#34;&gt;#&lt;/a&gt; 提交请求（投票）阶段&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;协调者向所有参与者发送 prepare 请求与事务内容，询问是否可以准备事务提交，并等待参与者的响应。&lt;/li&gt;
&lt;li&gt;参与者执行事务中包含的操作，并记录 undo 日志（用于回滚）和 redo 日志（用于重放），但不真正提交。&lt;/li&gt;
&lt;li&gt;参与者向协调者返回事务操作的执行结果，执行成功返回 yes，否则返回 no。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;提交执行阶段&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#提交执行阶段&#34;&gt;#&lt;/a&gt; 提交（执行）阶段&lt;/h3&gt;
&lt;p&gt;分为成功和失败两种情况：&lt;/p&gt;
&lt;p&gt;若所有参与者都返回 yes，说明事务可以提交：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;协调者向所有参与者发送提交（commit）请求。&lt;/li&gt;
&lt;li&gt;参与者收到提交（commit）请求后，将事务真正地提交上去，并释放占用的事务资源，并向协调者返回 ack。&lt;/li&gt;
&lt;li&gt;协调者收到所有参与者的 ack 消息，事务成功完成。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;若有参与者返回 no 或者超时未返回，说明事务中断，需要回滚：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;协调者向所有参与者发送 rollback 请求。&lt;/li&gt;
&lt;li&gt;参与者收到 rollback 请求后，根据 undo 日志回滚到事务执行前的状态，释放占用的事务资源，并向协调者返回 ack。&lt;/li&gt;
&lt;li&gt;协调者收到所有参与者的 ack 消息，事务回滚完成。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2pc的缺点&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#2pc的缺点&#34;&gt;#&lt;/a&gt; 2PC 的缺点&lt;/h3&gt;
&lt;p&gt;1、协调者存在单点问题。如果协调者挂了，整个 2PC 逻辑就彻底不能运行。&lt;/p&gt;
&lt;p&gt;2、执行过程是完全同步的。各参与者在等待其他参与者响应的过程中都处于阻塞状态，大并发下有性能问题。&lt;/p&gt;
&lt;p&gt;3、仍然存在不一致风险。如果由于网络异常等意外导致只有部分参与者收到了 commit 请求，就会造成部分参与者提交了事务而其他参与者未提交的情况。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;三阶段提交3pc&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#三阶段提交3pc&#34;&gt;#&lt;/a&gt; 三阶段提交（3PC）&lt;/h2&gt;
&lt;h3 id=&#34;cancommit-阶段&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#cancommit-阶段&#34;&gt;#&lt;/a&gt; CanCommit 阶段&lt;/h3&gt;
&lt;p&gt;类似于 2PC 的准备（第一）阶段。协调者向参与者发送 commit 请求，参与者如果可以提交就返回 Yes 响应，否则返回 No 响应。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;事务询问： 协调者向参与者发送 CanCommit 请求，询问是否可以执行事务提交操作，然后开始等待参与者的响应。&lt;/li&gt;
&lt;li&gt;响应反馈： 参与者接到 CanCommit 请求之后，正常情况下， 如果其自身认为可以顺利执行事务，则返回 Yes 响应，并进入预备状态， 否则返回 No。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;precommit-阶段&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#precommit-阶段&#34;&gt;#&lt;/a&gt; PreCommit 阶段&lt;/h3&gt;
&lt;p&gt;协调者根据参与者的反应情况来决定是否可以执行事务的 PreCommit 操作，根据响应情况，有以下两种可能：&lt;/p&gt;
&lt;p&gt;如果响应 Yes，则：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;发送预提交请求：协调者向参与者发送 PreCommit 请求，并进入 Prepared 阶段。&lt;/li&gt;
&lt;li&gt;事务预提交：参与者接收到 PreCommit 请求后，会执行事务操作，并将 undo 和 redo 信息记录到事务日志中。&lt;/li&gt;
&lt;li&gt;响应反馈：如果参与者成功的执行事务操作，则返回 ack 响应，同时开始等待最终指令。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;假如有任何一个参与者向协调者发送 No 响应，或者等待超时之后，协调者都没有接到参与者的响应，那么就执行事务的中断。则：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;发送中断请求：协调者向所有参与者发送 abort 请求。&lt;/li&gt;
&lt;li&gt;中断事务：参与者收到来自协调者的 abort 请求之后（或超时之后，仍未收到协调者的请求），执行事务的中断。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;docommit-阶段&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#docommit-阶段&#34;&gt;#&lt;/a&gt; doCommit 阶段&lt;/h3&gt;
&lt;p&gt;该阶段进行真正的事务提交，也可以分为执行提交和中断事务两种情况。&lt;/p&gt;
&lt;p&gt;如果执行成功，则有如下操作：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;发送提交请求&lt;/p&gt;
&lt;p&gt;协调者接收到参与者发送的 ack 响应，那么它将从预提交状态进入到提交状态，并向所有参与者发送 doCommit 请求。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;事务提交&lt;/p&gt;
&lt;p&gt;参与者接收到 doCommit 请求之后，执行正式的事务提交，并在完成事务提交之后释放所有事务资源。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;响应反馈&lt;/p&gt;
&lt;p&gt;事务提交完之后，向协调者发送 ack 响应。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;完成事务&lt;/p&gt;
&lt;p&gt;协调者接收到所有参与者的 ack 响应之后，完成事务。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;协调者没有接收到参与者发送的 ACK 响应（可能是接受者发送的不是 ACK 响应，也可能响应超时），那么就会执行中断事务。则有如下操作：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;发送中断请求&lt;/p&gt;
&lt;p&gt;协调者向所有参与者发送 abort 请求&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;事务回滚&lt;/p&gt;
&lt;p&gt;参与者接收到 abort 请求之后，利用其在阶段二记录的 undo 信息来执行事务的回滚操作，并在完成回滚之后释放所有的事务资源。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;反馈结果&lt;/p&gt;
&lt;p&gt;参与者完成事务回滚之后，向协调者发送 ACK 消息&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;中断事务&lt;/p&gt;
&lt;p&gt;协调者接收到参与者反馈的 ACK 消息之后，执行事务的中断。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;3pc的缺点&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#3pc的缺点&#34;&gt;#&lt;/a&gt; 3PC 的缺点&lt;/h3&gt;
&lt;p&gt;相对于 2PC，3PC 主要解决的单点故障问题，并减少阻塞，因为一旦参与者无法及时收到来自协调者的信息之后，他会默认执行 commit，而不会一直持有事务资源并处于阻塞状态。但是这种机制也会导致数据一致性问题，因为，由于网络原因，协调者发送的 abort 响应没有及时被参与者接收到，那么参与者在等待超时之后执行了 commit 操作，这样就和其他接到 abort 命令并执行回滚的参与者之间存在数据不一致的情况。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;多版本并发控制mvcc原理&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#多版本并发控制mvcc原理&#34;&gt;#&lt;/a&gt; 多版本并发控制（MVCC）原理&lt;/h2&gt;
&lt;p&gt;参考&lt;a href=&#34;https://juejin.cn/post/7016165148020703246&#34;&gt;看一遍就理解：MVCC 原理详解 - 掘金 (juejin.cn)&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;事务并发存在的问题&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#事务并发存在的问题&#34;&gt;#&lt;/a&gt; 事务并发存在的问题&lt;/h3&gt;
&lt;p&gt;事务并发会引起&lt;strong&gt;脏读、不可重复读、幻读&lt;/strong&gt;问题。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;脏读：如果一个事务读取到了另一个事务未提交修改过的数据，称发生了&lt;strong&gt;脏读&lt;/strong&gt;现象。&lt;/li&gt;
&lt;li&gt;不可重复读：同一个事务内，前后多次读取，读取到的数据内容不一致。在事务 A 范围内，两个相同的查询，读取同一条记录，却返回了不同的数据，这称为&lt;strong&gt;不可重复读&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;幻读：如果一个事务先根据某些搜索条件查询出一些记录，在该事务未提交时，另一个事务写入了一些符合那些搜索条件的记录（如 insert、delete、update），就意味着发生了&lt;strong&gt;幻读&lt;/strong&gt;。事务 A 查询一个范围的结果集，另一个并发事务 B 往这个范围中插入新的数据，并提交事务，然后事务 A 再次查询相同的范围，两次读取到的结果集却不一样了，这就是&lt;strong&gt;幻读&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;四大隔离级别&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#四大隔离级别&#34;&gt;#&lt;/a&gt; 四大隔离级别&lt;/h3&gt;
&lt;p&gt;为了解决并发事务存在的&lt;strong&gt;脏读、不可重复读、幻读&lt;/strong&gt;等问题，数据库设计了四种隔离级别，分别是&lt;strong&gt;读未提交，读已提交，可重复读，串行化（Serializable）&lt;/strong&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;读未提交&lt;/p&gt;
&lt;p&gt;读未提交隔离级别，只限制了两个数据&lt;strong&gt;不能同时修改&lt;/strong&gt;，但是修改数据的时候，即使事务未提交，都是可以被别的事务读取到的，这级别的事务隔离有脏读、重复读、幻读的问题；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;读已提交&lt;/p&gt;
&lt;p&gt;读已提交隔离级别，当前事务只能读取到其他事务&lt;strong&gt;提交&lt;/strong&gt;的数据，所以这种事务的隔离级别&lt;strong&gt;解决了脏读&lt;/strong&gt;问题，但还是会存在&lt;strong&gt;重复读、幻读&lt;/strong&gt;问题；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;可重复读&lt;/p&gt;
&lt;p&gt;可重复读隔离级别，限制了读取数据的时候，不可以进行修改，所以&lt;strong&gt;解决了重复读&lt;/strong&gt;的问题，但是读取范围数据的时候，是可以插入数据，所以还会存在&lt;strong&gt;幻读&lt;/strong&gt;问题；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;可串行化&lt;/p&gt;
&lt;p&gt;事务最高的隔离级别，在该级别下，所有事务都是进行&lt;strong&gt;串行化顺序&lt;/strong&gt;执行的。可以避免脏读、不可重复读与幻读所有并发问题。但是这种事务隔离级别下，事务执行很耗性能。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/images/image-20230907185451838.png&#34; alt=&#34;image-20230907185451838&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;mvcc概述&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#mvcc概述&#34;&gt;#&lt;/a&gt; MVCC 概述&lt;/h3&gt;
&lt;p&gt;通俗的说，数据库中同时存在多个版本的数据，并不是整个数据库的多个版本，而是某一条记录的多个版本同时存在，在某个事务对其进行操作的时候，需要查看这一条记录的隐藏列事务版本 id，比对事务 id 并根据事物隔离级别去判断读取哪个版本的数据。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;事务版本号&lt;/p&gt;
&lt;p&gt;事务每次开启前，都会从数据库获得一个&lt;strong&gt;自增长&lt;/strong&gt;的事务 ID，可以从事务 ID 判断事务的执行先后顺序，这就是事务版本号。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;隐式字段&lt;/p&gt;
&lt;p&gt;对于 InnoDB 存储引擎，每一行记录都有两个隐藏列&lt;strong&gt; trx_id&lt;/strong&gt;、&lt;strong&gt;roll_pointer&lt;/strong&gt;，如果表中没有主键和非 NULL 唯一键时，则还会有第三个隐藏的主键列&lt;strong&gt; row_id&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/image-20230907190609258.png&#34; alt=&#34;image-20230907190609258&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;undo log&lt;/p&gt;
&lt;p&gt;undo log，&lt;strong&gt;回滚日志&lt;/strong&gt;，用于记录数据被修改前的信息。在表记录修改之前，会先把数据拷贝到 undo log 里，如果事务回滚，即可以通过 undo log 来还原数据。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;版本链&lt;/p&gt;
&lt;p&gt;多个事务并行操作某一行数据时，不同事务对该行数据的修改会产生多个版本，然后通过回滚指针（roll_pointer），连成一个链表，这个链表就称为&lt;strong&gt;版本链&lt;/strong&gt;。通过版本链，我们就可以看出&lt;strong&gt;事务版本号、表格隐藏的列和 undo log&lt;/strong&gt; 它们之间的关系。如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/69abb7c89d1e4d9f8e242d9e0a410e3e~tplv-k3u1fbpfcp-zoom-i&#34; alt=&#34;版本链&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;快照读和当前读&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;快照读：&lt;/strong&gt; 读取的是记录数据的可见版本（有旧的版本）。不加锁，普通的 select 语句都是快照读。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;当前读&lt;/strong&gt;：读取的是记录数据的最新版本，显式加锁的都是当前读。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Read View&lt;/p&gt;
&lt;p&gt;事务执行 SQL 语句时，产生的读视图。实际上在 innodb 中，每个 SQL 语句执行前都会得到一个 Read View。它主要是用来做可见性判断的，即判断当前事务可见哪个版本的数据。&lt;/p&gt;
&lt;p&gt;相关参数：&lt;/p&gt;
&lt;p&gt;m_ids：当前系统中那些活跃 (未提交) 的读写事务 ID, 它数据结构为一个 List。&lt;/p&gt;
&lt;p&gt;min_limit_id：表示在生成 ReadView 时，当前系统中活跃的读写事务中最小的事务 id，即 m_ids 中的最小值。&lt;/p&gt;
&lt;p&gt;max_limit_id：表示生成 ReadView 时，系统中应该分配给下一个事务的 id 值。&lt;/p&gt;
&lt;p&gt;creator_trx_id：创建当前 read view 的事务 ID&lt;/p&gt;
&lt;p&gt;Read View 匹配规则：&lt;/p&gt;
&lt;p&gt;1、如果数据事务 ID  &lt;code&gt;trx_id &amp;lt; min_limit_id&lt;/code&gt; ，表明生成该版本的事务在生成 Read View 前，已经提交 (因为事务 ID 是递增的)，所以该版本可以被当前事务访问。&lt;/p&gt;
&lt;p&gt;2、如果 &lt;code&gt;trx_id&amp;gt;= max_limit_id&lt;/code&gt; ，表明生成该版本的事务在生成 ReadView 后才生成，所以该版本不可以被当前事务访问。&lt;/p&gt;
&lt;p&gt;3、如果  &lt;code&gt;min_limit_id =&amp;lt;trx_id&amp;lt; max_limit_id&lt;/code&gt; ，需要分 3 种情况讨论：&lt;/p&gt;
&lt;p&gt;（1）如果 &lt;code&gt;m_ids&lt;/code&gt;  包含 &lt;code&gt;trx_id&lt;/code&gt; , 则代表 Read View 生成时刻，这个事务还未提交，但是如果数据的 &lt;code&gt;trx_id&lt;/code&gt;  等于 &lt;code&gt;creator_trx_id&lt;/code&gt;  的话，表明数据是自己生成的，因此是&lt;strong&gt;可见&lt;/strong&gt;的。&lt;/p&gt;
&lt;p&gt;（2）如果 &lt;code&gt;m_ids&lt;/code&gt;  包含 &lt;code&gt;trx_id&lt;/code&gt; ，并且 &lt;code&gt;trx_id&lt;/code&gt;  不等于 &lt;code&gt;creator_trx_id&lt;/code&gt; ，则 Read   View 生成时，事务未提交，并且不是自己生产的，所以当前事务也是&lt;strong&gt;看不见&lt;/strong&gt;的；&lt;/p&gt;
&lt;p&gt;（3）如果 &lt;code&gt;m_ids&lt;/code&gt;  不包含 &lt;code&gt;trx_id&lt;/code&gt; ，则说明你这个事务在 Read View 生成之前就已经提交了，修改的结果，当前事务是能看见的。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;mvcc实现原理&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#mvcc实现原理&#34;&gt;#&lt;/a&gt; MVCC 实现原理&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;基于 MVCC 查询一条记录&lt;/p&gt;
&lt;p&gt;1、获取事务自己的版本号，即事务 ID；&lt;/p&gt;
&lt;p&gt;2、获取 Read View；&lt;/p&gt;
&lt;p&gt;3、查询得到的数据，然后 Read View 中的事务版本号进行比较；&lt;/p&gt;
&lt;p&gt;4、如果不符合 Read View 的可见性规则， 即就需要 Undo log 中历史快照；&lt;/p&gt;
&lt;p&gt;5、最后返回符合规则的数据。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;InnoDB 实现 MVCC，是通过 &lt;code&gt; Read View+ Undo Log&lt;/code&gt;  实现的，Undo Log 保存了历史快照，Read View 可见性规则帮助判断当前版本的数据是否可见。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;读已提交（RC）隔离级别，存在不可重复读问题&lt;/p&gt;
&lt;p&gt;在&lt;strong&gt;读已提交（RC）隔离级别&lt;/strong&gt;下，同一个事务里，两个相同的查询，读取同一条记录（id=1），却返回了不同的数据（&lt;strong&gt;第一次查出来是孙权，第二次查出来是曹操那条记录&lt;/strong&gt;），因此 RC 隔离级别，存在&lt;strong&gt;不可重复读&lt;/strong&gt;并发问题。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;可重复读（RR）隔离级别，解决不可重复读问题&lt;/p&gt;
&lt;p&gt;RR 可以解决不可重复读问题，就是跟 Read view 工作方式有关。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在读已提交（RC）隔离级别下，同一个事务里面，&lt;strong&gt;每一次查询都会产生一个新的 Read View 副本&lt;/strong&gt;，这样就可能造成同一个事务里前后读取数据可能不一致的问题（不可重复读并发问题）。&lt;/li&gt;
&lt;li&gt;在可重复读（RR）隔离级别下，&lt;strong&gt;一个事务里只会获取一次 read view&lt;/strong&gt;，都是副本共用的，从而保证每次查询的数据都是一样的。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;一致性算法&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#一致性算法&#34;&gt;#&lt;/a&gt; 一致性算法&lt;/h2&gt;
&lt;p&gt;1、为什么需要一致性算法？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;数据不能存在单个节点（主机）上，否则可能出现单点故障。&lt;/li&gt;
&lt;li&gt;多个节点（主机）需要保证具有相同的数据。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2、一致性分类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;强一致性&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;说明：保证系统改变提交以后立即改变集群的状态。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;模型：&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;Paxos&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;Raft（muti-paxos）&lt;/li&gt;
&lt;li&gt;ZAB（muti-paxos）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;弱一致性&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;说明：也叫最终一致性，系统不保证改变提交以后立即改变集群的状态，但是随着时间的推移最终状态是一致的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;模型：&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;DNS 系统&lt;/li&gt;
&lt;li&gt;Gossip 协议&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;paxos算法&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#paxos算法&#34;&gt;#&lt;/a&gt; Paxos 算法&lt;/h3&gt;
&lt;h4 id=&#34;算法流程&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#算法流程&#34;&gt;#&lt;/a&gt; 算法流程&lt;/h4&gt;
&lt;p&gt;Paxos 算法解决的问题正是分布式一致性问题，即一个分布式系统中的各个进程如何就某个值（决议）达成一致。&lt;/p&gt;
&lt;p&gt;Paxos 算法运行在允许宕机故障的异步系统中，不要求可靠的消息传递，可容忍消息丢失、延迟、乱序以及重复。它利用大多数 (Majority) 机制保证了 2F+1 的容错能力，即 2F+1 个节点的系统最多允许 F 个节点同时出现故障。&lt;/p&gt;
&lt;p&gt;一个或多个提议进程 (Proposer) 可以发起提案 (Proposal)，Paxos 算法使所有提案中的某一个提案，在所有进程中达成一致。系统中的多数派同时认可该提案，即达成了一致。最多只针对一个确定的提案达成一致。&lt;/p&gt;
&lt;p&gt;Paxos 将系统中的角色分为提议者 (Proposer)，决策者 (Acceptor)，和最终决策学习者 (Learner):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Proposer&lt;/strong&gt;: 提出提案 (Proposal)。Proposal 信息包括提案编号 (Proposal ID) 和提议的值 (Value)。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Acceptor&lt;/strong&gt;：参与决策，回应 Proposers 的提案。收到 Proposal 后可以接受提案，若 Proposal 获得多数 Acceptors 的接受，则称该 Proposal 被批准。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learner&lt;/strong&gt;：不参与决策，从 Proposers/Acceptors 学习最新达成一致的提案（Value）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Paxos 算法分为&lt;strong&gt;两个阶段&lt;/strong&gt;。具体如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;阶段一：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;(a) Proposer 选择一个&lt;strong&gt;提案编号 N&lt;/strong&gt;，然后向&lt;strong&gt;半数以上&lt;/strong&gt;的 Acceptor 发送编号为 N 的&lt;strong&gt; Prepare 请求&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;(b) 如果一个 Acceptor 收到一个编号为 N 的 Prepare 请求，且 N&lt;strong&gt; 大于&lt;/strong&gt;该 Acceptor 已经&lt;strong&gt;响应过的&lt;/strong&gt;所有&lt;strong&gt; Prepare 请求&lt;/strong&gt;的编号，那么它就会将它已经&lt;strong&gt;接受过的编号最大的提案（如果有的话）&lt;strong&gt;作为响应反馈给 Proposer，同时该 Acceptor 承诺&lt;/strong&gt;不再接受&lt;/strong&gt;任何&lt;strong&gt;编号小于 N 的提案&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;阶段二：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;(a) 如果 Proposer 收到&lt;strong&gt;半数以上&lt;/strong&gt; Acceptor 对其发出的编号为 N 的 Prepare 请求的&lt;strong&gt;响应&lt;/strong&gt;，那么它就会发送一个针对 **[N,V] 提案&lt;strong&gt;的&lt;/strong&gt; Accept 请求&lt;strong&gt;给&lt;/strong&gt;半数以上&lt;strong&gt;的 Acceptor。注意：V 就是收到的&lt;/strong&gt;响应&lt;strong&gt;中&lt;/strong&gt;编号最大的提案的 value**，如果响应中&lt;strong&gt;不包含任何提案&lt;/strong&gt;，那么 V 就由 Proposer&lt;strong&gt; 自己决定&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;(b) 如果 Acceptor 收到一个针对编号为 N 的提案的 Accept 请求，只要该 Acceptor&lt;strong&gt; 没有&lt;/strong&gt;对编号&lt;strong&gt;大于 N&lt;/strong&gt; 的&lt;strong&gt; Prepare 请求&lt;/strong&gt;做出过&lt;strong&gt;响应&lt;/strong&gt;，它就&lt;strong&gt;接受该提案&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;raft算法&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#raft算法&#34;&gt;#&lt;/a&gt; Raft 算法&lt;/h3&gt;
&lt;p&gt;Raft 将系统中的角色分为领导者（Leader）、跟随者（Follower）和候选人（Candidate）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Leader&lt;/strong&gt;：接受客户端请求，并向 Follower 同步请求日志，当日志同步到大多数节点上后告诉 Follower 提交日志。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Follower&lt;/strong&gt;：接受并持久化 Leader 同步的日志，在 Leader 告之日志可以提交之后，提交日志。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Candidate&lt;/strong&gt;：Leader 选举过程中的临时角色。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;概述&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#概述&#34;&gt;#&lt;/a&gt; 概述&lt;/h4&gt;
&lt;p&gt;Raft 要求系统在任意时刻最多只有一个 Leader，正常工作期间只有 Leader 和 Followers。Follower 只响应其他服务器的请求。如果 Follower 超时没有收到 Leader 的消息，它会成为一个 Candidate 并且开始一次 Leader 选举。收到大多数服务器投票的 Candidate 会成为新的 Leader。Leader 在宕机之前会一直保持 Leader 的状态。&lt;/p&gt;
&lt;p&gt;Raft 算法将时间分为一个个的任期（term），每一个 term 的开始都是 Leader 选举。在成功选举 Leader 之后，Leader 会在整个 term 内管理整个集群。如果 Leader 选举失败，该 term 就会因为没有 Leader 而结束。&lt;/p&gt;
&lt;h4 id=&#34;leader选举&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#leader选举&#34;&gt;#&lt;/a&gt; Leader 选举&lt;/h4&gt;
&lt;p&gt;Raft 使用心跳（heartbeat）触发 Leader 选举。当服务器启动时，初始化为 Follower。Leader 向所有 Followers 周期性发送 heartbeat。如果 Follower 在选举超时时间内没有收到 Leader 的 heartbeat，就会等待一段随机的时间后发起一次 Leader 选举。&lt;/p&gt;
&lt;p&gt;Follower 将其当前 term 加一然后转换为 Candidate。它首先给自己投票并且给集群中的其他服务器发送 RequestVote RPC。结果有以下三种情况：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;赢得了多数的选票，成功选举为 Leader；&lt;/li&gt;
&lt;li&gt;收到了 Leader 的消息，表示有其它服务器已经抢先当选了 Leader；&lt;/li&gt;
&lt;li&gt;没有服务器赢得多数的选票，Leader 选举失败，等待选举时间超时后发起下一次选举。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;选举出 Leader 后，Leader 通过定期向所有 Followers 发送心跳信息维持其统治。若 Follower 一段时间未收到 Leader 的心跳则认为 Leader 可能已经挂了，再次发起 Leader 选举过程。&lt;strong&gt;Raft 保证选举出的 Leader 上一定具有最新的已提交的日志&lt;/strong&gt;。&lt;/p&gt;
&lt;h4 id=&#34;日志同步&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#日志同步&#34;&gt;#&lt;/a&gt; 日志同步&lt;/h4&gt;
&lt;p&gt;Leader 选出后，就开始接收客户端的请求。Leader 把请求作为日志条目（Log entries）加入到它的日志中，然后并行的向其他服务器发起 AppendEntries RPC 复制日志条目。当这条日志被复制到大多数服务器上，Leader 将这条日志应用到它的状态机并向客户端返回执行结果。（某些 Followers 可能没有成功的复制日志，Leader 会无限的重试 AppendEntries RPC 直到所有的 Followers 最终存储了所有的日志条目）&lt;/p&gt;
&lt;p&gt;日志由有序编号（log index）的日志条目组成。每个日志条目包含它被创建时的任期号（term），和用于状态机执行的命令。如果一个日志条目被复制到大多数服务器上，就被认为可以提交（commit）了。&lt;/p&gt;
&lt;p&gt;Leader 通过强制 Followers 复制它的日志来处理日志的不一致，Followers 上的不一致的日志会被 Leader 的日志覆盖。&lt;/p&gt;
&lt;h4 id=&#34;安全性&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#安全性&#34;&gt;#&lt;/a&gt; 安全性&lt;/h4&gt;
&lt;p&gt;Raft 增加了如下两条限制以保证安全性：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;拥有最新的已提交的 log entry 的 Follower 才有资格成为 Leader。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;这个保证是在 RequestVote RPC 中做的，Candidate 在发送 RequestVote RPC 时，要带上自己的最后一条日志的 term 和 log index，其他节点收到消息时，如果发现自己的日志比请求中携带的更新，则拒绝投票。日志比较的原则是，如果本地的最后一条 log entry 的 term 更大，则 term 大的更新，如果 term 一样大，则 log index 更大的更新。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Leader 只能推进 commit index 来提交当前 term 的已经复制到大多数服务器上的日志，旧 term 日志的提交要等到提交当前 term 的日志来间接提交（log index 小于 commit index 的日志被间接提交）&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;之所以要这样，是因为可能会出现已提交的日志又被覆盖的情况。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;日志压缩&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#日志压缩&#34;&gt;#&lt;/a&gt; 日志压缩&lt;/h4&gt;
&lt;p&gt;在实际的系统中，不能让日志无限增长，否则系统重启时需要花很长的时间进行回放，从而影响可用性。Raft 采用对整个系统进行 snapshot 来解决，snapshot 之前的日志都可以丢弃。&lt;/p&gt;
&lt;p&gt;每个副本独立的对自己的系统状态进行 snapshot，并且只能对已经提交的日志记录进行 snapshot。&lt;/p&gt;
&lt;h4 id=&#34;成员变更&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#成员变更&#34;&gt;#&lt;/a&gt; 成员变更&lt;/h4&gt;
&lt;p&gt;成员变更是在集群运行过程中副本发生变化，如增加 / 减少副本数、节点替换等。&lt;/p&gt;
&lt;p&gt;如果将成员变更当成一般的一致性问题，直接向 Leader 发送成员变更请求，Leader 复制成员变更日志，达成多数派之后提交，各服务器提交成员变更日志后从旧成员配置（Cold）切换到新成员配置（Cnew）。因为各个服务器提交成员变更日志的时刻可能不同，造成各个服务器从旧成员配置（Cold）切换到新成员配置（Cnew）的时刻不同。成员变更不能影响服务的可用性，但是成员变更过程的某一时刻，可能出现在 Cold 和 Cnew 中同时存在两个不相交的多数派，进而可能选出两个 Leader，形成不同的决议，破坏安全性。&lt;/p&gt;
&lt;p&gt;由于成员变更的这一特殊性，成员变更不能当成一般的一致性问题去解决。&lt;/p&gt;
&lt;p&gt;为了解决这一问题，Raft 提出了两阶段的成员变更方法。集群先从旧成员配置 Cold 切换到一个过渡成员配置，称为共同一致（joint consensus），共同一致是旧成员配置 Cold 和新成员配置 Cnew 的组合 Cold U Cnew，一旦共同一致 Cold U Cnew 被提交，系统再切换到新成员配置 Cnew。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Raft 两阶段成员变更过程如下&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Leader 收到成员变更请求从 Cold 切成 Cold,new；&lt;/li&gt;
&lt;li&gt;Leader 在本地生成一个新的 log entry，其内容是 Cold∪Cnew，代表当前时刻新旧成员配置共存，写入本地日志，同时将该 log entry 复制至 Cold∪Cnew 中的所有副本。在此之后新的日志同步需要保证得到 Cold 和 Cnew 两个多数派的确认；&lt;/li&gt;
&lt;li&gt;Follower 收到 Cold∪Cnew 的 log entry 后更新本地日志，并且此时就以该配置作为自己的成员配置；&lt;/li&gt;
&lt;li&gt;如果 Cold 和 Cnew 中的两个多数派确认了 Cold U Cnew 这条日志，Leader 就提交这条 log entry 并切换到 Cnew；&lt;/li&gt;
&lt;li&gt;接下来 Leader 生成一条新的 log entry，其内容是新成员配置 Cnew，同样将该 log entry 写入本地日志，同时复制到 Follower 上；&lt;/li&gt;
&lt;li&gt;Follower 收到新成员配置 Cnew 后，将其写入日志，并且从此刻起，就以该配置作为自己的成员配置，并且如果发现自己不在 Cnew 这个成员配置中会自动退出；&lt;/li&gt;
&lt;li&gt;Leader 收到 Cnew 的多数派确认后，表示成员变更成功，后续的日志只要得到 Cnew 多数派确认即可。Leader 给客户端回复成员变更执行成功。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;如果增强成员变更的限制，假设 Cold 与 Cnew 任意的多数派交集不为空，这两个成员配置就无法各自形成多数派，那么成员变更方案就可能简化为一阶段。&lt;/p&gt;
&lt;p&gt;那么如何限制 Cold 与 Cnew，使之任意的多数派交集不为空呢？方法就是每次成员变更只允许增加或删除一个成员。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;一阶段成员变更&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;成员变更限制每次只能增加或删除一个成员（如果要变更多个成员，连续变更多次）。&lt;/li&gt;
&lt;li&gt;成员变更由 Leader 发起，Cnew 得到多数派确认后，返回客户端成员变更成功。&lt;/li&gt;
&lt;li&gt;一次成员变更成功前不允许开始下一次成员变更，因此新任 Leader 在开始提供服务前要将自己本地保存的最新成员配置重新投票形成多数派确认。&lt;/li&gt;
&lt;li&gt;Leader 只要开始同步新成员配置，即可开始使用新的成员配置进行日志同步。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Raft 与 Multi-Paxos 中相似的概念：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/v2-a932cb62a02604d5ec57dc0a046a1414_r.jpg&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;Raft 与 Multi-Paxos 的不同：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/v2-7679d235c0ac8056552ba88b677e73a2_r.jpg&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
 ]]></description>
        </item>
    </channel>
</rss>
