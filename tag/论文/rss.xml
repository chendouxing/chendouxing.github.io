<?xml version="1.0"?>
<rss version="2.0">
    <channel>
        <title>蟹堡王海星 • Posts by &#34;论文&#34; tag</title>
        <link>https://chendouxing.github.io</link>
        <description>花有重开日，人无再少年</description>
        <language>zh-CN</language>
        <pubDate>Mon, 09 Oct 2023 14:26:24 +0800</pubDate>
        <lastBuildDate>Mon, 09 Oct 2023 14:26:24 +0800</lastBuildDate>
        <category>分布式</category>
        <category>方法</category>
        <category>语言</category>
        <category>SQL</category>
        <category>论文</category>
        <category>数学</category>
        <category>课堂学习</category>
        <category>计算机等级考试</category>
        <item>
            <guid isPermalink="true">https://chendouxing.github.io/2023/10/09/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%BA%E6%96%87/</guid>
            <title>分布式论文</title>
            <link>https://chendouxing.github.io/2023/10/09/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%BA%E6%96%87/</link>
            <category>分布式</category>
            <category>论文</category>
            <pubDate>Mon, 09 Oct 2023 14:26:24 +0800</pubDate>
            <description><![CDATA[ &lt;link rel=&#34;stylesheet&#34; class=&#34;aplayer-secondary-style-marker&#34; href=&#34;\assets\css\APlayer.min.css&#34;&gt;&lt;script src=&#34;\assets\js\APlayer.min.js&#34; class=&#34;aplayer-secondary-script-marker&#34;&gt;&lt;/script&gt;&lt;h1 id=&#34;the-google-file-system&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#the-google-file-system&#34;&gt;#&lt;/a&gt; 《The Google File System》&lt;/h1&gt;
&lt;p&gt;（谷歌文件系统 GFS）&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/33944479&#34;&gt;Google File System 论文详析 - 知乎 (zhihu.com)&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;简介&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#简介&#34;&gt;#&lt;/a&gt; 简介&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;组件故障是常态而不是例外。持续监控、错误检测、容错和自动恢复必须成为系统的组成部分。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;按照传统的分布式文件系统，文件的大小是难以管理的，必须重新考虑设计假设和参数，如 I/O 操作和块大小。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;共同设计应用程序和文件系统 API 通过提高灵活性使整个系统受益。&lt;strong&gt;引入原子追加操作，这样多个客户端就可以并发地向一个文件追加内容，而无需在它们之间进行额外的同步。&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GFS 具有快照和记录追加操作。快照以低成本创建文件或目录树的副本。&lt;strong&gt;记录追加允许多个客户端并发地将数据追加到同一文件，同时保证每个客户端追加的原子性。&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;假设&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#假设&#34;&gt;#&lt;/a&gt; 假设&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;该系统由许多经常失效的廉价商品组件构建而成。它必须不断地自我监控，并定期检测、容忍组件故障并迅速恢复。&lt;/li&gt;
&lt;li&gt;系统存储适量的大型文件。&lt;/li&gt;
&lt;li&gt;工作负载主要包括两种读取：&lt;strong&gt;大的流读取和小的随机读取&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;工作负载还具有许多将数据追加到文件的大型顺序写入。一旦写好，文件很少再修改。支持在文件中任意位置的小写入，不需要是高效的。&lt;/li&gt;
&lt;li&gt;系统为并发附加到同一文件的多个客户端实现良好定义的语义（多个用户写同一个文件）。每台机器运行一个生产者，将并发地附加到一个文件中。&lt;/li&gt;
&lt;li&gt;高持续带宽比低延迟更重要。大多数目标应用程序都非常重视以高速率批量处理数据，而很少有应用程序对单个读取或写入有严格的响应时间要求。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;体系架构&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#体系架构&#34;&gt;#&lt;/a&gt; 体系架构&lt;/h2&gt;
&lt;p&gt;一个 GFS 集群由一个 master 和多个 chunkserver 组成，并由多个客户端访问，如图 1 所示。其中的每一个通常都是运行用户级服务器进程的商用 Linux 机器。文件被分成固定大小的块。每个 chunk 由一个不可变的、全局唯一的 64 位 chunk handle（块句柄），该 handle 由 master 在 chunk 创建时分配。Chunkserver 将块存储在本地磁盘上作为 Linux 文件，并读取或写入由块句柄和字节范围指定的块数据。为了可靠性，每个块都在多个块服务器上复制。默认情况下，我们存储三个副本，尽管用户可以为文件命名空间的不同区域指定不同的复制级别。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/image-20231009153731477.png&#34; alt=&#34;image-20231009153731477&#34;&gt;&lt;/p&gt;
&lt;p&gt;主服务器维护文件系统所有的元数据，包括命名空间、访问控制信息、从文件到块的映射以及块的当前位置。它还控制系统范围的活动，如块租用管理、孤立块的垃圾收集以及块服务器之间的块迁移。主服务器定期在&lt;strong&gt;心跳消息&lt;/strong&gt;中与每个块服务器通信，以向其提供指令并收集其状态。&lt;/p&gt;
&lt;p&gt;链接到每个应用程序的 GFS 客户端代码实现文件系统 API，并与主服务器和块服务器通信。客户端与主服务器交互进行元数据操作，但所有数据承载通信都直接到块服务器。&lt;/p&gt;
&lt;p&gt;客户端和块服务器都不缓存文件数据。因为大多数应用程序都流经大量的文件，或者工作集太大而无法缓存。不使用它们可以消除缓存一致性问题，从而简简化客户端和整个系统。(客户端会缓存元数据)。块服务器不需要缓存文件数据，因为块存储在本地文件中，因此 LINUX 的 buﬀer 缓存已经将频繁访问的数据保存在内存中。&lt;/p&gt;
&lt;h3 id=&#34;简单阅读的交互&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#简单阅读的交互&#34;&gt;#&lt;/a&gt; 简单阅读的交互&lt;/h3&gt;
&lt;p&gt;首先，使用固定的块大小，客户端将应用程序指定的文件名和字节偏移量转换为文件中的块指数。然后，它向主服务器发送包含文件名和块索引的请求。主服务器用相应的块句柄和副本的位置进行回复，客户端使用文件名和块指数作为键来缓存这些信息。（主服务器告诉客户端目标文件在哪个块服务器）&lt;/p&gt;
&lt;p&gt;然后客户端向其中一个副本（在块服务器中）发送请求，很可能是最近的副本。该请求指定块句柄和该块内的字节范围，在缓存信息过期或文件重新打开之前，对同一块的进一步读取不需要更多的客户端 - 主服务器交互。事实上，客户端通常在同一请求中请求多个块，并且主服务器还可以包括紧接在所请求的那些块之后的块的信息，这些额外的信息避开了几个未来的客户端 - 主服务器交互。&lt;/p&gt;
&lt;h3 id=&#34;块大小&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#块大小&#34;&gt;#&lt;/a&gt; 块大小&lt;/h3&gt;
&lt;p&gt;每个区块副本都作为普通的 Linux 文件存储在块服务器上，并且仅在需要时进行扩展，惰性空间分配避免了由于内部碎片而浪费空间。&lt;u&gt;（惰性空间分配时，空间的物理分配会尽可能延迟，直到累积了块大小大小的数据。换句话说，在磁盘上分配新块之前的决策过程在很大程度上受到要写入的数据大小的影响，即 64 MB 块的未使用部分最小化。【尽可能地填满前面的块，才开始创建写入后面的块】）&lt;/u&gt;&lt;/p&gt;
&lt;p&gt;大块的优势：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;减少了客户端与主服务器交互的需求，因为对同一块的读写只需要向主服务器发出一个初始请求，以获取块的位置信息。这种减少对于我们的工作负载尤其重要，因为应用程序大多按顺序读取和写入大型文件。即使对于小的随机读取，客户端也可以轻松地缓存多 TB 工作集的所有块位置信息（原因与优势 3 一致，主要由于元数据会更小，更加便于存储到内存中。）&lt;/li&gt;
&lt;li&gt;由于在大块上，客户端更有可能对给定的块执行许多操作，因此可以通过在延长的时间段内保持与块服务器的持久网络连接来减少网络开销。&lt;/li&gt;
&lt;li&gt;减小了存储在主服务器上的元数据的大小，这允许我们将元数据保存在内存中，这又带来了其他优势。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;大块的缺点：&lt;/p&gt;
&lt;p&gt;（使用惰性空间分配）&lt;/p&gt;
&lt;p&gt;一个小文件可能由少量块组成，可能只有一个块。如果许多客户端正在访问同一个文件，存储这些块的块服务器可能会成为热点。在实践中，热点并不是一个主要问题，因为我们的应用程序大多顺序读取大型多块文件。（热点是常见问题）&lt;/p&gt;
&lt;p&gt;当 GFS 首次被批处理队列系统使用时，热点确实出现了：一个可执行文件作为一个单一的块文件被写入 GFS，然后同时在数百台机器上启动，此时存储可执行文件的少数块服务器被数百个同时请求超载。&lt;strong&gt;解决方法：&lt;strong&gt;我们通过使用更高的复制因子存储这样的可执行文件，并通过使批处理队列系统错开应用程序的启动时间来解决这个问题。一个潜在的&lt;/strong&gt;长期解决方案&lt;/strong&gt;是允许客户端在这种情况下从其他客户端读取数据。&lt;/p&gt;
&lt;h3 id=&#34;元数据&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#元数据&#34;&gt;#&lt;/a&gt; 元数据&lt;/h3&gt;
&lt;p&gt;主服务器存储三种主要类型的元数据：&lt;strong&gt;文件和块命名空间&lt;/strong&gt;、&lt;strong&gt;从文件到块的映射&lt;/strong&gt;以及&lt;strong&gt;每个块的副本的位置&lt;/strong&gt;。所有元数据都保存在主存储器中。前两种类型（命名空间和文件到块的映射）通过将增量记录到存储在主机本地磁盘上的操作日志中并在远程机器上复制来保持持久性，使用日志使得我们可以简单、可靠地更新主服务器状态，并且在主服务器崩溃时不会有不一致的风险。&lt;strong&gt;主服务器不永久存储块位置信息&lt;/strong&gt;，&lt;strong&gt;相反，它会在主服务器启动时以及每当块服务器加入集群时询问每个块服务器的块位置信息&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&#34;一致性模型&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#一致性模型&#34;&gt;#&lt;/a&gt; 一致性模型&lt;/h3&gt;
&lt;h2 id=&#34;系统交互&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#系统交互&#34;&gt;#&lt;/a&gt; 系统交互&lt;/h2&gt;
&lt;p&gt;（描述客户端、主服务器和块服务器如何交互来实现数据变化、原子记录附加和快照。）&lt;/p&gt;
&lt;h3 id=&#34;租约和修改顺序&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#租约和修改顺序&#34;&gt;#&lt;/a&gt; 租约和修改顺序&lt;/h3&gt;
&lt;p&gt;变化是一种改变块的内容或元数据的操作，如写入或追加操作。&lt;strong&gt;每个变化在所有块的副本处执行&lt;/strong&gt;，使用租约来保持其一致的变化顺序。主节点向其中一个副本节点授予块租约，我们称之为原始节点（primary）。原始节点为块的所有变化选择一个连续的顺序。应用变化时，所有副本都遵循此顺序。因此，全局变化顺序首先由主服务器（master）选择的租约授予顺序来定义，并且在租约内由原始节点分配的序列号来定义。&lt;/p&gt;
&lt;p&gt;租约机制旨在最小化主服务器处的管理开销。只要块发生变化，原始节点就可以无限期地请求并通常从主服务器接收扩展。这些扩展请求和授予被捎带在主服务器和所有块服务器之间定期交换的心跳消息上。主服务器有时可能试图在租约到期之前撤销租约（例如，当主服务器想要禁用正在被重命名的文件上的变化时）。即使主服务器失去了与原始节点的通信，它也可以在旧租约到期后安全地将新租约授予另一个副本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;写的控制流程&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;客户端询问主服务器哪个块服务器持有块的当前租约以及其他副本的位置。如果没有块服务器具有租约，则主服务器将租约授予它选择的副本。&lt;/li&gt;
&lt;li&gt;主服务器回复原始节点的标识和其他（辅助）副本的位置。客户端缓存该数据以用于将来的变化。只有当原始节点变得不可访问或答复它不再持有租约时，它才需要再次与主服务器联系。&lt;/li&gt;
&lt;li&gt;客户端将数据推送到所有副本，客户端可以按任何顺序执行此操作。每个块服务器将数据存储在内部 LRU 缓冲区缓存中，直到数据被使用或失效。通过将数据流与控制流解耦，我们可以基于网络拓扑来调度昂贵的数据流来提高性能，而不管哪个块服务器是主要的（primary）。&lt;/li&gt;
&lt;li&gt;一旦所有副本都确认收到数据，客户端就向原始节点发送写请求。该请求标识先前推送到所有副本的数据。原始节点将连续的序列号分配给它可能从多个客户端接收的所有变化，这提供了必要的序列化。它以序列号顺序将变化应用到它自己的本地状态。&lt;/li&gt;
&lt;li&gt;原始节点将写入请求转发到所有辅助副本。每个辅助副本以由原始节点分配的相同序列号顺序应用增量。&lt;/li&gt;
&lt;li&gt;副本都回复原始节点，表示它们已经完成了操作。&lt;/li&gt;
&lt;li&gt;原始节点回复客户端。在任何副本中遇到的任何错误都将报告给客户端。如果出现错误，则可能在原始节点和辅助副本的任意子集上成功写入 (如果它在原始节点上发生故障，则不会为其分配序列号并转发），客户端请求被认为失败，并且修改的区域保持不一致状态。客户端代码通过重试失败的变化来处理此类错误。它将在步骤（3）到（7）中尝试几次，然后从写开始退回重试。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;/images/image-20231011181559699.png&#34; alt=&#34;image-20231011181559699&#34;&gt;&lt;/p&gt;
&lt;p&gt;如果应用程序的写入操作很大或跨越块边界，GFS 客户端代码将其分解为多个写入操作。它们都遵循上述控制流程，但可能与来自其他客户端的并发操作交织并被其覆盖。因此，共享文件区域可能最终包含来自不同客户端的片段，尽管副本将是相同的，因为各个操作在所有副本上以相同的顺序成功完成，这使文件区域处于一致但未定义的状态。&lt;/p&gt;
&lt;h3 id=&#34;记录追加&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#记录追加&#34;&gt;#&lt;/a&gt; 记录追加&lt;/h3&gt;
&lt;h3 id=&#34;快照&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#快照&#34;&gt;#&lt;/a&gt; 快照&lt;/h3&gt;
&lt;p&gt;使用标准的写时复制技术来实现快照。当主服务器接收到快照请求时，它首先撤销它要快照的文件中块的任何未完成的租约。这确保了对这些块的任何后续写入都将需要与主设备进行交互以找到租用保持器。在租约被撤销或到期后，主服务器将操作记录到磁盘。然后，它通过复制源文件或目录树的元数据，将此日志记录应用于其内存中状态。新创建的快照文件指向与源文件相同的块。&lt;/p&gt;
&lt;p&gt;在快照操作之后，当客户端第一次想要写入块 C 时，它会向主机发送一个请求，以查找当前的租户。主程序注意到块 C 的引用计数大于 1。它推迟对客户端请求的回复，而是选择新的组块句柄 C‘。然后，它要求每个拥有 C 的当前副本的块服务器创建一个名为 C‘的新块。通过在与原始数据块相同的数据块服务器上创建新数据块，我们确保可以在本地复制数据，而不是通过网络。从这一点来看，请求处理与任何块的请求处理没有什么不同：主服务器向其中一个副本授予新块 C‘的租约，并回复客户端，客户端可以正常写入块，而不知道它是从现有块创建的。&lt;/p&gt;
&lt;h2 id=&#34;主服务器操作&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#主服务器操作&#34;&gt;#&lt;/a&gt; 主服务器操作&lt;/h2&gt;
&lt;h3 id=&#34;命名空间管理和锁定&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#命名空间管理和锁定&#34;&gt;#&lt;/a&gt; 命名空间管理和锁定&lt;/h3&gt;
&lt;p&gt;许多主操作可能需要很长时间：例如，快照操作必须撤销快照覆盖的所有区块上的块服务器租约。我们不希望在其他主操作运行时延迟它们。因此，我们允许多个操作处于活动状态，并在命名空间的区域上使用锁来确保正确的序列化。&lt;/p&gt;
&lt;p&gt;GFS 在逻辑上将其命名空间表示为一个查找表，该表将完整路径名映射到元数据。通过前缀压缩，可以在内存中有效地表示该表。命名空间树中的每个节点（绝对文件名或绝对目录名）都有一个关联的读写锁。&lt;strong&gt;每个主操作在运行之前都会获得一组锁&lt;/strong&gt;。通常，如果它涉及 /d1/d2/…/dn/leaf，它将获取目录名 /d1，/d1/d2，…，/d1/d2/…/dn，以及完整路径名 /d1/d2/…/ 上的读锁或写锁。&lt;/p&gt;
&lt;p&gt;这种锁定模式的一个很好的特性是，它允许在同一目录中进行并发变化。例如，多个文件创建可以在同一个目录中同时执行：每个文件创建都在目录名上获得一个读锁定，在文件名上获得一个写锁定。目录名上的读锁定足以防止目录被删除、重命名或快照。对文件名序列化的写锁尝试创建具有相同名称的文件两次。&lt;/p&gt;
&lt;p&gt;由于命名空间可以有许多节点，读写锁对象被延迟分配，一旦不使用就被删除。此外，锁是以一致的总顺序获取的，以防止死锁：它们首先在名称空间树中按级别排序，并在同一级别内按字典顺序排序。&lt;/p&gt;
&lt;h3 id=&#34;副本管理&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#副本管理&#34;&gt;#&lt;/a&gt; 副本管理&lt;/h3&gt;
&lt;p&gt;当 master 创建一个 chunk 时，它会选择在哪里放置最初为空的副本。它考虑了几个因素。(1) 希望将新副本放置在磁盘空间利用率低于平均水平的 chunkserver 上。随着时间的推移，这将均衡块服务器之间的磁盘利用率。(2) 希望限制每个 chunkserver 上 “最近” 创建的数量。虽然创建本身很便宜，但它可靠地预测了即将到来的繁重的写入流量，因为块是在写入需要时创建的，并且在我们的 append-once-read-many 工作负载中，一旦它们被完全写入，它们通常就会变成只读。(3) 如上所述，希望跨机架分布块副本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;增加块副本的优先级&lt;/strong&gt;：一旦可用副本的数量低于用户指定的目标数量，主服务器就会重新复制这些块。这种情况可能有多种原因：chunkserver 变得不可用，它报告其副本可能已损坏，其中一个磁盘因错误而被禁用，或者复制目标增加。每个需要重新复制的块都基于几个因素进行优先级排序。一个是它离复制目标有多远。例如，我们给予一个丢失了两个副本的块比一个只丢失了一个副本的块更高的优先级。此外，我们更倾向于首先重新复制活动文件的块。最后，为了最小化故障对正在运行的应用程序的影响，我们提高了任何阻碍客户端进程的块的优先级。&lt;/p&gt;
&lt;p&gt;主服务器选择最高优先级的块，并通过指示一些块服务器直接从现有的有效副本复制块数据来 “克隆” 它。新副本的目标与创建副本的目标类似：均衡磁盘空间利用率，限制任何单个 chunkserver 上的活动克隆操作，以及将副本分布在机架上。为了防止克隆流量压倒客户端流量，主服务器限制集群和每个 chunkserver 的活动克隆操作数量。此外，每个 chunkserver 通过限制其对源 chunkserver 的读取请求来限制其在每个克隆操作上花费的带宽量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;块副本负载均衡&lt;/strong&gt;：最后，主服务器定期重新平衡副本：它检查当前副本的分布，并移动副本以获得更好的磁盘空间和负载平衡。同样在这个过程中，主服务器会逐渐填满一个新的 chunkserver，而不是立即用新的 chunk 和随之而来的繁重的写流量填满它。新副本的放置标准与上面讨论的类似。此外，主服务器还必须选择要删除的现有副本。一般来说，它倾向于删除那些低于平均可用空间的块服务器，以均衡磁盘空间使用。&lt;/p&gt;
&lt;h3 id=&#34;回收存储空间&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#回收存储空间&#34;&gt;#&lt;/a&gt; 回收存储空间&lt;/h3&gt;
&lt;p&gt;删除文件后，&lt;strong&gt;GFS 不会立即回收可用的物理存储&lt;/strong&gt;。它只在文件和块级别的常规垃圾收集期间才执行回收。这种方法使系统更简单可靠。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;当应用程序删除文件时，主服务器会像记录其他更改一样立即记录删除。但是，不是立即回收资源，而是将文件重命名为包含删除时间戳的隐藏名称&lt;/strong&gt;。在主服务器定期扫描文件系统命名空间时，如果任何此类隐藏文件的存在时间超过三天 (间隔是可配置的)，它会将其删除（自：为了实现高效恢复）。&lt;strong&gt;在此之前，该文件仍然可以使用新的特殊名称读取，并且可以通过将其重命名为 Normal 来恢复&lt;/strong&gt;。当隐藏文件从命名空间中移除时，其内存中元数据将被擦除，这实际上切断了它与所有数据块的连接。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;块服务器删除块&lt;/strong&gt;：在块命名空间的类似的常规扫描中，主服务器标识孤立块（即，那些不能从任何文件到达的块）并擦除那些块的元数据。在与主服务器定期交换的心跳消息中，每个块服务器报告它所拥有的块的子集，并且主服务器将被删除元数据的所有块的标识发送到相应的块服务器，块服务器可以自由删除这些块的副本。&lt;/p&gt;
&lt;p&gt;存储回收的垃圾收集方法的优点。首先，它在组件故障常见的大规模分布式系统中简单可靠。创建块可能会在某些块服务器上成功，但在其他块服务器上却失败，留下了主服务器不知道存在的副本。数据块删除消息可能会丢失，主服务器必须记住在故障时重新发送它们，包括它自己的和块服务器的。垃圾收集提供了一种统一且可靠的方法来清理任何不知道是否有用的副本。其次，它将存储回收合并到主服务器的常规后台活动中，例如命名空间的常规扫描和与块服务器的交互。因此，该过程是分批完成的，成本是摊销的。而且，只有在主服务器相对自由的时候才这样做，这样主服务器可以更快速响应需要及时关注的客户端请求。第三，回收存储的延迟提供了一个防止意外的、不可逆的删除的安全网（使删除可以及时恢复）。&lt;/p&gt;
&lt;h3 id=&#34;陈旧副本检测&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#陈旧副本检测&#34;&gt;#&lt;/a&gt; 陈旧副本检测&lt;/h3&gt;
&lt;p&gt;如果一个块服务器故障失效了，并且在它故障的时候错过了对块副本的修改，那么这些块副本就可能会变得陈旧。对于每个块，主服务器维护一个块版本号，以区分最新的副本和过时的副本。&lt;/p&gt;
&lt;p&gt;每当主服务器在块上授予新的租约时，它会增加块版本号并通知最新副本。主服务器和这些副本服务器都在其持久状态下记录新版本号。这发生在任何客户端被通知之前，因此在它可以开始写入块之前，如果另一个副本当前不可用，则其块版本号将不会被提升。当块服务器重新启动时，主服务器将检测到这个块服务器有一个陈旧的副本，并报告它的一组块及其相关的版本号。如果主服务器看到的版本号大于该副本记录中的版本号，则主服务器会假定它在授予租约时失败，因此会采用最新版本的副本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主服务器在常规垃圾收集中删除过时的副本。在此之前，当它回复客户端对区块信息的请求时，它实际上会认为过时的副本根本不存在&lt;/strong&gt;。作为另一种保护措施，当主服务器通知客户端哪个块服务器持有块的租约时，或者当它指示块服务器在克隆操作中从另一个块服务器读取块时，主服务器包括块版本号。客户端或块服务器在执行操作时验证版本号，以便始终访问最新数据。&lt;/p&gt;
&lt;h2 id=&#34;容错与诊断&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#容错与诊断&#34;&gt;#&lt;/a&gt; 容错与诊断&lt;/h2&gt;
&lt;p&gt;设计系统时最大的挑战之一是处理频繁的组件故障。组件的质量和数量一起使这些问题变得更常见，而不是例外：我们不能完全信任机器，也不能完全信任磁盘。组件故障可能导致系统不可用，或者更糟糕的是，数据损坏。我们讨论如何应对这些挑战，以及我们在系统中内置的工具，以便在问题发生时对其进行诊断。&lt;/p&gt;
&lt;h3 id=&#34;高可用性&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#高可用性&#34;&gt;#&lt;/a&gt; 高可用性&lt;/h3&gt;
&lt;p&gt;在 GFS 集群中的数百台服务器中，有些服务器在任何给定时间都是不可用的。通过两个简单而有效的策略保持整个系统的高度可用性：&lt;strong&gt;快速恢复和复制&lt;/strong&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;快速恢复&lt;/p&gt;
&lt;p&gt;无论它们如何终止，主服务器和块服务器都被设计为恢复其状态并在几秒钟内启动。事实上，我们不区分正常和异常终止；服务器通常只是通过终止进程来关闭。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;块复制&lt;/p&gt;
&lt;p&gt;如前所述，每个块在不同机架上的多个块服务器上复制。用户可以为文件命名空间的不同部分指定不同的复制级别。主服务器根据需要克隆现有的副本，以保持每个块完全复制。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;主服务器复制&lt;/p&gt;
&lt;p&gt;复制主服务器状态以提高可靠性。它的操作日志和检查点在多台机器上复制。只有在日志记录被刷新到磁盘本地和所有主服务器副本上之后，状态的变化才被认为是已提交的。为了简单起见，一个主进程仍然负责所有的变化以及后台活动，如在内部改变系统的垃圾收集。当它失败时，它几乎可以立即重新启动。如果其计算机或磁盘出现故障，GFS 外部的监视基础结构将在其他位置启动一个新的主进程，并使用复制的操作日志。&lt;/p&gt;
&lt;p&gt;此外，即使在主服务器关闭时，“影子” 主机也提供对文件系统的只读访问。它们是影子，而不是镜子，因为它们可能会稍微滞后于主光，通常是几分之一秒。它们增强了未被主动改变的文件或不介意获得稍微陈旧结果的应用程序的读取可用性。事实上，由于文件内容是从 chunkserver 读取的，应用程序不会观察到陈旧的文件内容。&lt;/p&gt;
&lt;p&gt;为了使自己保持知情，影子主服务器读取不断增长的操作日志的副本，并将与主服务器完全相同的更改序列应用于其数据结构。与主服务器一样，它在启动时（此后很少）轮流交互 chunkserver 以定位 chunkreplicas，并与它们频繁交换心跳消息以监视它们的状态。它仅在由主服务器决定创建和删除副本，而导致的副本位置更新方面依赖于主服务器。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;数据完整性&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#数据完整性&#34;&gt;#&lt;/a&gt; 数据完整性&lt;/h3&gt;
&lt;p&gt;每个块服务器使用校验和来检测存储数据的损坏。由于 GFS 集群通常在数百台机器上有数千个磁盘，因此它经常会遇到磁盘故障，导致读取和写入路径上的数据损坏或丢失。我们可以使用其他块副本从损坏中恢复，但是通过比较块服务器上的副本来检测损坏是不切实际的。此外，不同的副本可能是独立的：GFS 变化的语义，特别是前面讨论的原子记录附加，不保证相同的副本。因此，每个块服务器都必须通过维护校验和来独立验证自己副本的完整性。&lt;/p&gt;
&lt;p&gt;每个块都有一个对应的 32 位校验和。与其他元数据一样，校验和保存在内存中，并与日志记录一起持久存储，与用户数据分开。&lt;/p&gt;
&lt;p&gt;对于读取，块服务器在将任何数据返回给请求者（无论是客户端还是另一个块服务器）之前，验证与读取范围重叠的数据块的校验和。因此，块服务器不会将损坏发送到其他服务器或客户端。如果块与记录的校验和不匹配，则块服务器向请求者返回错误，并向主服务器报告不匹配。作为响应，请求者将从其他副本读取，而主服务器将从另一个副本克隆块。在一个有效的新副本就位之后，主服务器指示报告不匹配的块服务器删除它的副本。&lt;/p&gt;
&lt;p&gt;校验和计算针对附加到块末尾的写入（而不是覆盖现有数据的写入）进行了大量优化，因为它们在我们的工作负载中占主导地位。我们只是递增更新最后一个部分校验和，并为任何由增加填充的全新校验和块计算新的校验和。即使最后一个部分校验和块已经损坏，我们现在无法检测到它，新的校验和值也不会与存储的数据匹配，并且在下一次读取块时会像往常一样检测到损坏。&lt;/p&gt;
&lt;p&gt;相反，如果写操作覆盖了块的现有范围，我们必须读取并验证被覆盖范围的第一个和最后一个块，然后执行写操作，最后计算并记录新的校验和。如果我们在部分覆盖前没有验证第一个和最后一个块，新的校验和可能会隐藏未被覆盖的区域中存在的损坏。&lt;/p&gt;
&lt;p&gt;在空闲期间，chunkserver 可以扫描和验证非活动块的内容，这使我们能够检测很少被读取的块中的损坏。一旦检测到损坏，主设备就可以创建新的未损坏的副本并删除损坏的副本。这可以防止一个不活动但已损坏的块副本欺骗主服务器，使其认为它有该块的有效副本。&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;bigtable-a-distributed-storage-system-for-structured-data&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#bigtable-a-distributed-storage-system-for-structured-data&#34;&gt;#&lt;/a&gt; 《Bigtable: A Distributed Storage System for Structured Data》&lt;/h1&gt;
&lt;p&gt;BigTable 支持客户端寻找存储中数据的位置属性，同时，允许客户端动态控制是从内存还是从磁盘选择数据。&lt;/p&gt;
&lt;h2 id=&#34;数据模型&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#数据模型&#34;&gt;#&lt;/a&gt; 数据模型&lt;/h2&gt;
&lt;p&gt;数据被组织成三个维度：行、列和时间戳，&lt;strong&gt;(row：string，column：string，time：int64) → string&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;将行键、列键和时间戳引用的存储称为单元格。将行分组在一起以形成负载平衡单元，将列分组在一起以形成访问控制和资源核算单元。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;行键&lt;/p&gt;
&lt;p&gt;Bigtable 按照行键的字典顺序维护数据。表中的行键是任意字符串。在单个行键下的每次数据读写都是可序列化的（由版本号控制），它使客户端在存在对同一行的并发更新时更容易推断系统的行为。换句话说，&lt;strong&gt;行是 Bigtable 中事务一致性的单位&lt;/strong&gt;，&lt;strong&gt;Bigtable 目前不支持跨行事务&lt;/strong&gt;。（自：只能对单表进行并发更新）&lt;strong&gt;Bigtable 通过行关键字的字典顺序来组织数据。表中的每个行都可以动态分区。每个分区叫做一个”Tablet”，Tablet 是数据分布和负载均衡调整的最小单位。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;存储方面&lt;/strong&gt;：具有连续键的磁盘被分组到小块（tablet）中，小块形成分布和负载平衡的单元。因此，短行范围的读取更高效，并且通常仅需要与少量机器通信（减少网络开销）。客户端可以通过选择行键来利用此属性，以便为数据访问获得&lt;strong&gt;良好的局部性&lt;/strong&gt;。例如，在 Webtable 中，通过反转 URL 的主机名组件，同一域中的页面被分组到连续的行中。&lt;a href=&#34;http://xn--maps-965fpjo21fs7k.google.com/index.html%E7%9A%84%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E5%9C%A8com.google.maps/index.html%E8%A1%8C%E4%B8%8B%E9%9D%A2%EF%BC%8C%E5%B0%86%E6%9D%A5%E8%87%AA%E5%90%8C%E4%B8%80%E4%B8%AA%E5%9F%9F%E7%9A%84%E9%A1%B5%E9%9D%A2%E5%AD%98%E5%82%A8%E5%9C%A8%E5%BD%BC%E6%AD%A4%E9%99%84%E8%BF%91%EF%BC%8C%E8%BF%99%E5%8F%AF%E4%BB%A5%E4%BD%BF%E6%9F%90%E4%BA%9B%E4%B8%BB%E6%9C%BA%E5%92%8C%E5%9F%9F%E5%88%86%E6%9E%90%E6%9B%B4%E6%9C%89%E6%95%88%E3%80%82&#34;&gt;我们会将 maps.google.com/index.html 的数据存储在 com.google.maps/index.html 行下面，将来自同一个域的页面存储在彼此附近，这可以使某些主机和域分析更有效。&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/image-20231115140811457.png&#34; alt=&#34;image-20231115140811457&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;列键&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;列键被分组到称为列族的集合中，这些集合构成访问控制的单元&lt;/strong&gt;。存储在列族中的所有数据通常都是相同类型的。创建一个族之后，可以使用族中的任何列键：数据可以存储在这样的列键下，而不会影响表的模式。表中不同列族的数量要小（最多几百个），并且族在操作过程中通常很少改变，这种限制使广泛共享的元数据不会太大（修改的增量和频率小，相关元数据也小）。相反，表可以具有无限数量的列。&lt;/p&gt;
&lt;p&gt;可以通过更改表的模式来删除整个列族，在这种情况下，存储在该族中任何列键下的数据都将被删除。然而，由于 Bigtable 不支持跨多行的事务，因此如果存储在特定列键下的数据驻留在多行中，则无法原子地删除该数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;列键使用以下语法命名：family：qualifier&lt;/strong&gt;（自：主域名：子域名）。family 名称必须是可打印的，但 qualifier 可以是任意字符串。Webtable 的一个示例列族，它存储编写网页时使用的语言。我们只使用一个带有空限定符（empty qualifier）的列键来存储每个网页的 ID（自：第一列列名为空，内容为网页地址 ID）。此表的另一个有用的列族是锚，此族中的每个列键表示单个锚，如图 1 所示。&lt;strong&gt;限定符是引用站点的名称，单元格包含与链接关联的文本&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;时间戳&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;表中的不同单元格可以包含同一数据的多个版本，其中版本按时间戳索引&lt;/strong&gt;。Bigtable 时间戳是 64 位整数。它们可以由 Bigtable 隐式分配，在这种情况下，它们表示以微秒为单位的 “真实的时间”，也可以由客户端应用程序显式分配。需要避免冲突的应用程序必须自己生成唯一的时间戳。&lt;strong&gt;单元格的不同版本按时间戳降序存储&lt;/strong&gt;，&lt;strong&gt;以便可以首先读取最新版本&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;为了使版本数据的管理不那么繁重，Bigtable 支持两个每列族设置，告诉 Bigtable 自动收集版本数据垃圾。客户端可以指定只保留数据的最后 n 个版本，或者只保留足够新的版本。&lt;/p&gt;
&lt;p&gt;在我们的 Webtable 示例中，我们可以将存储在 contents：列中的抓取页面的时间戳设置为实际抓取这些页面版本的时间。上面描述的垃圾收集机制使我们能够告诉 Bigtable 只保留每个页面的最新三个版本。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;api&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#api&#34;&gt;#&lt;/a&gt; API&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Bigtable 支持单行事务，可用于对存储在单行键下的数据执行原子读 - 修改 - 写序列。Bigtable 目前不支持跨行键的一般事务，尽管它提供了一个接口，用于在客户端跨行键进行重复写入。&lt;/li&gt;
&lt;li&gt;Bigtable 允许单元格用作整数计数器。&lt;/li&gt;
&lt;li&gt;Bigtable 支持在服务器的地址空间中执行客户端提供的脚本。这些脚本是用一种名为 Sawzall 的语言编写的，该语言是由 Google 开发的，用于处理数据。目前，我们基于 Sawzall 的 API 不允许客户端脚本写回 Bigtable，但它允许各种形式的数据转换，基于任意表达式的过滤，以及通过各种运算符的汇总。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;bigtable基础设施&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#bigtable基础设施&#34;&gt;#&lt;/a&gt; Bigtable 基础设施&lt;/h2&gt;
&lt;h3 id=&#34;gfs&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#gfs&#34;&gt;#&lt;/a&gt; GFS&lt;/h3&gt;
&lt;p&gt;Bigtable 使用 GFS 存储日志和数据文件。GFS 是一个分布式文件系统，它维护每个文件的多个副本，以提高可靠性和可用性。&lt;/p&gt;
&lt;h3 id=&#34;sstable&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#sstable&#34;&gt;#&lt;/a&gt; SSTable&lt;/h3&gt;
&lt;p&gt;Google SSTable 不可变文件格式在内部用于存储 Bigtable 数据文件。SSTable 提供了一个持久的、有序的、不可变的从键到值的映射，其中键和值都是任意字节字符串。&lt;strong&gt;SSTable 提供的操作用于查找与指定键关联的值&lt;/strong&gt;，&lt;strong&gt;以及遍历指定键范围内的所有键 / 值对&lt;/strong&gt;。在内部，每个 SSTable 都包含一个块序列（默认情况下，每个块的大小为 64 KB，但大小是可配置的）。块索引（存储在 SSTable 的末尾）用于定位块；当 SSTable 打开时，索引被加载到内存中。查找可以通过单个磁盘寻道来执行：我们首先&lt;strong&gt;通过在内存索引中执行二进制搜索来找到适当的块，然后从磁盘读取适当的块&lt;/strong&gt;。SSTable 可以完全映射到内存中，这允许我们在不接触磁盘的情况下执行查找和扫描。&lt;/p&gt;
&lt;h3 id=&#34;chubby&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#chubby&#34;&gt;#&lt;/a&gt; Chubby&lt;/h3&gt;
&lt;p&gt;Bigtable 依赖于一个高可用性和持久性的分布式锁服务，称为 Chubby 。一个 Chubby 服务由五个活动副本组成，其中一个被选为主服务器并主动服务请求。当大多数副本正在运行并且可以相互通信时，服务是活动的。Chubby 使用 Paxos 算法来保持其副本在面对失败时的一致性。Chubby 提供了一个由目录和小文件组成的命名空间。每个目录或文件都可以用作锁，对文件的读取和写入都是原子的。Chubby 客户端库提供 Chubby 文件的一致缓存。每一个 Chubby 客户端都维护一个与 Chubby 服务的会话。如果客户端无法在租约到期时间内续订其会话租约，则该客户端的会话将到期。当客户端的会话过期时，它将丢失所有锁和打开的句柄。Chubby 客户端还可以在 Chubby 文件和目录上注册回调，以通知更改或会话过期。&lt;/p&gt;
&lt;p&gt;Bigtable 使用 Chubby 来完成各种任务：确保任何时候最多有一个活动的主服务器；存储 Bigtable 数据的引导位置；发现 tablet 服务器并确定 tablet 服务器的死亡，以及存储 Bigtable 模式。如果 Chubby 长时间不可用，Bigtable 也将不可用。&lt;/p&gt;
&lt;h1 id=&#34;spanner-googles-globally-distributed-database&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#spanner-googles-globally-distributed-database&#34;&gt;#&lt;/a&gt; 《Spanner: Google’s Globally Distributed Database》&lt;/h1&gt;
&lt;h2 id=&#34;介绍&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#介绍&#34;&gt;#&lt;/a&gt; 介绍&lt;/h2&gt;
&lt;p&gt;spanner 可以随着数据量和服务器数量的增加，自动对数据进行跨节点分片和数据迁移，以达到负载均衡和故障转移。spanner 可以实现应用程序的高可用性，能应对大范围的自然灾害等故障。&lt;strong&gt;spanner 的重心是实现跨数据中心的数据强一致性和外部一致性，这些功能是通过 spanner 提供全局时间戳来实现的&lt;/strong&gt;。&lt;/p&gt;
&lt;h2 id=&#34;基本原理&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#基本原理&#34;&gt;#&lt;/a&gt; 基本原理&lt;/h2&gt;
&lt;p&gt;spanner 有一个全局管理器（universemaster）和放置驱动程序（placement driver），每个区域（Zones）均含有一个主服务器（zonemaster）、多个位置代理（location proxy）和跨服务器（spanserver）。每个区域中，zonemaster 将数据分配给 spanserver，spanserver 向客户端提供数据，客户端通过位置代理来定位对其服务的 spanserver。全局管理器存储所有 zone 的状态信息，放置驱动程序（placement driver）处理跨区域的自动数据移动，放置驱动程序定期与跨区服务器通信，以查找需要移动的数据，来满足更新的复制约束或平衡负载。（自：每个 zone 类似于磁盘，主要用于存储数据和向客户端提供数据，全局管理器和放置驱动程序类似于主服务器，负责调度和存储各区域的状态信息）&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/image-20231120165837699.png&#34; alt=&#34;image-20231120165837699&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;spanserver的实现&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#spanserver的实现&#34;&gt;#&lt;/a&gt; spanserver 的实现&lt;/h2&gt;
&lt;p&gt;与 Bigtable 不同，spanner 会为数据分配时间戳，在底层，每个 spanserver 负责多个 tablet 数据结构，tablet 的状态信息会存储在类似 B 树的文件和一个写前日志中，这些文件都存储在 Colossus 的文件系统中。spanserver 为每一个 tablet 分配一个 paxos，并且 paxos 将其元数据和日志存储到对应的 tablet 中。&lt;/p&gt;
&lt;p&gt;每个副本（replica）的键值映射状态存储在其对应的 tablet 中，写操作必须在 leader 上启动 Paxos 协议，在任何足够新的副本上直接从底层 tablet 读取访问状态。副本集合共同组成一个 Paxos 组。spanserver 为每一个副本加上锁表来实现并发控制。&lt;/p&gt;
&lt;p&gt;每个 spanserver 还实现了一个事务管理器来支持分布式事务。事务管理器用于实现参与领导者，组中的其他副本将被称为参与跟随者。如果一个事务只涉及一个 Paxos 组（大多数事务都是这样），那么它可以绕过事务管理器，因为锁表和 Paxos 一起提供事务性。如果事务涉及多个 Paxos 组，则这些组的领导者协调执行两阶段提交，选择一个参与者组作为协调器：该组的参与领导者将被称为协调器领导者，该组的跟随者将被称为协调器跟随者。每个事务管理器的状态都存储在底层 Paxos 组中。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/image-20231120184142870.png&#34; alt=&#34;image-20231120184142870&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;目录和放置&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#目录和放置&#34;&gt;#&lt;/a&gt; 目录和放置&lt;/h2&gt;
&lt;p&gt;spanner 将一组共享公共前缀的连续键设为一个&lt;strong&gt;目录&lt;/strong&gt;，应用程序可以通过目录选择键来放置和寻找其数据的位置。（自：目录类似于 B + 树）&lt;/p&gt;
&lt;p&gt;spanner 以目录为单位在 paxos 组之间进行移动（一个 paxos 组可以包含多个目录），将经常一起被访问的目录放在一组或将目录放置在离访问者最近的组。（自：目录加对应的数据一起移动）&lt;/p&gt;
&lt;p&gt;spanner 中的 tablet 与 Bigtable 的 tablet 不同，Bigtable 的 tablet 是行空间的单个词典顺序上连续的分区，而 spanner 的 tablet 是一个容器，可以封装行空间的多个分区。（自：spanner 将多个频繁访问的目录放在一起作为一个 tablet）&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/image-20231121152641902.png&#34; alt=&#34;image-20231121152641902&#34;&gt;&lt;/p&gt;
&lt;p&gt;spanner 移动目录通过增加一个后台任务 movedir，该任务在后台移动数据，以防止阻塞正常的写入和读取。（自：惰性移动，当没有任务执行时才移动数据）事实上，如果一个目录太大，Spanner 会将它分割成多个片段。片段可以从不同的 Paxos 组（因此是不同的服务器）提供服务。&lt;strong&gt;Movedir 实际上是在组之间移动片段，而不是整个目录&lt;/strong&gt;。&lt;/p&gt;
&lt;h2 id=&#34;并发控制&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#并发控制&#34;&gt;#&lt;/a&gt; 并发控制&lt;/h2&gt;
&lt;h3 id=&#34;时间戳管理&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#时间戳管理&#34;&gt;#&lt;/a&gt; 时间戳管理&lt;/h3&gt;
&lt;p&gt;外部一致性事务&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;快照事务&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;快照事务中的读取在系统选择的时间戳执行而不锁定，因此传入的写入不会被阻止。快照读取是过去的读取，它在没有锁定的情况下执行。客户端可以为快照读取指定时间戳，也可以提供所需时间戳过期时间的上限并让 Spanner 选择时间戳。在任何一种情况下，快照读取的执行都将在足够新的任何副本上继续。&lt;/p&gt;
&lt;p&gt;对于快照事务和快照读取，一旦选择了时间戳，提交是不可避免的，除非该时间戳处的数据已被垃圾收集。因此，客户端可以避免在重试循环内缓冲结果。当服务器出现故障时，客户端可以在内部通过重复时间戳和当前读取位置来在不同的服务器上继续查询。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;过去的非阻塞读取&lt;/strong&gt;&lt;/p&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">https://chendouxing.github.io/2023/09/06/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/</guid>
            <title>分布式理论</title>
            <link>https://chendouxing.github.io/2023/09/06/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/</link>
            <category>分布式</category>
            <category>论文</category>
            <pubDate>Wed, 06 Sep 2023 14:38:56 +0800</pubDate>
            <description><![CDATA[ &lt;link rel=&#34;stylesheet&#34; class=&#34;aplayer-secondary-style-marker&#34; href=&#34;\assets\css\APlayer.min.css&#34;&gt;&lt;script src=&#34;\assets\js\APlayer.min.js&#34; class=&#34;aplayer-secondary-script-marker&#34;&gt;&lt;/script&gt;&lt;h1 id=&#34;分布式理论&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#分布式理论&#34;&gt;#&lt;/a&gt; 分布式理论&lt;/h1&gt;
&lt;h2 id=&#34;cap理论&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#cap理论&#34;&gt;#&lt;/a&gt; CAP 理论&lt;/h2&gt;
&lt;h3 id=&#34;理论概述&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#理论概述&#34;&gt;#&lt;/a&gt; 理论概述&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;一个分布式系统最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）这三项中的两项&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&#34;一致性&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#一致性&#34;&gt;#&lt;/a&gt; 一致性&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;对于一致性，可以分为从客户端和服务端两个不同的视角。&lt;/p&gt;
&lt;p&gt;客户端：从客户端来看，一致性主要指的是多并发访问时更新过的数据如何获取的问题。&lt;/p&gt;
&lt;p&gt;服务端：从服务端来看，则是更新如何分布到整个系统，以保证数据最终一致。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对于一致性，可以分为强 / 弱 / 最终一致性三类&lt;/p&gt;
&lt;p&gt;从客户端角度，多进程并发访问时，更新过的数据在不同进程如何获取的不同策略，决定了不同的一致性。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;强一致性：对于关系型数据库，要求更新过的数据能被后续的访问都能看到，这是强一致性。&lt;/li&gt;
&lt;li&gt;弱一致性：如果能容忍后续的部分或者全部访问不到，则是弱一致性。&lt;/li&gt;
&lt;li&gt;最终一致性：如果经过一段时间后要求能访问到更新后的数据，则是最终一致性。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;可用性&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#可用性&#34;&gt;#&lt;/a&gt; 可用性&lt;/h3&gt;
&lt;p&gt;可用性指服务在正常响应时间内一直可用。&lt;/p&gt;
&lt;p&gt;好的可用性主要是指系统能够很好的为用户服务，不出现用户操作失败或者访问超时等用户体验不好的情况。可用性通常情况下与分布式数据冗余，负载均衡等有着很大的关联。&lt;/p&gt;
&lt;h3 id=&#34;分区容错性&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#分区容错性&#34;&gt;#&lt;/a&gt; 分区容错性&lt;/h3&gt;
&lt;p&gt;分区容错性指分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性或可用性的服务。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;base模型&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#base模型&#34;&gt;#&lt;/a&gt; BASE 模型&lt;/h2&gt;
&lt;h3 id=&#34;理论概述-2&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#理论概述-2&#34;&gt;#&lt;/a&gt; 理论概述&lt;/h3&gt;
&lt;p&gt;Base 理论是三要素的缩写：基本可用（Basically Available）、软状态（Soft-state）、最终一致性（Eventually Consistency）。&lt;/p&gt;
&lt;h3 id=&#34;基本可用&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#基本可用&#34;&gt;#&lt;/a&gt; 基本可用&lt;/h3&gt;
&lt;p&gt;“基本可用” 要求系统能够基本运行，一直提供服务，强调的是分布式系统在出现不可预知故障的时候，允许损失部分可用性。&lt;/p&gt;
&lt;h3 id=&#34;软状态&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#软状态&#34;&gt;#&lt;/a&gt; 软状态&lt;/h3&gt;
&lt;p&gt;相对于 ACID 事务中原子性要求（要么做，要么不做），强调的是强制一致性，要求多个节点的数据副本是一致的，强调数据的一致性。这种原子性可以理解为” 硬状态 “。&lt;/p&gt;
&lt;p&gt;软状态则允许系统中的数据存在中间状态，并认为该状态不影响系统的整体可用性，即允许系统在不同节点的数据副本上存在数据延时。&lt;/p&gt;
&lt;h3 id=&#34;最终一致性&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#最终一致性&#34;&gt;#&lt;/a&gt; 最终一致性&lt;/h3&gt;
&lt;p&gt;数据不可能一直处于软状态，必须在一个时间期限后达到各个节点的一致性。在期限过后，应当保证所有副本中的数据保持一致性，也就是达到了数据的最终一致性。&lt;/p&gt;
&lt;p&gt;在系统设计中，最终一致性实现的时间取决于网络延时、系统负载、不同的存储选型，不同数据复制方案设计等因素。也就是说，谁都不保证用户什么时候能看到更新好的数据，但是总会看到的。&lt;/p&gt;
&lt;p&gt;最终一致性作为弱一致性中的特例，强调的是所有数据副本，在经过一段时间的同步后，最终能够到达一致的状态，不需要实时保证系统数据的强一致性，而到达最终一致性。&lt;/p&gt;
&lt;p&gt;根据业务需求的不同，最终一致性中又有很多种情况：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;因果一致性：要求有因果关系的操作顺序得到保证，非因果关系的操作顺序则无所谓。例如微信朋友圈的评论以及对评论的答复所构成的因果关系。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;会话一致性：在操作顺序得到保证的前提下，保证用户在同一个会话里读取数据时保证数据是最新的，如分布式系统 Session 一致性解决方案。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;单调读一致性：用户读取某个数据值后，其后续操作不会读取到该数据更早版本的值。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;单调写一致性：要求数据的所有副本，以相同的顺序执行所有的更新操作，也称为时间轴一致性。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/images/v2-3d4420b4d4eb057f590da858ba6c7523_r.jpg&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;两阶段提交2pc&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#两阶段提交2pc&#34;&gt;#&lt;/a&gt; 两阶段提交（2PC）&lt;/h2&gt;
&lt;p&gt;在分布式系统中，为了让每个节点都能够感知到其他节点的事务执行状况，需要引入一个中心节点来统一处理所有节点的执行逻辑，这个中心节点叫做协调者（coordinator），被中心节点调度的其他业务节点叫做参与者（participant）。&lt;/p&gt;
&lt;p&gt;2PC 将分布式事务分成了两个阶段，两个阶段分别为提交请求（投票）和提交（执行）。协调者根据参与者的响应来决定是否需要真正地执行事务。具体步骤如下：&lt;/p&gt;
&lt;h3 id=&#34;提交请求投票阶段&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#提交请求投票阶段&#34;&gt;#&lt;/a&gt; 提交请求（投票）阶段&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;协调者向所有参与者发送 prepare 请求与事务内容，询问是否可以准备事务提交，并等待参与者的响应。&lt;/li&gt;
&lt;li&gt;参与者执行事务中包含的操作，并记录 undo 日志（用于回滚）和 redo 日志（用于重做），但不真正提交。&lt;/li&gt;
&lt;li&gt;参与者向协调者返回事务操作的执行结果，执行成功返回 yes，否则返回 no。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;提交执行阶段&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#提交执行阶段&#34;&gt;#&lt;/a&gt; 提交（执行）阶段&lt;/h3&gt;
&lt;p&gt;分为成功和失败两种情况：&lt;/p&gt;
&lt;p&gt;若所有参与者都返回 yes，说明事务可以提交：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;协调者向所有参与者发送提交（commit）请求。&lt;/li&gt;
&lt;li&gt;参与者收到提交（commit）请求后，将事务真正地提交上去，并释放占用的事务资源，并向协调者返回 ack。&lt;/li&gt;
&lt;li&gt;协调者收到所有参与者的 ack 消息，事务成功完成。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;若有参与者返回 no 或者超时未返回，说明事务中断，需要回滚：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;协调者向所有参与者发送 rollback（回滚）请求。&lt;/li&gt;
&lt;li&gt;参与者收到 rollback 请求后，根据 undo 日志回滚到事务执行前的状态，释放占用的事务资源，并向协调者返回 ack。&lt;/li&gt;
&lt;li&gt;协调者收到所有参与者的 ack 消息，事务回滚完成。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2pc的缺点&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#2pc的缺点&#34;&gt;#&lt;/a&gt; 2PC 的缺点&lt;/h3&gt;
&lt;p&gt;1、协调者存在单点问题。如果协调者挂了，整个 2PC 逻辑就彻底不能运行。&lt;/p&gt;
&lt;p&gt;2、执行过程是完全同步的。各参与者在等待其他参与者响应的过程中都处于阻塞状态，大并发下有性能问题。&lt;/p&gt;
&lt;p&gt;3、仍然存在不一致风险。如果由于网络异常等意外导致只有部分参与者收到了 commit 请求，就会造成部分参与者提交了事务而其他参与者未提交的情况。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;三阶段提交3pc&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#三阶段提交3pc&#34;&gt;#&lt;/a&gt; 三阶段提交（3PC）&lt;/h2&gt;
&lt;h3 id=&#34;cancommit-阶段&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#cancommit-阶段&#34;&gt;#&lt;/a&gt; CanCommit 阶段&lt;/h3&gt;
&lt;p&gt;类似于 2PC 的准备（第一）阶段。协调者向参与者发送 commit 请求，参与者如果可以提交就返回 Yes 响应，否则返回 No 响应。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;事务询问： 协调者向参与者发送 CanCommit 请求，询问是否可以执行事务提交操作，然后开始等待参与者的响应。&lt;/li&gt;
&lt;li&gt;响应反馈： 参与者接到 CanCommit 请求之后，正常情况下， 如果其自身认为可以顺利执行事务，则返回 Yes 响应，并进入预备状态， 否则返回 No。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;precommit-阶段&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#precommit-阶段&#34;&gt;#&lt;/a&gt; PreCommit 阶段&lt;/h3&gt;
&lt;p&gt;协调者根据参与者的反应情况来决定是否可以执行事务的 PreCommit 操作，根据响应情况，有以下两种可能：&lt;/p&gt;
&lt;p&gt;如果响应 Yes，则：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;发送预提交请求：协调者向参与者发送 PreCommit 请求，并进入 Prepared 阶段。&lt;/li&gt;
&lt;li&gt;事务预提交：参与者接收到 PreCommit 请求后，会执行事务操作，并将 undo 和 redo 信息记录到事务日志中。&lt;/li&gt;
&lt;li&gt;响应反馈：如果参与者成功的执行事务操作，则返回 ack 响应，同时开始等待最终指令。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;假如有任何一个参与者向协调者发送 No 响应，或者等待超时之后，协调者都没有接到参与者的响应，那么就执行事务的中断。则：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;发送中断请求：协调者向所有参与者发送 abort 请求。&lt;/li&gt;
&lt;li&gt;中断事务：参与者收到来自协调者的 abort 请求之后（或超时之后，仍未收到协调者的请求），执行事务的中断。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;docommit-阶段&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#docommit-阶段&#34;&gt;#&lt;/a&gt; doCommit 阶段&lt;/h3&gt;
&lt;p&gt;该阶段进行真正的事务提交，也可以分为执行提交和中断事务两种情况。&lt;/p&gt;
&lt;p&gt;如果执行成功，则有如下操作：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;发送提交请求&lt;/p&gt;
&lt;p&gt;协调者接收到参与者发送的 ack 响应，那么它将从预提交状态进入到提交状态，并向所有参与者发送 doCommit 请求。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;事务提交&lt;/p&gt;
&lt;p&gt;参与者接收到 doCommit 请求之后，执行正式的事务提交，并在完成事务提交之后释放所有事务资源。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;响应反馈&lt;/p&gt;
&lt;p&gt;事务提交完之后，向协调者发送 ack 响应。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;完成事务&lt;/p&gt;
&lt;p&gt;协调者接收到所有参与者的 ack 响应之后，完成事务。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;协调者没有接收到参与者发送的 ACK 响应（可能是接受者发送的不是 ACK 响应，也可能响应超时），那么就会执行中断事务。则有如下操作：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;发送中断请求&lt;/p&gt;
&lt;p&gt;协调者向所有参与者发送 abort 请求&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;事务回滚&lt;/p&gt;
&lt;p&gt;参与者接收到 abort 请求之后，利用其在阶段二记录的 undo 信息来执行事务的回滚操作，并在完成回滚之后释放所有的事务资源。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;反馈结果&lt;/p&gt;
&lt;p&gt;参与者完成事务回滚之后，向协调者发送 ACK 消息&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;中断事务&lt;/p&gt;
&lt;p&gt;协调者接收到参与者反馈的 ACK 消息之后，执行事务的中断。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;3pc的缺点&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#3pc的缺点&#34;&gt;#&lt;/a&gt; 3PC 的缺点&lt;/h3&gt;
&lt;p&gt;相对于 2PC，3PC 主要解决的单点故障问题，并减少阻塞，因为一旦参与者无法及时收到来自协调者的信息之后，他会默认执行 commit，而不会一直持有事务资源并处于阻塞状态。但是这种机制也会导致数据一致性问题，因为，由于网络原因，协调者发送的 abort 响应没有及时被参与者接收到，那么参与者在等待超时之后执行了 commit 操作，这样就和其他接到 abort 命令并执行回滚的参与者之间存在数据不一致的情况。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;多版本并发控制mvcc原理&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#多版本并发控制mvcc原理&#34;&gt;#&lt;/a&gt; 多版本并发控制（MVCC）原理&lt;/h2&gt;
&lt;p&gt;参考&lt;a href=&#34;https://juejin.cn/post/7016165148020703246&#34;&gt;看一遍就理解：MVCC 原理详解 - 掘金 (juejin.cn)&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;事务并发存在的问题&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#事务并发存在的问题&#34;&gt;#&lt;/a&gt; 事务并发存在的问题&lt;/h3&gt;
&lt;p&gt;事务并发会引起&lt;strong&gt;脏读、不可重复读、幻读&lt;/strong&gt;问题。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;脏读：如果一个事务读取到了另一个事务未提交修改过的数据，称发生了&lt;strong&gt;脏读&lt;/strong&gt;现象。&lt;/li&gt;
&lt;li&gt;不可重复读：同一个事务内，前后多次读取，读取到的数据内容不一致。在事务 A 范围内，两个相同的查询，读取同一条记录，却返回了不同的数据，这称为&lt;strong&gt;不可重复读&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;幻读：如果一个事务先根据某些搜索条件查询出一些记录，在该事务未提交时，另一个事务写入了一些符合那些搜索条件的记录（如 insert、delete、update），就意味着发生了&lt;strong&gt;幻读&lt;/strong&gt;。事务 A 查询一个范围的结果集，另一个并发事务 B 往这个范围中插入新的数据，并提交事务，然后事务 A 再次查询相同的范围，两次读取到的结果集却不一样了，这就是&lt;strong&gt;幻读&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;四大隔离级别&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#四大隔离级别&#34;&gt;#&lt;/a&gt; 四大隔离级别&lt;/h3&gt;
&lt;p&gt;为了解决并发事务存在的&lt;strong&gt;脏读、不可重复读、幻读&lt;/strong&gt;等问题，数据库设计了四种隔离级别，分别是&lt;strong&gt;读未提交，读已提交，可重复读，串行化（Serializable）&lt;/strong&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;读未提交&lt;/p&gt;
&lt;p&gt;读未提交隔离级别，只限制了两个数据&lt;strong&gt;不能同时修改&lt;/strong&gt;，但是修改数据的时候，即使事务未提交，都是可以被别的事务读取到的，这级别的事务隔离有脏读、重复读、幻读的问题；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;读已提交&lt;/p&gt;
&lt;p&gt;读已提交隔离级别，当前事务只能读取到其他事务&lt;strong&gt;提交&lt;/strong&gt;的数据，所以这种事务的隔离级别&lt;strong&gt;解决了脏读&lt;/strong&gt;问题，但还是会存在&lt;strong&gt;重复读、幻读&lt;/strong&gt;问题；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;可重复读&lt;/p&gt;
&lt;p&gt;可重复读隔离级别，限制了读取数据的时候，不可以进行修改，所以&lt;strong&gt;解决了重复读&lt;/strong&gt;的问题，但是读取范围数据的时候，是可以插入数据，所以还会存在&lt;strong&gt;幻读&lt;/strong&gt;问题；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;可串行化&lt;/p&gt;
&lt;p&gt;事务最高的隔离级别，在该级别下，所有事务都是进行&lt;strong&gt;串行化顺序&lt;/strong&gt;执行的。可以避免脏读、不可重复读与幻读所有并发问题。但是这种事务隔离级别下，事务执行很耗性能。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/images/image-20230907185451838.png&#34; alt=&#34;image-20230907185451838&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;mvcc概述&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#mvcc概述&#34;&gt;#&lt;/a&gt; MVCC 概述&lt;/h3&gt;
&lt;p&gt;通俗的说，数据库中同时存在多个版本的数据，并不是整个数据库的多个版本，而是某一条记录的多个版本同时存在，在某个事务对其进行操作的时候，需要查看这一条记录的隐藏列事务版本 id，比对事务 id 并根据事物隔离级别去判断读取哪个版本的数据。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;事务版本号&lt;/p&gt;
&lt;p&gt;事务每次开启前，都会从数据库获得一个&lt;strong&gt;自增长&lt;/strong&gt;的事务 ID，可以从事务 ID 判断事务的执行先后顺序，这就是事务版本号。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;隐式字段&lt;/p&gt;
&lt;p&gt;对于 InnoDB 存储引擎，每一行记录都有两个隐藏列&lt;strong&gt; trx_id&lt;/strong&gt;、&lt;strong&gt;roll_pointer&lt;/strong&gt;，如果表中没有主键和非 NULL 唯一键时，则还会有第三个隐藏的主键列&lt;strong&gt; row_id&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/image-20230907190609258.png&#34; alt=&#34;image-20230907190609258&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;undo log&lt;/p&gt;
&lt;p&gt;undo log，&lt;strong&gt;回滚日志&lt;/strong&gt;，用于记录数据被修改前的信息。在表记录修改之前，会先把数据拷贝到 undo log 里，如果事务回滚，即可以通过 undo log 来还原数据。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;版本链&lt;/p&gt;
&lt;p&gt;多个事务并行操作某一行数据时，不同事务对该行数据的修改会产生多个版本，然后通过回滚指针（roll_pointer），连成一个链表，这个链表就称为&lt;strong&gt;版本链&lt;/strong&gt;。通过版本链，我们就可以看出&lt;strong&gt;事务版本号、表格隐藏的列和 undo log&lt;/strong&gt; 它们之间的关系。如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/69abb7c89d1e4d9f8e242d9e0a410e3e~tplv-k3u1fbpfcp-zoom-i&#34; alt=&#34;版本链&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;快照读和当前读&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;快照读：&lt;/strong&gt; 读取的是记录数据的可见版本（有旧的版本）。不加锁，普通的 select 语句都是快照读。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;当前读&lt;/strong&gt;：读取的是记录数据的最新版本，显式加锁的都是当前读。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Read View&lt;/p&gt;
&lt;p&gt;事务执行 SQL 语句时，产生的读视图。实际上在 innodb 中，每个 SQL 语句执行前都会得到一个 Read View。它主要是用来做可见性判断的，即判断当前事务可见哪个版本的数据。&lt;/p&gt;
&lt;p&gt;相关参数：&lt;/p&gt;
&lt;p&gt;m_ids：当前系统中那些活跃 (未提交) 的读写事务 ID, 它数据结构为一个 List。&lt;/p&gt;
&lt;p&gt;min_limit_id：表示在生成 ReadView 时，当前系统中活跃的读写事务中最小的事务 id，即 m_ids 中的最小值。&lt;/p&gt;
&lt;p&gt;max_limit_id：表示生成 ReadView 时，系统中应该分配给下一个事务的 id 值。&lt;/p&gt;
&lt;p&gt;creator_trx_id：创建当前 read view 的事务 ID&lt;/p&gt;
&lt;p&gt;Read View 匹配规则：&lt;/p&gt;
&lt;p&gt;1、如果数据事务 ID  &lt;code&gt;trx_id &amp;lt; min_limit_id&lt;/code&gt; ，表明生成该版本的事务在生成 Read View 前，已经提交 (因为事务 ID 是递增的)，所以该版本可以被当前事务访问。&lt;/p&gt;
&lt;p&gt;2、如果 &lt;code&gt;trx_id&amp;gt;= max_limit_id&lt;/code&gt; ，表明生成该版本的事务在生成 ReadView 后才生成，所以该版本不可以被当前事务访问。&lt;/p&gt;
&lt;p&gt;3、如果  &lt;code&gt;min_limit_id =&amp;lt;trx_id&amp;lt; max_limit_id&lt;/code&gt; ，需要分 3 种情况讨论：&lt;/p&gt;
&lt;p&gt;（1）如果 &lt;code&gt;m_ids&lt;/code&gt;  包含 &lt;code&gt;trx_id&lt;/code&gt; , 则代表 Read View 生成时刻，这个事务还未提交，但是如果数据的 &lt;code&gt;trx_id&lt;/code&gt;  等于 &lt;code&gt;creator_trx_id&lt;/code&gt;  的话，表明数据是自己生成的，因此是&lt;strong&gt;可见&lt;/strong&gt;的。&lt;/p&gt;
&lt;p&gt;（2）如果 &lt;code&gt;m_ids&lt;/code&gt;  包含 &lt;code&gt;trx_id&lt;/code&gt; ，并且 &lt;code&gt;trx_id&lt;/code&gt;  不等于 &lt;code&gt;creator_trx_id&lt;/code&gt; ，则 Read   View 生成时，事务未提交，并且不是自己生产的，所以当前事务也是&lt;strong&gt;看不见&lt;/strong&gt;的；&lt;/p&gt;
&lt;p&gt;（3）如果 &lt;code&gt;m_ids&lt;/code&gt;  不包含 &lt;code&gt;trx_id&lt;/code&gt; ，则说明你这个事务在 Read View 生成之前就已经提交了，修改的结果，当前事务是能看见的。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;mvcc实现原理&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#mvcc实现原理&#34;&gt;#&lt;/a&gt; MVCC 实现原理&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;基于 MVCC 查询一条记录&lt;/p&gt;
&lt;p&gt;1、获取事务自己的版本号，即事务 ID；&lt;/p&gt;
&lt;p&gt;2、获取 Read View；&lt;/p&gt;
&lt;p&gt;3、查询得到的数据，然后 Read View 中的事务版本号进行比较；&lt;/p&gt;
&lt;p&gt;4、如果不符合 Read View 的可见性规则， 即就需要 Undo log 中历史快照；&lt;/p&gt;
&lt;p&gt;5、最后返回符合规则的数据。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;InnoDB 实现 MVCC，是通过 &lt;code&gt; Read View+ Undo Log&lt;/code&gt;  实现的，Undo Log 保存了历史快照，Read View 可见性规则帮助判断当前版本的数据是否可见。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;读已提交（RC）隔离级别，存在不可重复读问题&lt;/p&gt;
&lt;p&gt;在&lt;strong&gt;读已提交（RC）隔离级别&lt;/strong&gt;下，同一个事务里，两个相同的查询，读取同一条记录（id=1），却返回了不同的数据（&lt;strong&gt;第一次查出来是孙权，第二次查出来是曹操那条记录&lt;/strong&gt;），因此 RC 隔离级别，存在&lt;strong&gt;不可重复读&lt;/strong&gt;并发问题。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;可重复读（RR）隔离级别，解决不可重复读问题&lt;/p&gt;
&lt;p&gt;RR 可以解决不可重复读问题，就是跟 Read view 工作方式有关。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在读已提交（RC）隔离级别下，同一个事务里面，&lt;strong&gt;每一次查询都会产生一个新的 Read View 副本&lt;/strong&gt;，这样就可能造成同一个事务里前后读取数据可能不一致的问题（不可重复读并发问题）。&lt;/li&gt;
&lt;li&gt;在可重复读（RR）隔离级别下，&lt;strong&gt;一个事务里只会获取一次 read view&lt;/strong&gt;，都是副本共用的，从而保证每次查询的数据都是一样的。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;一致性算法&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#一致性算法&#34;&gt;#&lt;/a&gt; 一致性算法&lt;/h2&gt;
&lt;p&gt;1、为什么需要一致性算法？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;数据不能存在单个节点（主机）上，否则可能出现单点故障。&lt;/li&gt;
&lt;li&gt;多个节点（主机）需要保证具有相同的数据。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2、一致性分类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;强一致性&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;说明：保证系统改变提交以后立即改变集群的状态。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;模型：&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;Paxos&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;Raft（muti-paxos）&lt;/li&gt;
&lt;li&gt;ZAB（muti-paxos）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;弱一致性&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;说明：也叫最终一致性，系统不保证改变提交以后立即改变集群的状态，但是随着时间的推移最终状态是一致的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;模型：&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;DNS 系统&lt;/li&gt;
&lt;li&gt;Gossip 协议&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;paxos算法&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#paxos算法&#34;&gt;#&lt;/a&gt; Paxos 算法&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://juejin.cn/post/7202557470341349431&#34;&gt;图解 paxos 论文《The Part-Time Parliament》 - 掘金 (juejin.cn)&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;算法流程&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#算法流程&#34;&gt;#&lt;/a&gt; 算法流程&lt;/h4&gt;
&lt;p&gt;Paxos 算法解决的问题正是分布式一致性问题，即一个分布式系统中的各个进程如何就某个值（决议）达成一致。&lt;/p&gt;
&lt;p&gt;Paxos 算法运行在允许宕机故障的异步系统中，不要求可靠的消息传递，可容忍消息丢失、延迟、乱序以及重复。它利用大多数 (Majority) 机制保证了 2F+1 的容错能力，即 2F+1 个节点的系统最多允许 F 个节点同时出现故障。&lt;/p&gt;
&lt;p&gt;一个或多个提议进程 (Proposer) 可以发起提案 (Proposal)，Paxos 算法使所有提案中的某一个提案，在所有进程中达成一致。系统中的多数派同时认可该提案，即达成了一致。最多只针对一个确定的提案达成一致。&lt;/p&gt;
&lt;p&gt;Paxos 将系统中的角色分为提议者 (Proposer)，决策者 (Acceptor)，和最终决策学习者 (Learner):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Proposer&lt;/strong&gt;: 提出提案 (Proposal)。Proposal 信息包括提案编号 (Proposal ID) 和提议的值 (Value)。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Acceptor&lt;/strong&gt;：参与决策，回应 Proposers 的提案。收到 Proposal 后可以接受提案，若 Proposal 获得多数 Acceptors 的接受，则称该 Proposal 被批准。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learner&lt;/strong&gt;：不参与决策，从 Proposers/Acceptors 学习最新达成一致的提案（Value）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Paxos 算法分为&lt;strong&gt;两个阶段&lt;/strong&gt;。具体如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;阶段一：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;(a) Proposer 选择一个&lt;strong&gt;提案编号 N&lt;/strong&gt;，然后向&lt;strong&gt;半数以上&lt;/strong&gt;的 Acceptor 发送编号为 N 的&lt;strong&gt; Prepare 请求&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;(b) 如果一个 Acceptor 收到一个编号为 N 的 Prepare 请求，且 N&lt;strong&gt; 大于&lt;/strong&gt;该 Acceptor 已经&lt;strong&gt;响应过的&lt;/strong&gt;所有&lt;strong&gt; Prepare 请求&lt;/strong&gt;的编号，那么它就会将它已经&lt;strong&gt;接受过的编号最大的提案（如果有的话）&lt;strong&gt;作为响应反馈给 Proposer，同时该 Acceptor 承诺&lt;/strong&gt;不再接受&lt;/strong&gt;任何&lt;strong&gt;编号小于 N 的提案&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;阶段二：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;(a) 如果 Proposer 收到&lt;strong&gt;半数以上&lt;/strong&gt; Acceptor 对其发出的编号为 N 的 Prepare 请求的&lt;strong&gt;响应&lt;/strong&gt;，那么它就会发送一个针对 **[N,V] 提案&lt;strong&gt;的&lt;/strong&gt; Accept 请求&lt;strong&gt;给&lt;/strong&gt;半数以上&lt;strong&gt;的 Acceptor。注意：V 就是收到的&lt;/strong&gt;响应&lt;strong&gt;中&lt;/strong&gt;编号最大的提案的 value**，如果响应中&lt;strong&gt;不包含任何提案&lt;/strong&gt;，那么 V 就由 Proposer&lt;strong&gt; 自己决定&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;(b) 如果 Acceptor 收到一个针对编号为 N 的提案的 Accept 请求，只要该 Acceptor&lt;strong&gt; 没有&lt;/strong&gt;对编号&lt;strong&gt;大于 N&lt;/strong&gt; 的&lt;strong&gt; Prepare 请求&lt;/strong&gt;做出过&lt;strong&gt;响应&lt;/strong&gt;，它就&lt;strong&gt;接受该提案&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;raft算法&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#raft算法&#34;&gt;#&lt;/a&gt; Raft 算法&lt;/h3&gt;
&lt;p&gt;Raft 将系统中的角色分为领导者（Leader）、跟随者（Follower）和候选人（Candidate）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Leader&lt;/strong&gt;：接受客户端请求，并向 Follower 同步请求日志，当日志同步到大多数节点上后告诉 Follower 提交日志。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Follower&lt;/strong&gt;：接受并持久化 Leader 同步的日志，在 Leader 告之日志可以提交之后，提交日志。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Candidate&lt;/strong&gt;：Leader 选举过程中的临时角色。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;概述&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#概述&#34;&gt;#&lt;/a&gt; 概述&lt;/h4&gt;
&lt;p&gt;Raft 要求系统在任意时刻最多只有一个 Leader，正常工作期间只有 Leader 和 Followers。Follower 只响应其他服务器的请求。如果 Follower 超时没有收到 Leader 的消息，它会成为一个 Candidate 并且开始一次 Leader 选举。收到大多数服务器投票的 Candidate 会成为新的 Leader。Leader 在宕机之前会一直保持 Leader 的状态。&lt;/p&gt;
&lt;p&gt;Raft 算法将时间分为一个个的任期（term），每一个 term 的开始都是 Leader 选举。在成功选举 Leader 之后，Leader 会在整个 term 内管理整个集群。如果 Leader 选举失败，该 term 就会因为没有 Leader 而结束。&lt;/p&gt;
&lt;h4 id=&#34;leader选举&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#leader选举&#34;&gt;#&lt;/a&gt; Leader 选举&lt;/h4&gt;
&lt;p&gt;Raft 使用心跳（heartbeat）触发 Leader 选举。当服务器启动时，初始化为 Follower。Leader 向所有 Followers 周期性发送 heartbeat。如果 Follower 在选举超时时间内没有收到 Leader 的 heartbeat，就会等待一段随机的时间后发起一次 Leader 选举。&lt;/p&gt;
&lt;p&gt;Follower 将其当前 term 加一然后转换为 Candidate。它首先给自己投票并且给集群中的其他服务器发送 RequestVote RPC。结果有以下三种情况：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;赢得了多数的选票，成功选举为 Leader；&lt;/li&gt;
&lt;li&gt;收到了 Leader 的消息，表示有其它服务器已经抢先当选了 Leader；&lt;/li&gt;
&lt;li&gt;没有服务器赢得多数的选票，Leader 选举失败，等待选举时间超时后发起下一次选举。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;选举出 Leader 后，Leader 通过定期向所有 Followers 发送心跳信息维持其统治。若 Follower 一段时间未收到 Leader 的心跳则认为 Leader 可能已经挂了，再次发起 Leader 选举过程。&lt;strong&gt;Raft 保证选举出的 Leader 上一定具有最新的已提交的日志&lt;/strong&gt;。&lt;/p&gt;
&lt;h4 id=&#34;日志同步&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#日志同步&#34;&gt;#&lt;/a&gt; 日志同步&lt;/h4&gt;
&lt;p&gt;Leader 选出后，就开始接收客户端的请求。Leader 把请求作为日志条目（Log entries）加入到它的日志中，然后并行的向其他服务器发起 AppendEntries RPC 复制日志条目。当这条日志被复制到大多数服务器上，Leader 将这条日志应用到它的状态机并向客户端返回执行结果。（某些 Followers 可能没有成功的复制日志，Leader 会无限的重试 AppendEntries RPC 直到所有的 Followers 最终存储了所有的日志条目）&lt;/p&gt;
&lt;p&gt;日志由有序编号（log index）的日志条目组成。每个日志条目包含它被创建时的任期号（term），和用于状态机执行的命令。如果一个日志条目被复制到大多数服务器上，就被认为可以提交（commit）了。&lt;/p&gt;
&lt;p&gt;Leader 通过强制 Followers 复制它的日志来处理日志的不一致，Followers 上的不一致的日志会被 Leader 的日志覆盖。&lt;/p&gt;
&lt;h4 id=&#34;安全性&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#安全性&#34;&gt;#&lt;/a&gt; 安全性&lt;/h4&gt;
&lt;p&gt;Raft 增加了如下两条限制以保证安全性：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;拥有最新的已提交的 log entry 的 Follower 才有资格成为 Leader。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;这个保证是在 RequestVote RPC 中做的，Candidate 在发送 RequestVote RPC 时，要带上自己的最后一条日志的 term 和 log index，其他节点收到消息时，如果发现自己的日志比请求中携带的更新，则拒绝投票。日志比较的原则是，如果本地的最后一条 log entry 的 term 更大，则 term 大的更新，如果 term 一样大，则 log index 更大的更新。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Leader 只能推进 commit index 来提交当前 term 的已经复制到大多数服务器上的日志，旧 term 日志的提交要等到提交当前 term 的日志来间接提交（log index 小于 commit index 的日志被间接提交）&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;之所以要这样，是因为可能会出现已提交的日志又被覆盖的情况。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;日志压缩&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#日志压缩&#34;&gt;#&lt;/a&gt; 日志压缩&lt;/h4&gt;
&lt;p&gt;在实际的系统中，不能让日志无限增长，否则系统重启时需要花很长的时间进行回放，从而影响可用性。Raft 采用对整个系统进行 snapshot 来解决，snapshot 之前的日志都可以丢弃。&lt;/p&gt;
&lt;p&gt;每个副本独立的对自己的系统状态进行 snapshot，并且只能对已经提交的日志记录进行 snapshot。&lt;/p&gt;
&lt;h4 id=&#34;成员变更&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#成员变更&#34;&gt;#&lt;/a&gt; 成员变更&lt;/h4&gt;
&lt;p&gt;成员变更是在集群运行过程中副本发生变化，如增加 / 减少副本数、节点替换等。&lt;/p&gt;
&lt;p&gt;如果将成员变更当成一般的一致性问题，直接向 Leader 发送成员变更请求，Leader 复制成员变更日志，达成多数派之后提交，各服务器提交成员变更日志后从旧成员配置（Cold）切换到新成员配置（Cnew）。因为各个服务器提交成员变更日志的时刻可能不同，造成各个服务器从旧成员配置（Cold）切换到新成员配置（Cnew）的时刻不同。成员变更不能影响服务的可用性，但是成员变更过程的某一时刻，可能出现在 Cold 和 Cnew 中同时存在两个不相交的多数派，进而可能选出两个 Leader，形成不同的决议，破坏安全性。&lt;/p&gt;
&lt;p&gt;由于成员变更的这一特殊性，成员变更不能当成一般的一致性问题去解决。&lt;/p&gt;
&lt;p&gt;为了解决这一问题，Raft 提出了两阶段的成员变更方法。集群先从旧成员配置 Cold 切换到一个过渡成员配置，称为共同一致（joint consensus），共同一致是旧成员配置 Cold 和新成员配置 Cnew 的组合 Cold U Cnew，一旦共同一致 Cold U Cnew 被提交，系统再切换到新成员配置 Cnew。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Raft 两阶段成员变更过程如下&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Leader 收到成员变更请求从 Cold 切成 Cold,new；&lt;/li&gt;
&lt;li&gt;Leader 在本地生成一个新的 log entry，其内容是 Cold∪Cnew，代表当前时刻新旧成员配置共存，写入本地日志，同时将该 log entry 复制至 Cold∪Cnew 中的所有副本。在此之后新的日志同步需要保证得到 Cold 和 Cnew 两个多数派的确认；&lt;/li&gt;
&lt;li&gt;Follower 收到 Cold∪Cnew 的 log entry 后更新本地日志，并且此时就以该配置作为自己的成员配置；&lt;/li&gt;
&lt;li&gt;如果 Cold 和 Cnew 中的两个多数派确认了 Cold U Cnew 这条日志，Leader 就提交这条 log entry 并切换到 Cnew；&lt;/li&gt;
&lt;li&gt;接下来 Leader 生成一条新的 log entry，其内容是新成员配置 Cnew，同样将该 log entry 写入本地日志，同时复制到 Follower 上；&lt;/li&gt;
&lt;li&gt;Follower 收到新成员配置 Cnew 后，将其写入日志，并且从此刻起，就以该配置作为自己的成员配置，并且如果发现自己不在 Cnew 这个成员配置中会自动退出；&lt;/li&gt;
&lt;li&gt;Leader 收到 Cnew 的多数派确认后，表示成员变更成功，后续的日志只要得到 Cnew 多数派确认即可。Leader 给客户端回复成员变更执行成功。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;如果增强成员变更的限制，假设 Cold 与 Cnew 任意的多数派交集不为空，这两个成员配置就无法各自形成多数派，那么成员变更方案就可能简化为一阶段。&lt;/p&gt;
&lt;p&gt;那么如何限制 Cold 与 Cnew，使之任意的多数派交集不为空呢？方法就是每次成员变更只允许增加或删除一个成员。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;一阶段成员变更&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;成员变更限制每次只能增加或删除一个成员（如果要变更多个成员，连续变更多次）。&lt;/li&gt;
&lt;li&gt;成员变更由 Leader 发起，Cnew 得到多数派确认后，返回客户端成员变更成功。&lt;/li&gt;
&lt;li&gt;一次成员变更成功前不允许开始下一次成员变更，因此新任 Leader 在开始提供服务前要将自己本地保存的最新成员配置重新投票形成多数派确认。&lt;/li&gt;
&lt;li&gt;Leader 只要开始同步新成员配置，即可开始使用新的成员配置进行日志同步。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Raft 与 Multi-Paxos 中相似的概念：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/v2-a932cb62a02604d5ec57dc0a046a1414_r.jpg&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;Raft 与 Multi-Paxos 的不同：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/v2-7679d235c0ac8056552ba88b677e73a2_r.jpg&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">https://chendouxing.github.io/2023/03/28/%E8%AE%BA%E6%96%87/</guid>
            <title>论文阅读概要</title>
            <link>https://chendouxing.github.io/2023/03/28/%E8%AE%BA%E6%96%87/</link>
            <category>论文</category>
            <pubDate>Tue, 28 Mar 2023 18:51:10 +0800</pubDate>
            <description><![CDATA[ &lt;link rel=&#34;stylesheet&#34; class=&#34;aplayer-secondary-style-marker&#34; href=&#34;\assets\css\APlayer.min.css&#34;&gt;&lt;script src=&#34;\assets\js\APlayer.min.js&#34; class=&#34;aplayer-secondary-script-marker&#34;&gt;&lt;/script&gt;&lt;h3 id=&#34;数据一致性知识点&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#数据一致性知识点&#34;&gt;#&lt;/a&gt; 数据一致性知识点&lt;/h3&gt;
&lt;p&gt;数据一致性分为强一致性、弱一致性和因果一致性。&lt;/p&gt;
&lt;p&gt;强一致性要求任何数据更新在所有副本中立即可见；弱一致性不要求数据更新在所有副本中立即可见，它通过使用一定的规则使所有副本在一定时间内保持相同的状态。根据对分布式环境的分析和 CAP 定理（即在分布式系统中，一致性、可用性和分区容忍度最多只能有两个条件），强一致性导致高延迟和对网络分区的零容忍，弱一致性导致短时间内数据不一致，从而导致应用异常。&lt;/p&gt;
&lt;p&gt;因果一致性协议是介于强一致性和弱一致性之间的协议，能够有效解决强一致性带来的高延迟和零分区容忍问题，避免弱一致性带来的应用异常。它满足了分布式环境和 CAP 定理的要求。因果一致性以其独特的优势在分布式存储系统的构建中得到了广泛的应用。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;napa-powering-scalable-data-warehousing-with-robust-query-performance-at-google&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#napa-powering-scalable-data-warehousing-with-robust-query-performance-at-google&#34;&gt;#&lt;/a&gt; 《Napa: powering scalable data warehousing with robust query performance at Google》&lt;/h3&gt;
&lt;p&gt;Ankur 等学者（引文：Ankur Agiwal, Kevin Lai, Indrajit Roy,et al. 2021. Napa: powering scalable data warehousing with robust query performance at Google. Proc. VLDB Endow. 14, 12 (July 2021), 2986–2997.）由于 Google 服务不断生成大量数据，同时需要在可扩展性、亚秒级查询响应时间、可用性和强一致性的条件下存储和服务这些数据集的挑战下，开发部署了了一个分析数据管理系统 Napa。Napa 相比 Mesa，Napa 使用物化视图取代了 Mesa 繁琐的刷新技术来加速分析型查询；Napa 将数据新鲜度、成本和查询性能三者根据客户需求进行调整，使需求转换为内部数据库配置，并引入可查询时间戳（Queryable Timestamp，QT）。使用 QT 判断物化视图的延迟度，如图所示，&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/image-20230306162011309-16800541710091.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;QT 之前为可查询时间，QT 到 Now（）之间的内容无法查询，[Now（）-QT] 代表增量的数据新鲜度。如果 QT（table）= X，则客户端可以查询在时间 X 之前摄取到表中的所有数据，QT 充当屏障，在 X 之后接收的任何数据都对客户端查询隐藏。一旦在（Y-X）范围内接收的数据被优化以满足查询性能要求，QT 的值将从 X 前进到 Y。QT 在读时使用最小值作为读的时间点，数据库的 QT 是数据库所有表 QT 值的最小值。&lt;/p&gt;
&lt;p&gt;例子：本地 QT 值为 100、90、83、75、64 的 5 个 Napa 副本，并且查询服务要求大多数副本可用，则跨所有站点的新 QT 被设置为 83，因为大多数副本至少到 83 为止是最新的。Napa 将使用 QT 至少为 83 的副本来回答查询，因为可以保证对这些副本的查询只需要读取本地可用的增量。&lt;/p&gt;
&lt;p&gt;对于提高查询性能，Napa 通过三种技术实现：（1）通过更积极地使用视图，（2）通过改变存储策略，Napa 在存储层提前合并增量，减少查询时增量的数量，并因此减少尾部等待时间，（3）通过解耦摄取、视图维护和查询执行。数据操作在数据中心的每个副本异步执行，而元数据操作定期执行，确保副本之间的数据可用性。Napa 直接使用 F1 作为执行引擎。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;lazy-maintenance-of-materialized-views&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#lazy-maintenance-of-materialized-views&#34;&gt;#&lt;/a&gt; 《Lazy maintenance of materialized views》&lt;/h3&gt;
&lt;p&gt;数据库系统通常在基表更新的同一事务中急切地维护视图，这使得视图维护开销很大，Zhou 等学者（引文：Jingren Zhou, Per-Ake Larson, and Hicham G. Elmongui. 2007. Lazy maintenance of materialized views. In Proceedings of the 33rd international conference on Very large data bases (VLDB &#39;07). VLDB Endowment, 231–242.）提出了一种新的惰性维护物化视图的方法，既能减轻视图维护的负担，也能保证查询到最新的视图。惰性维护基本原则：（1）基表更新时，系统不维护视图，只是存储增量信息；（2）系统有空闲周期时，维护由低优先级作业执行；（3）视图在查询时不是最新的，在允许查询访问它之前，该视图被立即维护更新，但仅维护该视图。&lt;/p&gt;
&lt;p&gt;如图所示，图中显示惰性视图维护的整体系统设计。&lt;/p&gt;
&lt;p&gt;增量表：存储基表的增量流，表中每行有两个附加列（事务序列号和语句序列号）表示哪些事务和语句生成了该增量行。&lt;/p&gt;
&lt;p&gt;维护管理器：跟踪维护任务以及生成维护任务并调度，周期性地创建低优先级作业，删除增量表中过时的增量。管理器中包含一个哈希表，以快速寻找给定视图的所有维护任务。&lt;/p&gt;
&lt;p&gt;任务表：存储和删除维护任务，仅用于恢复，不做正常处理。&lt;/p&gt;
&lt;p&gt;维护任务：由需要维护的视图信息、更新的基表信息、事务序列号、语句序列号、原始事务提交序列号和任务当前状态组成。事务序列号和语句序列号用于惰性维护时定位增量行和基表版本，原始事务提交序列号确定任务维护顺序，任务当前状态用于维护管理器计划和跟踪任务。&lt;/p&gt;
&lt;p&gt;当查询开始之前，管理器检查计划使用的视图是否有挂起的维护任务（确保需要使用的视图都是最新的），若存在维护任务，管理器将立即执行这些任务，同时基表上所有的索引立即更新，任务完成后，查询继续执行。如果挂起的维护任务不影响查询访问的视图，则该视图不必立即更新。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/image-20230306162430648-16800542101424.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;图为惰性维护和紧急维护在响应时间上的比较。&lt;/p&gt;
&lt;p&gt;a、基表更新便立即更新视图，当查询到达时，更新已经全部完成，视图内容为最新的，此时，查询立即完成；&lt;/p&gt;
&lt;p&gt;b（1）、基表三次更新 T1、T2、T3 完成后，维护管理器等待系统空闲时，创建低优先级作业完成维护任务，查询到达时，视图是最新的，查询快速完成，查询响应时间与 a 相同；&lt;/p&gt;
&lt;p&gt;b（2）、基表完成更新后，视图进行维护时，查询到达，查询等待维护任务完成后继续执行；&lt;/p&gt;
&lt;p&gt;b（3）、基表更新完成，查询在系统维护视图之前到达，查询在开始时发出按需维护请求，并等待维护完成，查询继续执行。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;知识点流处理系统&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#知识点流处理系统&#34;&gt;#&lt;/a&gt; 知识点：流处理系统&lt;/h3&gt;
&lt;p&gt;（&lt;a href=&#34;https://zhuanlan.zhihu.com/p/59643962&#34;&gt;Stream SQL 的执行原理与 Flink 的实现 - 知乎 (zhihu.com)&lt;/a&gt;）&lt;/p&gt;
&lt;p&gt;** 流 (Stream)** 可以被看作一种无界 (Unbounded) 且只可追加的 (Append-Only) 的数据表。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;流处理系统的时间操作&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在现实世界的分布式系统当中处理流主要面对的问题有：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;绝对时钟 (Absolute Clock) 问题：现实世界当中的时钟往往不精确，而且在分布式系统当中实现时间绝对同步是 (物理上) 不可能的；&lt;/li&gt;
&lt;li&gt;时间倾斜 (Time Skew) 问题：由于系统中必然会存在网络延迟、网络中断和系统崩溃等问题， 发送到系统的消息和系统内部的消息很可能失序乃至丢失。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;解决方案&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;1. 对于第一个问题，现代操作系统普遍使用事件驱动 (Event-Driven) 模型来处理。通过使用消息本身携带的生成时间 (Event-Time) 而不是系统接收到消息的时间 (Processing Time) 来进行处理。这样的系统当中，时钟是由事件来驱动的。 没有新的事件到来，系统的状态就如同冻结起来了一样。&lt;/p&gt;
&lt;p&gt;** 激发器 (Trigger)** 是一类特别的事件，当这种事件的消息被接收到时，某些任务会被激发和执行。&lt;/p&gt;
&lt;p&gt;** 水印 (Watermark)** 简单来说，就是根据消息的事件时间来决定一条消息应该被处理还是被丢弃的标记。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/v2-461d11fa37482e48a8bde66a0b91cf2c_r.jpg&#34; alt=&#34;v2-461d11fa37482e48a8bde66a0b91cf2c_r&#34;&gt;&lt;/p&gt;
&lt;p&gt;上图中，从右到左是消息到达的时间，在某时刻，消息 8 通过激发器激发了一次对水印的修改。此时水印的时间限制被修改为 4 。 这意味着之后到达的标号时间小于 4 的消息都会被丢弃。在消息 8 之后到达的消息 7 和 5，虽然时间戳比消息 8 要早， 但是因为仍在 Watermark 的范围里，因此会被考虑在内。最后到达的消息有时间标号 9，他是一条当前观察到过的消息之后的消息， 因此也会被处理。&lt;/p&gt;
&lt;p&gt;水印的规则：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;水印应该永远小于当前处理过的事件的时间戳；&lt;/li&gt;
&lt;li&gt;水印是通过激发器的激发来移动的，算子可以自己决定移动水印的时间，而不是每个接收到的事件都会改变水印；&lt;/li&gt;
&lt;li&gt;水印必须是单调递增的。否则，一旦水印向前移动，我们无法知道是否已经有被包含在水印范围里的消息被丢弃。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;** 窗口 (Window)** 是一种设计出来让用户更好地描述它们对时间的需求的工具。 他可以让用户在一个窗口里以有限时间 / 数据范围的方式操作数据，同时也为进一步优化时间空间成本提供了可能。&lt;/p&gt;
&lt;p&gt;流处理系统提供的常见的窗口类型有：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;固定窗口 (Fixed Window)：长度固定的窗口，每个窗口一个紧跟着一个，将时间维度划分成片段；&lt;/li&gt;
&lt;li&gt;滑动窗口 (Sliding Window)：长度固定，但每个窗口的开始时间相比于前一个窗口都有一个固定的时间偏移；&lt;/li&gt;
&lt;li&gt;会话窗口 (Session Window)：使用事件的属性和相互的时间间隔把他们组织在一个窗口里。这些窗口的开始时间、 持续长度等都会变化。这类窗口用于用户追踪、线索跟踪等场景十分有效。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;/images/v2-35d35ee365a77543cc91532e223d5632_r.jpg&#34; alt=&#34;v2-35d35ee365a77543cc91532e223d5632_r&#34;&gt;&lt;/p&gt;
&lt;p&gt;** 窗口和水印是两个不同的概念。** 一个窗口的结束时间已经过去， 并不意味着这个窗口不能再接收迟到的落入这个窗口的消息。实际上，这个窗口的内部状态可以被一直保存， 以便于在接收到新的消息之后刷新窗口内容并输出增量表。一个窗口及其内部状态将只会在水印完全通过之后被回收。 事实上，一个窗口的打开和关闭都有赖于激发器的作用。&lt;/p&gt;
&lt;p&gt;** 句点 (Punctation)** 是一些在窗口关闭之前激发的激发器，它使得窗口可以输出它的中间结果而不必等到整个窗口的消息都处理完成。这对于提供低延迟数据传达十分有用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Stream Join 的语义&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;由于 Stream 都是无边界的数据，传统数据表当中的 Join 概念在流处理系统当中可能不完全适用。 流处理系统当中的 Join 往往有以下几种类别和语义：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Stream 与纯静态表 Join。这里的纯静态表的内容不会改变，因此 Join 的实现只是在 Stream 端对每个消息在静态表内进行查询；&lt;/li&gt;
&lt;li&gt;Stream 与动态表的快照 Join。动态表的内容可能会出现增删改等情况，这里的 Join 的语义是， 当对流当中的某个消息实施 Join ，相当于查询了动态表在那一时刻的快照；&lt;/li&gt;
&lt;li&gt;Stream 与 Stream Join，操作的两边都是 Stream。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;动态表与 Stream Join&lt;/strong&gt; 则是通过将动态表处理成类似 MVCC 并发控制那样的形式， 因此在每一个来自于 Stream 的消息需要 Join 时，只需要查询对应&lt;strong&gt;事件时间&lt;/strong&gt;下动态表的快照即可。&lt;/p&gt;
&lt;p&gt;在 Stream Join 动态表这个模型中，动态表这端虽然可能也是由 Stream 而来，但是对动态表的插入和修改操作 (也表示称 Stream 消息) 并不会激发 Join 结果的刷新。 这些消息只是被加入到内部状态中，等待 Stream 里的消息激发刷新时查询。 这是动态表 Join Stream 与 Stream Join Stream 最大的区别。&lt;/p&gt;
&lt;p&gt;Stream 与 Stream Join 在两边有消息来的时候都有可能激发大量查询和修改操作，因此面临着严峻的查询放大放大和修改放大问题。在流式处理系统中，一种解决方法是结合窗口语义实现局部的 Join，下图描述了不同窗口下 Join 的语义。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/v2-dac4bd8ff5613358f3fcd04a231c8b31_r.jpg&#34; alt=&#34;v2-dac4bd8ff5613358f3fcd04a231c8b31_r&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;watermarks-in-stream-processing-systems-semantics-and-comparative-analysis-of-apache-flink-and-google-cloud-dataflow&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#watermarks-in-stream-processing-systems-semantics-and-comparative-analysis-of-apache-flink-and-google-cloud-dataflow&#34;&gt;#&lt;/a&gt; 《Watermarks in Stream Processing Systems: Semantics and Comparative Analysis of Apache Flink and Google Cloud Dataflow 》&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;准备就绪&lt;/strong&gt;：对于流处理，准备就绪是指具有可用于产生某个输出的所有必要输入的状态。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;健康监测&lt;/strong&gt;：由于水位线跟踪整个数据处理管道的进度，因此影响管道进度的任何问题都将表现为水位线中的延迟。假设流处理框架提供了水印的足够细粒度的视图（例如，在管道中的物理阶段之间和内部进行划分），则通常可以通过查找第一个延迟水印来精确定位管道中的问题。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;convergent-causal-consistency-for-social-media-posts&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#convergent-causal-consistency-for-social-media-posts&#34;&gt;#&lt;/a&gt; 《Convergent Causal Consistency for Social Media Posts 》&lt;/h3&gt;
&lt;p&gt;异地复制服务在云存储管理中发挥着至关重要的作用，它可以提供增强的可用性、更高的可靠性和更低的延迟，从而按需访问共享基础架构和数据资源。在这样的环境中，一致性对于其中需要对复制的数据进行更新的分布式存储系统是关键的和基本的考虑。收敛因果一致性模型为在线人机交互服务提供了有用的语义，已成为一种流行的一致性模型。自适应非完全复制策略具有降低社交网络系统中消息计数的潜在好处。但是，静态复制对于随时间变化的工作负载效率不高。提出了一个因果 + 一致性协议 CaDRoP，支持自适应动态复制，该协议对帖子后的所有评论具有收敛性，并支持帖子之间的因果排序。根据 Amazon Web Service 的实际价格，采用不同的 PUT 率对 CaDRoP 协议在实际工作负载下进行了评估。结果表明，与在静态复制的数据存储中运行相比，CaDRoP 可以产生明显更低的成本。我们通过将其与透视最优复制解决方案进行比较来进一步评估 CaDRoP。研究结果表明，使用缓存时，CaDRoP 只会产生大约 6% 〜 16% 的额外成本。没有缓存，CaDRoP 在稳定状态下会带来 2% 〜 4.5% 的额外开销。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;fine-grained-distributed-consistency-guarantees-with-effect-orchestration&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#fine-grained-distributed-consistency-guarantees-with-effect-orchestration&#34;&gt;#&lt;/a&gt; 《Fine-grained Distributed Consistency Guarantees with Effect Orchestration 》&lt;/h3&gt;
&lt;p&gt;高可用性的分布式应用程序通常需要在地理上分布的存储上复制数据，而这些存储在默认情况下提供的一致性保证较弱。不幸的是，在弱一致性下可能出现不期望的行为，这可能违反应用程序的正确性，迫使设计人员要么实现复杂的特别机制来避免这些异常，要么通过牺牲性能来选择使用更高级别的一致性来运行应用程序。在本文中，我们描述了一个轻量级的运行时系统，它使开发人员不必进行这样的权衡。相反，我们的方法利用了声明性公理规范，这些规范反映了任何正确的实现都必须满足的必要约束，以指导运行时一致性实施和监视机制。实验结果表明，本文提出的细粒度一致性实施机制的性能（可证明是最优的和安全的）优于普通存储提供的一致性保证。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;simba-tunable-end-to-end-data-consistency-for-mobile-apps&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#simba-tunable-end-to-end-data-consistency-for-mobile-apps&#34;&gt;#&lt;/a&gt; 《Simba: Tunable End-to-End Data Consistency for Mobile Apps 》&lt;/h3&gt;
&lt;p&gt;云连接移动的应用程序的开发人员需要确保跨多个设备的应用程序和用户数据的一致性。移动的应用程序在各种使用场景下需要不同的分布式数据一致性选择。应用程序还需要优雅地处理间歇性连接和断开、有限的带宽以及客户端和服务器故障。应用程序的数据模型也可能很复杂，跨越相互依赖的结构化和非结构化数据，需要在本地、云和其他移动的设备上以原子方式存储和更新。&lt;/p&gt;
&lt;p&gt;在本文中，我们研究了几个流行的应用程序，发现许多应用程序在并发使用时由于数据一致性处理不当而表现出不良行为。针对这些缺点，我们提出了一种新的数据抽象，称为 sTable，它统一了表格和对象数据模型，并允许应用程序从一组分布式一致性方案中进行选择；写入此抽象的移动的应用程序可以毫不费力地与云和其他移动设备同步数据，同时受益于端到端数据一致性。我们构建了 Simba（一个数据同步服务）来演示我们提出的抽象的实用性，并通过编写新应用程序和移植不一致的应用程序来对其进行评估。实验结果表明，Simba 在同步延迟、带宽消耗、服务器吞吐量以及用户数和数据量的伸缩性方面表现良好。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;viewdf-declarative-incremental-view-maintenance-for-streaming-data&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#viewdf-declarative-incremental-view-maintenance-for-streaming-data&#34;&gt;#&lt;/a&gt; 《ViewDF: Declarative incremental view maintenance for streaming data 》&lt;/h3&gt;
&lt;p&gt;我们提供 ViewDF：用于物化视图的增量维护的灵活且声明性的框架。该框架的主要组成部分是视图增量函数（ViewDF），它声明性地指定当新的一批数据到达时如何更新物化视图。我们描述并实验评估了一个基于该思想的原型系统，该系统允许用户直接编写 ViewDF，并自动将常见的流查询类转换为 ViewDF。我们的方法概括了增量视图维护的现有工作，并支持流分析中常见的视图的新优化，包括具有模式匹配和滑动窗口的视图。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;multidirectional-replication-for-supporting-strong-consistency-low-latency-and-high-throughput&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#multidirectional-replication-for-supporting-strong-consistency-low-latency-and-high-throughput&#34;&gt;#&lt;/a&gt; 《Multidirectional Replication for Supporting Strong Consistency, Low Latency, and High Throughput 》&lt;/h3&gt;
&lt;p&gt;许多复制协议可以用在各种分布式应用中，例如分布式数据库、协作应用和分布式议程，它们牺牲了强一致性以实现更低的延迟和更高的吞吐量。本文描述了单向复制和多向复制的设计、规范、实现和评估，它们挑战了这种不灵活的折衷。通过并行传播结果状态（如在主备份复制中），但使用较少的消息，同时具有处理写请求的头部和处理读请求的尾部（如在链复制中），单向复制改善了两个协议的延迟和吞吐量，而不损害强一致性。为了提高单向复制的计算和通信资源的利用率，多向复制将对象划分为多个逻辑碎片，并在每个逻辑碎片上运行一个单向复制实例。我们已经通过三个步骤完成了提议的协议：（1）将主备复制和链式复制合并为单向复制；（2）将单向复制和逻辑分片合并为多向复制；（3）实施与评价。实验结果表明，与主备复制相比，单向复制在处理具有读写冲突的读请求时，延迟大约降低了 66%; 与链式复制相比，当使用基本设置处理写请求时，单向复制在吞吐量方面显示出大约 59% 的改进；与单向复制相比，多向复制在可处理的客户端数量方面提高了 150%。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;pgce-a-distributed-storage-causal-consistency-model-based-on-partial-geo-replication-and-cloud-edge-collaboration-architecture&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#pgce-a-distributed-storage-causal-consistency-model-based-on-partial-geo-replication-and-cloud-edge-collaboration-architecture&#34;&gt;#&lt;/a&gt; 《PGCE: A distributed storage causal consistency model based on partial geo-replication and cloud-edge collaboration architecture 》&lt;/h3&gt;
&lt;p&gt;目前的因果一致性模型难以应对云存储系统中的高同步开销和响应延迟。提出了一种基于部分地理复制和云边缘协作结构的分布式存储因果一致性模型。该模型基于 Cloud-Edge 协作的分布式网络架构，通过哈希函数将云数据集划分为多个子集，并将这些子集存储在靠近用户网络的边缘节点上，实现局部异地复制。同时，通过建立时间戳稳定机制和元数据处理服务，在保证因果关系的前提下实现节点间的数据一致性，降低了元数据处理和数据同步的开销。客户端直接与边缘节点交互，减少了与云 DC 交互的响应延迟。与现有模型相比，PGCE 在响应延迟和吞吐量方面具有更好的性能。&lt;/p&gt;
&lt;p&gt;PGCE 的云端可以与已有的因果一致性模式相结合，边缘端的每个节点存储云数据集的一个子集，实现部分地理复制。通过使用副本数据定位矩阵，数据更新仅传播到存储相同数据的节点，从而减少了元数据处理和数据同步开销。客户端的数据请求可以在边缘节点得到及时的处理和响应，减少了直接与云 DC 进行数据交互所带来的延迟。同时，将稳定状态下的数据定期传输到云端进行全局数据的更新和同步。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/image-20230310191238109-168005432929913.png&#34; alt=&#34;image-20230310191238109&#34;&gt;&lt;/p&gt;
&lt;p&gt;图为 PGCE 基于云边缘协作存储架构设计。Cloud 存储客户端写入的所有数据，Edge 由大量靠近用户网络的边缘节点组成，采用部分复制策略存储云数据集的子集，根据节点或 DC 间的存储关系构建点对点 FIFO 数据同步链路。同时，在边缘端设置时间戳稳定机制和元数据处理节点，为客户端提供因果一致的数据存储和查询服务。客户端由许多终端用户组成，可以通过 Migrate 操作（当我们读取的键没有存储在本地节点上时：首先，利用 Hash 函数找到与关键字对应的目标分区。其次，通过矩阵存储关系找到目标分区所在的节点，执行客户端迁移连接到目标节点，实现对非本地节点的数据阅读。）连接到其他边缘节点上，访问本地节点上没有存储的数据。&lt;/p&gt;
&lt;p&gt;PGCE 建立了时间戳稳定机制和 Saturn 元数据服务来跟踪因果一致性。在每个 Edge-side 中设置一个 Saturn 节点（元数据处理服务器）。Saturn 节点接收所有需要更新或写入的元数据，并调用 Saturn 接口服务来执行全局因果关系排序，代替在每个边缘节点执行的依赖性检查工作，并以较低的开销确保 PGCE 的因果关系一致性。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../images/image-20230310192903417-168005435460916.png&#34; alt=&#34;image-20230310192903417&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../images/image-20230310193906414-168005437774019.png&#34; alt=&#34;image-20230310193906414&#34;&gt;&lt;/p&gt;
&lt;p&gt;例子：PGCE 利用矩阵来描述边缘节点和分区（DC) 之间的存储关系。矩阵的行表示分区，列表示节点，元素 1 表示存储该分区的节点，元素 0 表示不存储该分区的节点。从矩阵可以看出，当 P1 被更新时，我们仅将同步消息传播到 Node1 和 Node2，而不传播到 Node3 和 Node4。&lt;/p&gt;
&lt;p&gt;基于部分地理复制，每个数据中心存储完整数据集的一个子集，减少了完全地理复制系统的存储、同步和元数据处理开销。&lt;/p&gt;
&lt;p&gt;&lt;u&gt;基于因果一致性的分布式存储系统，通过&lt;strong&gt;地理复制策略&lt;/strong&gt;实现分布在不同位置的 DC 之间的数据同步。地域复制策略主要分为两类：完全异地复制和部分异地复制。在完全异地复制场景中，数据更新需要同步到所有副本，随着数据中心数量的增长，数据同步开销巨大。在部分异地复制方案中，数据更新不需要与所有副本同步，从而减少了同步开销，但这会迫使站点增加远程存储数据项的元数据管理成本。&lt;/u&gt;&lt;/p&gt;
&lt;p&gt;&lt;u&gt;&lt;strong&gt;边缘计算&lt;/strong&gt;是指在靠近事物或数据源头的一侧集成网络、计算、存储、应用等核心能力的开放平台，就近提供服务。它具有低延迟、低宽带操作和隐私保护的优势，这不仅降低了云的负载，还满足了边缘客户端的需求，提供了更好的用户体验。&lt;strong&gt;云计算适用于全局性、非实时性、长周期的大数据处理和分析&lt;/strong&gt;。&lt;strong&gt;边缘计算具有实时性、数据安全性和隐私保护等优点，适合于实时、短周期的数据处理和分析&lt;/strong&gt;&lt;/u&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;replicated-data-types-that-unify-eventual-consistency-and-observable-atomic-consistency&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#replicated-data-types-that-unify-eventual-consistency-and-observable-atomic-consistency&#34;&gt;#&lt;/a&gt; 《Replicated data types that unify eventual consistency and observable atomic consistency 》&lt;/h3&gt;
&lt;p&gt;强一致性广泛应用于关系数据库等系统中。在分布式系统中，强一致性确保所有客户机在所有服务器上原子地观察一致的数据更新。然而，当同步发生时，这样的系统需要牺牲可用性。&lt;/p&gt;
&lt;p&gt;我们提出了一个新的一致性协议，可观察原子一致性协议（OACP），使写主导的应用程序尽可能快和一致的需要。OACP 结合了以下优点：（1）可合并的数据类型，具体地说，收敛的复制数据类型，以减少同步，以及（2）可靠的总顺序广播，以提供按需的强一致性。我们还提供了一个高级编程接口，以提高分布式编程的效率和正确性。&lt;/p&gt;
&lt;p&gt;本文给出了 OACP 重写逻辑的形式化、机械化模型，并利用模型检测工具 Maude 验证了 OACP 的关键正确性。此外，我们还提供了一个基于 Akka 的 OACP 原型实现，Akka 是一个广泛使用的基于角色的中间件。实验结果表明，与现有的 Raft 一致性协议相比，OACP 协议能够有效降低协调开销。我们的结果还表明，OACP 通过可合并的数据类型提高了可用性，并为实现强一致性提供了可接受的延迟，从而允许有原则地放松强一致性以提高性能。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;towards-the-synthesis-of-coherencereplication-protocols-from-consistency-models-via-real-time-orderings&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#towards-the-synthesis-of-coherencereplication-protocols-from-consistency-models-via-real-time-orderings&#34;&gt;#&lt;/a&gt; 《Towards the Synthesis of Coherence/Replication Protocols from Consistency Models via Real-Time Orderings 》&lt;/h3&gt;
&lt;p&gt;本工作集中于具有读写接口的共享存储器系统（例如，分布式数据存储或多处理器）。在这样的系统的核心驻留有负责执行它们的一致性保证的协议。设计一个正确有效地执行一致性的协议是一项非常具有挑战性的任务。我们的总体愿景是自动完成这项任务。在这项工作中，我们通过建立从一致性规范自动推断协议所需的理论基础，朝着这一愿景迈出了一步。具体来说，我们提出了一组数学抽象，称为实时排序（rt 排序），用于对协议进行建模。然后我们创建一个从一致性保证到执行保证的最小 rt - 序的映射。最后，我们非正式地将 rt - 序与协议实现技术联系起来。因此，rt - 排序充当一致性和协议设计之间的中间抽象，其使得能够将一致性保证自动翻译成协议实现。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;crdts主题&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#crdts主题&#34;&gt;#&lt;/a&gt; CRDTs 主题&lt;/h3&gt;
&lt;p&gt;无冲突复制数据类型（Conflict-free replicated data type，CRDTs）广泛应用于工业分布式系统，如 Riak 和 AntidoteDB 。它们适合于实现高可用性和可伸缩性的复制共享数据，因为它们支持并发更新，并且如果所有副本最终执行所有更新，则它们保证收敛到相同的状态。&lt;/p&gt;
&lt;p&gt;两个主要类别的 CRDTs:&lt;strong&gt; 行动 CRDTs&lt;/strong&gt; 和&lt;strong&gt;基于状态的 CRDTs（CvDRT)&lt;/strong&gt;。行动 CRDTs 传播交换副本之间的更新操作。他们需要 “完全一次交付”, 这就需要可靠的因果广播。基于状态的 CRDTs, 也叫做 CvRDTs, 传播整个状态之间的 CRDT 副本只要更新状态。交换功能是用于合并多个修正 CRDT 的状态，因此可以发送多次。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;deepsqueeze-deep-semantic-compression-for-tabular-data&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#deepsqueeze-deep-semantic-compression-for-tabular-data&#34;&gt;#&lt;/a&gt; 《DeepSqueeze: Deep Semantic Compression for Tabular Data 》&lt;/h3&gt;
&lt;p&gt;大型数据集的快速扩散，高效的数据压缩比以往变得更加重要。柱状压缩技术 (如字典编码，游程编码，增量编码) 表格数据已经证明非常有效，但其通常压缩单个列而不考虑潜在的列之间的关系，如函数依赖和相互关系。语义压缩技术，另一方面，是为了利用这种关系只存储必要列的一个子集来推断，但现有方法不能有效地识别复杂的关系所在列的关系。&lt;/p&gt;
&lt;p&gt;DeepSqeeze 是一个新的语义压缩框架，通过使用自动编码器将元组映射到低维空间，捕获表格数据中分类列和数值列之间的复杂关系。DeepSqeeze 还支持数值数据有损压缩的保证误差范围，并与常见的列式压缩格式结合使用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;通用压缩算法&lt;/strong&gt;忽略了数据集的高级语义，只对原始比特进行操作。这些技术分为两类：（1）无损和（2）有损。&lt;/p&gt;
&lt;p&gt;无损压缩是可逆的，这意味着原始输入可以从压缩格式完美恢复。无损压缩算法通常通过识别和去除输入数据中的统计冗余来操作。例如，霍夫曼编码 [24] 用可变长度代码替换符号，将较短的代码分配给更频繁的符号，使得它们需要更少的空间。&lt;/p&gt;
&lt;p&gt;有损压缩，与无损压缩不同。有损压缩通过丢弃不必要的信息（例如截断测量值的最低有效位或对图像进行二次采样）来减小数据大小。由于信息的丢失，该过程是不可逆的。虽然一些用例需要输入的完美可恢复性（例如，银行交易），DeepSqueeze 工作集中在通常可以容忍不同程度的丢失的数据存档场景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DeepSqeeze 是第一个将自动编码器应用于表格数据的语义压缩方法&lt;/strong&gt;。重要的是，DeepSqeeze 可以捕获分类列和数值列之间的复杂关系，并且我们通过用户指定的错误阈值将数值的损失纳入模型中。DeepSqeeze 使用自动编码器将元组映射到低维空间，允许对许多列之间的复杂关系进行建模。&lt;/p&gt;
&lt;h4 id=&#34;deepsqeeze的压缩过程&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#deepsqeeze的压缩过程&#34;&gt;#&lt;/a&gt; DeepSqeeze 的压缩过程&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;/images/image-20230403183001601.png&#34; alt=&#34;image-20230403183001601&#34;&gt;&lt;/p&gt;
&lt;h5 id=&#34;预处理&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#预处理&#34;&gt;#&lt;/a&gt; 预处理&lt;/h5&gt;
&lt;p&gt;作为输入，DeepSqeeze 接受一个表格数据集，该数据集由分类列和数值列的任意组合以及指定列类型的元数据组成。&lt;/p&gt;
&lt;p&gt;预处理将此输入数据预处理为模型可以操作的格式。根据数据类型，应用各种转换（例如，字典编码），以及遵守数值列的指定误差阈值的量化版本。&lt;/p&gt;
&lt;p&gt;以下为每种列类型的不同预处理技术：&lt;/p&gt;
&lt;p&gt;1. 分类列&lt;/p&gt;
&lt;p&gt;分类列包含不同的、无序的值，通常表示为字符串。对于具有很少不同值的列，字典编码是一种普遍的压缩技术，它涉及用较小的代码替换较大的数据类型。DeepSqeeze 将分类列中的每个非重复值替换为唯一的整数代码，产生两个用途：（1）已经减小了输入数据的大小；（2）将分类值转换为模型所需的数值类型。例如，DeepSqeeze 将分别用 {0，1，2，3} 替换列中的值 {A，B，C，D}。&lt;/p&gt;
&lt;p&gt;另一方面，对于具有许多不同值的分类列（例如，唯一字符串，主键），DeepSqeeze 会自动将它们从正常的压缩管道中排除，并回撤到现有的压缩技术。然而，在值的分布偏斜的特定情况下，在训练期间可以忽略不频繁出现的值，使得仅在最频繁出现的分类值上训练模型。由于减少分类列的可能输出值的数量可以显著减少模型中的参数数量，因此与错误预测不常见值相关的少量额外开销会被模型大小的大幅减少所抵消。&lt;/p&gt;
&lt;p&gt;2. 数字列&lt;/p&gt;
&lt;p&gt;数值列可以包含整数或浮点数。在这两种情况下，第一步都是执行最小最大缩放，将所有值归一化到 [0，1] 范围，这将为模型训练准备数值列，并解决列值之间的缩放差异。&lt;/p&gt;
&lt;p&gt;然而，许多应用可以容忍某种程度的不精确性，因为它们不需要精确的结果（例如，视觉数据探索），或者因为在数据生成过程中存在一些其它形式的噪声（例如，传感器硬件的限制）。因此，还将允许用户为每列指定错误阈值与有损压缩进行有效结合。允许有损压缩的一种方式是通过接受指定误差阈值内的值的任何预测来扩展无损版本。此外，我们必须修改相关的损失函数以考虑错误阈值，这样模型就不会惩罚正确的预测。&lt;/p&gt;
&lt;p&gt;替代方法是量化列，其涉及用使用指定误差阈值计算的不相交桶的中点替换值。例如，考虑一个数值列，其值在 [0，100] 范围内，用户指定的错误阈值为 10%; 我们的量化方法将用离散桶中点替换这些连续值：{10，30，50，70，90}。由于量化已经将误差阈值合并到桶创建中，因此我们不需要修改模型或损失函数。&lt;/p&gt;
&lt;p&gt;与第一种方法相比，量化允许模型学习离散映射，这具有两个主要优点：（1）模型可以简单得多，因此尺寸更小；（2）离散值更容易预测，导致更少的物化故障。此外，针对量化值的具体化故障的列压缩比连续值有效得多。&lt;/p&gt;
&lt;h5 id=&#34;模型构建&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#模型构建&#34;&gt;#&lt;/a&gt; 模型构建&lt;/h5&gt;
&lt;p&gt;构建一个自动编码器的人工神经网络，它将预处理的输入数据映射到低维表示。自动编码器类似于其他降维技术，但它们能够同时跨多个列学习复杂的关系。无监督训练过程在数据集上迭代进行，直到收敛。重要的是，与传统的机器学习设置不同，我们的&lt;strong&gt;目标是将模型过拟合到输入数据，以最小化压缩输出的大小&lt;/strong&gt;。将模型过拟合到训练数据的一种方式是创建能够捕获数据集中的所有关系的复杂模型，而替代方法涉及构建多个更简单的模型，称为&lt;strong&gt;专家混合&lt;/strong&gt;，其专门用于包含相似元组的数据的某些分区。图中显示了三个专门的模型，元组通过一个阀门路由到模型，该模型在训练期间学习数据的最佳划分。&lt;/p&gt;
&lt;p&gt;下图描绘了用于压缩具有一个分类（C）列和三个数值（N）列的数据集的示例自动编码器。如图所示，自动编码器由两个几乎对称的模型组成：（1）编码器，其将输入元组映射到低维空间；以及（2）试图重构输入元组的解码器。共享中间层表示每个元组的学习表示（代码）。DeepSqeeze 总是用比原始输入元组更小的表示层来构造自动编码器，这是将数据映射到低维空间的瓶颈。DeepSqeeze 存储这些代码，并在解压缩期间使用解码器重新创建元组。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/image-20230403200107007.png&#34; alt=&#34;image-20230403200107007&#34;&gt;&lt;/p&gt;
&lt;p&gt;为了处理现实世界数据集中经常出现的混合分类列和数值列，DeepSqeeze 需要基于列类型信息动态调整基本自动编码器架构。图中的数值列在输出层中只需要一个节点，而分类列则需要为每个不同的值指定一个节点。在上一个示例中，具有可能值 {A，B，C，D} 的分类列需要四个输出节点（图中的蓝色）。&lt;/p&gt;
&lt;p&gt;与集成分类列相关的关键问题之一是在最终完全连接层中引入大量连接所导致的模型大小的潜在爆炸。为了解决这个问题，使用&lt;strong&gt;参数共享技术&lt;/strong&gt;，涉及所有分类列的共享输出层，而不是每列每个不同值的单个节点。&lt;strong&gt;共享输出层的大小由任何分类列中不同值的最大数目来限定&lt;/strong&gt;。同时，还必须在输出层之前添加一个辅助层，每个分类列有一个节点，以及一个额外的信号节点。信号节点简单地为每个分类列提供索引，通知共享层如何解释来自特定输出的辅助层的值。&lt;/p&gt;
&lt;p&gt;下图描述数据集解码器的最后两层，该数据集具有三个分类列，分别用于传统架构（左）和参数共享版本（右）。每列都有不同数量的不同值，这些值是用颜色编码的。由于数据集有三列，我们的辅助层包含三个节点加上一个信号节点 s。共享输出层有五个节点，这是所有列中不同值的最大数量。如图所示，辅助层显著减小了完全连接层的尺寸。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/image-20230403201110277.png&#34; alt=&#34;image-20230403201110277&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;u&gt;专家混合&lt;/u&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;构建几个较小且不太复杂的模型来处理不相交的数据子集&lt;/strong&gt;。这种方法是，每个较小的模型可以学习一组有限的简单映射。&lt;/p&gt;
&lt;p&gt;本文使用了一个稀疏门控的专家混合体，它学习如何以无监督的方式而不是传统的聚类算法来最好地划分数据集。该体系结构的核心是一个门，它是一个独立的模型，将元组分配给最适合的专家（即，对于每个元组具有最高精度的模型）。因此，门需要每个专家一个输出。&lt;/p&gt;
&lt;p&gt;具体过程：由图 1 所示，将每个输入元组分配给三个专家中的一个。然而，在实践中，输入元组被同时馈送给所有专家，并且门为除了所选择的专家的预测之外的所有专家产生掩码。&lt;/p&gt;
&lt;p&gt;与代码大小类似，专家数量是一个超参数，必须根据具体情况进行选择。使用太少的专家将导致模型精度差，而太多的专家将不必要地增加模型的大小。&lt;/p&gt;
&lt;h5 id=&#34;物化&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#物化&#34;&gt;#&lt;/a&gt; 物化&lt;/h5&gt;
&lt;p&gt;物化使用经过训练的自动编码器来生成低维表示，在图中标记为 Codes。这些代码中的每一个表示单个元组，并且在大小上比原始元组小得多。同时，还必须保存模型的解码器部分，以便从代码中重建原始元组，以及从我们的模型中纠正错误预测（用 X 表示）的失败值。&lt;/p&gt;
&lt;p&gt;物化解码器通常表示总体压缩输出大小的相对小的部分，这表明优化的资源消耗将更好地花费在其他地方（例如，减小具体化故障的大小）。&lt;/p&gt;
&lt;p&gt;对于代码物化，模型产生最小宽度为 64 位的代码（即，双精度浮点值），但 64 位通常较大。因此，&lt;strong&gt;DeepSqeeze 迭代地截断每个代码，直到代码大小的减少不再支付相应的失败数量的增加&lt;/strong&gt;。由于浮点值通常难以压缩，因此在截断之后将代码转换为整数的最后步骤可以进一步减小大小。对于这种优化，我们将代码乘以通常所需的最小幂，以确保具有最大小数位数的代码转换为整数，然后将结果转换为整数类型，然后可以使用标准整数压缩技术（例如，增量编码）。&lt;/p&gt;
&lt;p&gt;物化故障：故障的物化代表压缩输出大小的最大部分。本文使用 Parquet 来压缩物化故障。&lt;/p&gt;
&lt;h4 id=&#34;解压缩&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#解压缩&#34;&gt;#&lt;/a&gt; 解压缩&lt;/h4&gt;
&lt;p&gt;解压缩过程基本上执行压缩过程中每个步骤的逆过程。具体地，解压缩流水线通过将物化代码传送到所保存的解码器，解码器重构出与预处理输出结果的近似版本。然后，将每个重建的元组与具体化的故障进行比较，并将所有错误值替换为正确值。在反转预处理的最后一步之后，已经恢复了原始数据集。&lt;/p&gt;
&lt;p&gt;专家映射：对于具有单个专家的模型，使用单个解码器来解压缩所有元组。然而，对于具有多个专家的模型，需要物化将代码映射到正确解码器的元数据。本文考虑两种方式存储这些映射：&lt;/p&gt;
&lt;p&gt;第一种方法如图 1 所示，其中代码和故障由专家分组，并与其原始索引沿着存储。这些索引告诉 DeepSqeeze 重建原始文件的顺序，并且它们通常可以通过 delta 编码进行有效压缩。&lt;/p&gt;
&lt;p&gt;第二种方法涉及存储所有元组以及每个元组的附加专家分配标签，然后 DeepSqeeze 可以使用该标签来选择正确的解码器。与存储索引类似，这些标签通常可以使用行程长度编码进行有效压缩。&lt;/p&gt;
&lt;p&gt;在这两个备选方案之间作出选择取决于数据，因此必须根据具体情况作出选择。对于不必在原始数据集中保持元组的确切顺序，例如关系表。我们可以通过使用第一种方法来保存额外的空间，该方法存储按专家分组的元组，而无需物化索引。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;f1-query-declarative-querying-at-scale&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#f1-query-declarative-querying-at-scale&#34;&gt;#&lt;/a&gt; 《F1 Query: Declarative Querying at Scale》&lt;/h3&gt;
&lt;h4 id=&#34;动机&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#动机&#34;&gt;#&lt;/a&gt; 动机&lt;/h4&gt;
&lt;p&gt;在类似 Google 这样的大型企业中，数据处理和分析在数据大小、延迟、数据源和新鲜度以及对自定义业务逻辑的需求方面有不同的需求。许多数据处理系统仅仅专注于该需求功能的特定情况，例如，专注于事务式查询、中型 OLAP 查询的提取 - 转换 - 加载（ETL）管道。有些系统是高度可扩展的，而有些则不是。有些系统主要作为一个封闭的筒仓运行，而其他系统可以轻松地从其他来源提取数据。有些系统查询实时数据，但有些系统必须先摄取数据，然后才能高效地查询数据。&lt;/p&gt;
&lt;h4 id=&#34;优势&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#优势&#34;&gt;#&lt;/a&gt; 优势&lt;/h4&gt;
&lt;p&gt;F1 Query，是一个 SQL 查询引擎，它的优势并不在于它专注于做好一件事，而是它的目标是覆盖企业数据处理和分析需求空间的各个角落。F1 Query 模糊了事务性、交互式和批处理工作负载之间的传统区别，支持以下功能：（i）仅影响少数记录的 OLTP 点查询，（ii）大量数据的低延迟 OLAP 查询，以及（iii）将来自不同来源的数据转换和混合到支持复杂分析和报告工作负载的新表中的大型 ETL 管道。F1 Query 还通过支持与自定义业务逻辑集成的声明性查询，大大减少了开发硬编码数据处理管道的需求。因此，F1 是一个通用的查询系统，可以支持企业数据处理和分析的绝大多数用例。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;数据中心框架&lt;/p&gt;
&lt;p&gt;F1 Query 是为数据中心而不是单个服务器或紧密耦合的集群构建的。传统的无共享数据库管理系统试图将数据的计算和处理始终保持在数据所在的位置。此外，经典范例将数据库存储子系统与查询处理层紧密耦合，通常共享内存管理、存储布局等。相比之下，F1 Query 将数据库存储与查询处理分离，因此，它可以作为数据中心中所有数据的引擎，在很大程度上消除了访问本地与远程数据时的吞吐量和延迟差异。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;可伸缩性&lt;/p&gt;
&lt;p&gt;客户端的需求差异很大，不仅在被处理的数据集的大小方面，而且在延迟和可靠性要求以及允许的资源成本方面。在 F1 Query 中，短查询在单个节点上执行，而较大的查询在低开销的分布式执行模式下执行，没有检查点和有限的可靠性保证。最大的查询以可靠的面向批处理的执行模式运行，该模式使用 MapReduce 框架。在这些模式中的每一种模式中，F1 Query 通过增加用于查询处理的计算并行性来减轻大数据大小的高延迟。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;可扩展性&lt;/p&gt;
&lt;p&gt;客户端能够使用 F1 Query 来满足任何数据处理需求，包括那些在 SQL 中不容易表达或需要访问新格式数据的需求。为了满足这一需求，F1 Query 具有高度的可扩展性。它支持用户定义函数（UDF）、用户定义聚合函数（UDA）和表值函数（TVF），以便将用本机代码编写的复杂业务逻辑集成到查询执行中。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;f1查询架构的概述&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#f1查询架构的概述&#34;&gt;#&lt;/a&gt; F1 查询架构的概述&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;/images/image-20230828202120168.png&#34; alt=&#34;image-20230828202120168&#34;&gt;&lt;/p&gt;
&lt;h5 id=&#34;架构&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#架构&#34;&gt;#&lt;/a&gt; 架构&lt;/h5&gt;
&lt;p&gt;图 1 描述了单个数据中心内的基本架构和组件之间的通信。用户通过 F1 Query 的客户端库与 F1 Query 进行交互，该库将请求发送到几个专用服务器中的一个（称为 F1 服务器）。F1 主站是数据中心内的专用节点，负责查询执行的运行时监控，并维护该数据中心的所有 F1 服务器。小型查询和事务开始在接收请求的直接 F1 服务器上执行。F1 通过在 worker 池中的 worker 上动态地提供执行线程来调度用于分布式执行的较大查询。最大的查询被安排在使用 MapReduce 框架的批处理执行模式中执行。最终结果在 F1 服务器上收集，然后返回给客户端。F1 服务器和 worker 通常是无状态的，允许客户端每次与任意 F1 服务器通信。由于 F1 服务器和工作器不存储数据，因此添加新的 F1 服务器或工作器不会触发任何数据重新分配成本。因此，数据中心的 F1 Query 部署可以通过添加更多服务器或工作器轻松地向外展。&lt;/p&gt;
&lt;h5 id=&#34;查询执行&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#查询执行&#34;&gt;#&lt;/a&gt; 查询执行&lt;/h5&gt;
&lt;p&gt;用户通过其客户端库与 F1 Query 交互。客户端的查询请求可能到达许多 F1 服务器中的一个。在到达 F1 服务器时，F1 服务器首先解析和分析 SQL 查询，然后提取查询访问的所有数据源和接收器的列表。如果任何数据源或接收器在本地数据中心中不可用，并且在更靠近数据源或接收器的其他数据中心处存在 F1 服务器，则 F1 服务器将查询连同关于可用于运行查询的最佳数据中心集的信息发送回客户端。然后，客户端将查询重新发送到目标数据中心的 F1 服务器以供执行。可以注意到，虽然存储和计算的分解以及高性能网络结构已经消除了数据中心内的许多局部性问题，但是从一组许多地理上分布的数据中心中选择接近数据的数据中心仍然对查询处理延迟具有很大的影响。&lt;/p&gt;
&lt;p&gt;在 F1 服务器上，查询执行从规划阶段开始，在规划阶段中，优化器将分析的查询抽象语法树转换为关系代数运算符的 DAG（有向无环图），然后在逻辑和物理级别上对其进行优化。然后将最终的执行计划移交给执行层。根据客户端指定的执行模式首选项，F1Query 以交互模式或批处理模式（使用 MapReduce 框架）在 F1 服务器和 worker 上执行查询，如图 2 所示。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/image-20230828161456746.png&#34; alt=&#34;image-20230828161456746&#34;&gt;&lt;/p&gt;
&lt;p&gt;对于交互式执行，查询优化器应用启发式方法在单节点集中式执行和分布式执行之间进行选择。在集中式执行中，服务器在接收查询的第一个 F1 服务器上立即分析、规划和执行查询。在分布式模式下，接收查询的第一个 F1 服务器仅充当查询协调器。该服务器调度单独的 worker 上的工作，然后这些 worker 一起执行查询。交互式执行模式为中小型查询提供了良好的性能和资源高效的执行。&lt;/p&gt;
&lt;p&gt;批处理模式为处理大量数据的长时间运行的查询（大型查询）提供了更高的可靠性。F1 服务器将在批处理模式下运行的查询的计划存储在单独的执行存储库中。批处理模式分发和调度逻辑使用 MapReduce 框架异步运行查询。在此模式下执行查询可以容忍服务器重新启动和故障。&lt;/p&gt;
&lt;h5 id=&#34;数据来源&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#数据来源&#34;&gt;#&lt;/a&gt; 数据来源&lt;/h5&gt;
&lt;p&gt;数据中心的 F1 服务器和 worker 不仅可以访问它们所在的数据中心中的数据，还可以访问任何其他 Google 数据中心中的数据。处理和分析存储层使得能够从各种来源检索数据，从 Spanner 和 Bigtable 等分布式存储系统到具有不同结构的普通文件，如逗号分隔值文本文件（CSV），面向记录的二进制格式和压缩列式文件格式，如 ColumnIO 和 Capacitor 。F1 Query 为支持它的数据源提供一致和可重复的读取，包括 Spanner 存储服务管理的数据。&lt;/p&gt;
&lt;p&gt;为了支持对异构数据源的查询，F1 Query 抽象了每种存储类型的详细信息。它使所有数据看起来就像是存储在关系表中一样，并允许连接存储在不同来源中的数据。它使用全局编录服务来维护和检索有关以不同格式和系统存储的数据源的元信息。F1 查询还允许查询通过全局编录服务不可用的源。在这种情况下，客户端必须提供一个 DEFINE TABLE 语句，该语句描述如何将底层数据源表示为关系数据库表。下面展示了一个从 CSV 格式的 Colossus 文件中检索数据的示例。F1 查询必须知道文件的位置和类型以及其中包含的列的名称和类型。注意，不同的数据源可能需要不同的信息来描述其结构，这取决于其独特的属性。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/image-20230828162323322.png&#34; alt=&#34;image-20230828162323322&#34;&gt;&lt;/p&gt;
&lt;p&gt;虽然 F1 Query 原生支持 Google 中最广泛使用的数据源，但客户端偶尔需要通过事先未知的机制访问数据。为此，F1 支持使用名为表值函数（TVF）的扩展 API 添加新的自定义数据源，在后面章节中有更详细的描述。&lt;/p&gt;
&lt;h5 id=&#34;数据下移&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#数据下移&#34;&gt;#&lt;/a&gt; 数据下移&lt;/h5&gt;
&lt;p&gt;查询的输出可以返回给客户端，但是查询也可以请求将其输出存储到外部数据接收器中。接收器可以包括各种格式的文件或以其他方式使用各种远程存储服务。与数据源一样，接收器可以是由编录服务管理的表，也可以是手动指定的目标。托管表由 CREATE TABLE 语句创建。默认情况下，它们作为存储在 Colossus 文件系统中的文件实现。手动指定的存储目标是使用 EXPORT DATA 语句指定的，使用的规范类似于用于阅读回相同数据的相应 DEFINE TABLE 规范。除了这些选项之外，查询还可以创建会话本地临时表。&lt;/p&gt;
&lt;h5 id=&#34;查询语言&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#查询语言&#34;&gt;#&lt;/a&gt; 查询语言&lt;/h5&gt;
&lt;p&gt;F1 Query 符合 SQL 2011 标准，具有支持查询嵌套结构化数据的扩展。F1 Query 支持标准 SQL 功能，包括左 / 右 / 完全外部联接、聚合、表和表达式子查询、WITH 子句和分析窗口函数。对于结构化数据，F1 Query 支持可变长度的 ARRAY 类型，以及 STRUCs，这些类型与 SQL 标准行类型很相似。F1 Query 还支持 Protocol Buffers [9]，这是一种在 Google 广泛使用的结构化数据交换格式。&lt;/p&gt;
&lt;h4 id=&#34;执行内核和交互式执行模式&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#执行内核和交互式执行模式&#34;&gt;#&lt;/a&gt; 执行内核和交互式执行模式&lt;/h4&gt;
&lt;p&gt;默认情况下，F1 Query 以称为交互式执行的同步联机模式执行查询。F1 查询支持两种类型的交互式执行模式：集中式和分布式。在规划阶段，优化器分析查询并确定是以集中模式还是分布式模式执行。在集中式模式下，当前 F1 服务器使用单个执行线程立即执行查询计划。相反，在分布式模式下，当前 F1 服务器充当查询协调器。它将工作安排在称为 F1 工作进程的其他进程上，然后这些进程一起并行执行查询。在本节中，我们将详细描述 F1 查询交互式执行。&lt;/p&gt;
&lt;h5 id=&#34;单线程执行内核&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#单线程执行内核&#34;&gt;#&lt;/a&gt; 单线程执行内核&lt;/h5&gt;
&lt;p&gt;图 3 描述了一个 SQL 查询和用于集中式模式执行的结果查询计划。在这种模式下，F1 Query 使用单线程执行内核。图 3 所示的矩形框是执行计划中的运算符。单线程执行使用基于递归拉取的模型以 8 KiB 的批量处理元组。执行运算符递归地调用基础运算符上的 GetNext（），直到叶运算符检索到批量元组。叶子通常是从数据源读取的 Scan 运算符。每个数据源都有自己的扫描运算符实现，其特征集取决于数据源的类型。一些源仅允许全表扫描，而其他源也支持基于键的索引查找。一些源代码还支持在非键字段上下推简单筛选表达式。一个单独的 ARRAY 扫描运算符根据需要从数组类型的表达式中生成行。对于可能包含协议缓冲区的输入数据，所有扫描操作符都支持立即在数据源扫描节点处进行协议缓冲区解码，从而确保执行器不会传递大的编码协议缓冲区团块，除非它们全部被需要。相反，每个扫描操作符立即提取查询所需的最小字段集。F1 Query 还支持几个高性能的列数据源，这些数据源分别存储协议缓冲区或 SQL 结构的字段，并且根本不需要任何协议缓冲区解码。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/image-20230828164255793.png&#34; alt=&#34;image-20230828164255793&#34;&gt;&lt;/p&gt;
&lt;p&gt;F1 查询支持多种连接操作符，包括查找连接（索引嵌套循环连接）、散列连接、合并连接和数组连接。hash join 操作符实现了一个多级递归混合 hash join，并将磁盘溢出到 Colossus 分布式文件系统。查找连接操作符从其左输入读取包含键的行，并使用这些键在其右输入（必须是扫描操作符）上执行索引查找。合并联接运算符合并共享相同排序顺序的两个输入。F1 Query 还有一个针对 Spanner 表的集成扫描 / 连接操作符，它对来自基础表的数据流实现合并连接。数组连接是数组扫描的相关连接，其中数组表达式引用连接的左输入，在 SQL 查询中写为数组值表达式 f（）的 T JOIN UNNEST（f（T））。&lt;/p&gt;
&lt;p&gt;除了扫描和连接之外，F1 Query 还有投影、聚合（基于排序和磁盘溢出）、排序、联合和分析窗口函数的运算符。所有执行操作符（包括扫描和联接）都内置支持，用于在其输出行上应用筛选谓词以及 LIMIT 和 OFFSET 操作。&lt;/p&gt;
&lt;h5 id=&#34;分布式执行&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#分布式执行&#34;&gt;#&lt;/a&gt; 分布式执行&lt;/h5&gt;
&lt;p&gt;当优化器检测到分布式执行计划最适合使用分区读取以高并行度扫描输入表时，它会生成这样的计划。在这种情况下，查询执行计划被分割成查询片段，如图 4 所示。每个片段在一组 F1 工作节点上调度。这些片段同时执行，同时具有流水线和浓密并行性。工作节点是多线程的，并且一些工作节点可以执行同一查询的多个独立部分。&lt;/p&gt;
&lt;p&gt;优化器采用自底向上的策略，根据查询计划中每个操作符的输入分布要求计算计划片段边界。每个单独的操作者可以具有跨工作者分发其输入数据的要求。如果存在，则通常在一些字段集合上对需求进行散列。典型的示例包括用于聚合的分组键或用于散列联接的联接键。当这个需求与输入操作符的元组分布兼容时，优化器将两个操作符都计划在同一个片段中。否则，它计划两个操作符之间的交换操作符以生成片段边界。&lt;/p&gt;
&lt;p&gt;下一步是为每个片段选择并行 worker 的数量（参见图 4）。片段以独立的并行度操作。叶运算符的表扫描中的底层数据组织确定初始并行化，并具有上限。然后，宽度计算器递归地将该信息向上传播到查询计划树。例如，一个 50 个 worker 片段和一个 100 个 worker 片段之间的散列连接将使用 100 个 worker 来执行，以适应两个输入中较大的一个。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/image-20230828165230110.png&#34; alt=&#34;image-20230828165230110&#34;&gt;&lt;/p&gt;
&lt;p&gt;以下查询说明了分布式执行：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/image-20230828165308940.png&#34; alt=&#34;image-20230828165308940&#34;&gt;&lt;/p&gt;
&lt;p&gt;此查询涉及两个表：Ads 是一个用于存储广告信息的 Spanner 表，而 Clicks 是一个存储广告点击的表，定义在 Google 的分析数据仓库 Mesa 中。此查询查找在 Chrome OS 上发生的所有广告点击，广告开始日期在 2018-05-14 之后。然后，它聚合符合条件的元组以查找每个区域的点击，并按点击次数的降序。&lt;/p&gt;
&lt;p&gt;此查询的一个可能计划如图 5 所示。在执行期间，数据流自下而上通过每个操作符，直到到达聚合和排序操作符。一千个工作进程中的每一个都扫描 Clicks 表中的数据。查询规划器将过滤器 Clicks.OS = &#39;ChromeOS’下推到 Mesa 扫描本身中，以便仅将满足过滤器的行从 Mesa 返回到 F1 worker。200 名工作人员使用过滤器 Ads 处理 Ads 表的扫描。StartDate &amp;gt; ‘2018-05- 14’。来自两次扫描的数据流入一个散列连接运算符，然后同一个 F1 工作进程对连接结果执行部分聚合。最后，F1 服务器执行完整的聚合，并将排序后的输出返回给客户端。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/image-20230828165933726.png&#34; alt=&#34;image-20230828165933726&#34;&gt;&lt;/p&gt;
&lt;h5 id=&#34;分区策略&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#分区策略&#34;&gt;#&lt;/a&gt; 分区策略&lt;/h5&gt;
&lt;p&gt;在分布式执行模式下，F1 Query 并行执行多个片段。执行和数据流可以被视为 DAG，如图 4 所示。数据通过使用交换操作符进行重新分区而跨每个片段边界移动。对于每个元组，发送方应用分区函数来确定该元组的目的地分区。每个分区号对应于目的地片段中的特定 worker。&lt;/p&gt;
&lt;p&gt;交换操作使用从每个源片段分区到所有目的地片段分区的直接远程过程调用（RPC，简称 RPC）来实现，其中每个发送方和接收方之间具有流控制。这种基于 RPC 的通信模式可以很好地扩展到每个片段数千个分区。需要更高并行度的查询通常以批处理执行模式运行。F1 Query 的交换操作符在数据中心本地运行，利用 Google 的 Jupiter 网络。Jupiter 允许由数万台主机组成的集群中的每台服务器与同一集群中的任何其他服务器进行通信，其持续带宽至少为 10 Gb/s。&lt;/p&gt;
&lt;p&gt;查询优化器将每个扫描操作符规划为查询执行计划中的叶沿着期望的 N 个工作器并行性。为了以并行化的方式执行扫描，工作必须被分布，以便每个扫描工作器产生元组的不重叠子集，并且所有工作器一起产生完整的输出。然后，查询调度程序要求扫描操作符将其自身划分为 N 个分区。作为响应，扫描操作器产生 N 个或更多个分区描述。为了实现这一点，调度器然后调度计划的副本以在 N 个工作器上运行，并向每个工作器发送先前获得的分区描述中的一个。然后，每个工作器产生由其分区描述的数据子集。在一些情况下，分区的实际数量（例如，基于文件的表的数据文件的数量）可能超过 N，在这种情况下，查询执行器随时间动态地将分区分配给可用的工作者。这种方法避免了由偏斜引起的扫描的长尾延迟。&lt;/p&gt;
&lt;p&gt;一些操作符在与其输入之一相同的计划片段中执行。例如，查找连接在与其左输入相同的片段中执行，仅处理由该输入的相同分区产生的元组的查找。相反，如图 5 所示，散列连接的执行通常需要多个片段，每个片段具有多个分区。查询优化器将每个输入扫描操作符（或其他子计划）计划在单独的片段中，除非输入操作符的数据分布已经与散列联接键兼容。这些源片段（图 5 中的 SCAN 点击和 SCAN 广告）中的每一个将其数据发送到包含散列连接操作符的相同目的地片段（图 5 中的右侧所示）。两个输入片段都使用基于联接键的散列的相同分区函数来发送它们的数据。这确保了具有相同联接键的所有行最终在相同的目标分区中，从而允许每个散列联接分区针对键空间的特定子集执行联接。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/image-20230828170121934.png&#34; alt=&#34;image-20230828170121934&#34;&gt;&lt;/p&gt;
&lt;p&gt;聚合操作符通常还需要重新分区。对于具有分组键的聚合，查询计划通过分组键的散列重新划分输入元组，并且使用聚合操作符将这些元组发送到目的地片段。对于没有分组键的聚合，所有元组都被发送到单个目的地。图 5 包含了一个没有分组键的聚合示例。如图所示，通过在执行尽力而为存储器中部分聚合的交换操作符之前添加第二聚合操作符来优化聚合。这减少了传输的数据量，并且对于具有分组密钥的聚合，其减轻了在目的地片段处的完全聚合期间热分组密钥的不利影响。&lt;/p&gt;
&lt;p&gt;如前所述，F1 中的执行计划是 DAG 形状的，可能具有多个根。对于数据流 DAG 中的分叉，计划片段重新划分为多个目的片段，每个目的片段具有不同的划分功能。这些 DAG fork 为 SQL WITH 子句和由优化器消除重复数据的相同子计划实现 run-once 语义。DAG 分叉还用于其他复杂的计划，例如，分析函数和 DISTINCT 输入上的多个聚合。DAG 分叉对消费者片段中不同的数据消费速度敏感，以及如果多个分支在稍后再次合并时阻塞，则对分布式死锁敏感。示例包括来自 DAG 分叉的自散列连接，其尝试在构建阶段初始消耗所有元组。实现 DAG 分叉的 Exchange 操作符通过在内存中缓冲数据，然后在所有消费者被阻止时将数据溢出到 Colossus 来解决这些问题。&lt;/p&gt;
&lt;h5 id=&#34;性能注意事项&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#性能注意事项&#34;&gt;#&lt;/a&gt; 性能注意事项&lt;/h5&gt;
&lt;p&gt;F1 Query 中查询性能问题的主要原因包括不对称和次优的数据源访问模式。散列连接可能对来自两个输入的热键特别敏感。加载到哈希表（构建输入）的输入中的热键可能导致溢出，因为一个 worker 将需要比其他 worker 存储更多的元组。其他输入（探测输入）中的热键可能会产生 CPU 或网络瓶颈。对于一个输入足够小以适合内存的情况，F1 Query 支持广播散列连接，该散列连接读取小的构建输入并将所有结果元组的副本广播到所有散列连接工作器。然后，每个 worker 构建哈希表的相同副本。此广播散列联接对偏斜不敏感，尽管它对意外的大生成输入敏感。&lt;/p&gt;
&lt;p&gt;在查询执行期间，所有查找联接都使用索引键检索远程数据。由于底层数据源的延迟分布中的长尾，简单的逐键实现将导致非常缓慢的执行，底层数据源通常是分布式系统本身。因此，F1Query 的查找联接运算符使用大量外部行。如果在同一批中多次请求相同的查找关键字，则这种大批量允许重复数据删除。扫描运算符实现还可以使用更大的批来优化数据检索。例如，分区数据源使用较大的批处理来查找必须从同一远程数据源分区读取的多个键，将它们合并到单个有效的数据源访问中。如果一个批处理所需的远程请求数超过了对数据源的最大并行请求数，则来自底层存储系统的尾部延迟将被隐藏，因为请求可能会无序完成，并且运行时间较长的请求不会阻止其他较短请求的进程。&lt;/p&gt;
&lt;p&gt;将查找连接直接放置在其左输入上时，也会经常出现偏斜和不希望的访问模式。取决于输入数据分布，数据源访问模式可以是任意的，并且可能根本不存在去重，因为对相同键的请求跨片段分区散布。堆叠多个查找连接会导致在序列期间某些键与其他键连接的行不成比例的情况下不对称。为了消除这些影响，查询优化器可以使用几个分区函数之一来重新分区左输入。分区函数的选择决定了查找联接的数据源访问模式，这对性能有很大的影响。例如，散列分区确保每个键仅源自一个节点，从而实现查找的重复删除，但对于数据源，来自每个节点的访问模式将仍然看起来像随机访问。范围分区的数据源（如 Spanner 和 Bigtable）在查找期间从键空间局部性中受益匪浅：当密钥集中在密钥空间的小范围内时，它们可能驻留在同一个数据源分区上，并且可以作为单个远程数据源访问的一部分返回到 F1 服务器。利用这一点的一种方法是使用显式的静态范围分区来为每个目的片段的分区分配固定的键范围，但这种策略有时对偏斜敏感。一种更好的基于范围的策略称为动态范围重新划分，它根据本地分布信息计算每个发送器中的单独范围划分函数。这是基于这样的原理，即在一个输入计划片段分区处观察到的分布通常非常接近整体数据分布。在许多情况下，这会导致查找模式在键空间中具有比其他分区策略高得多的局部性。此外，它在执行查找的工作者上产生完全均匀的工作负载分布。我们已经观察到，这种策略优于静态确定的理想范围划分的基础上，在查找数据源中的关键分布，特别是在左输入偏斜的情况下，仅使用关键空间的子集。动态范围重新划分还通过将它们散布在更多的目的地节点上来自适应地响应输入数据流中的临时热键，而不是创建临时热点的静态范围划分。&lt;/p&gt;
&lt;p&gt;F1 查询操作符通常在内存中执行，而不检查指向磁盘和尽可能多的流数据。这避免了将中间结果保存到磁盘的成本，并使查询尽可能快地运行以使用输入数据。当与数据源中的积极缓存相结合时，该策略使复杂的分布式查询能够在数十或数百毫秒内运行完成。内存中执行对 F1 服务器和工作进程故障很敏感。客户端库通过透明地重试失败的查询来解决这个问题。实际上，运行长达一小时的查询足够可靠，但运行时间较长的查询可能会反复失败。在这些情况下，F1 查询的批处理执行模式成为上级选择，如下一节所述。&lt;/p&gt;
&lt;h4 id=&#34;基于mapreduce的批处理执行模式&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#基于mapreduce的批处理执行模式&#34;&gt;#&lt;/a&gt; 基于 MapReduce 的批处理执行模式&lt;/h4&gt;
&lt;p&gt;F1 Query 支持交互式分析以及对长时间运行的大量数据进行大规模转换。这些大规模转换通常处理 ETL（提取 - 转换 - 加载）工作流。Google 的许多 ETL 处理管道历史上都是使用 MapReduce 或 FlumeJava [19] 开发的，主要使用自定义代码进行数据转换。尽管定制的 ETL 处理管道是有效的，但它们会产生高的开发和维护成本。此外，自定义管道不太适合 SQL 查询优化器可以执行的许多有用的优化，例如过滤器下推或属性修剪。例如，当仅需要少量字段时，手写流水线可能不必要地在级之间传递大的数据结构，因为优化这一点的额外努力是禁止的并且增加了太多的维护开销。SQL 的声明性本质使得这种手动优化变得不必要，因此最好对这种管道使用 SQL。&lt;/p&gt;
&lt;p&gt;交互模式的内存中处理模型不适合处理在长时间运行的查询期间可能发生的 worker 故障。为了解决这个问题，F1 Query 添加了一个新的执行模式，称为批处理模式。批处理模式允许长时间运行的查询即使在 F1 服务器或 worker 出现故障时也能可靠地执行。此外，它还处理客户端故障，允许客户端提交查询进行异步处理，然后断开连接。&lt;/p&gt;
&lt;p&gt;构建在图 2 所示的 F1Query 框架之上，F1Query 批处理模式与两种交互模式共享相同的查询规范、查询优化和执行计划生成组件。模式之间的关键区别发生在执行调度期间。在交互模式下，查询同步执行。F1 服务器监视整个查询的进度，直到完成为止。相反，对于批处理模式，F1 服务器异步调度查询以供执行。中央注册表记录查询的进度。这种架构带来了以下挑战：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在批处理模式下，查询计划执行运算符必须以不同的方式进行通信，因为查询计划片段是异步执行的。在分布式交互模式中，所有片段同时处于活动状态，并使用远程过程调用进行通信。这在批处理模式下是不可行的，因为查询的不同片段在不同的时间执行。&lt;/li&gt;
&lt;li&gt;由于批处理查询是长时间运行的，因此我们必须考虑执行过程中可能出现的瞬时故障，包括机器重新启动。这需要一个容错机制来持久化查询的中间状态并保证向前进展。&lt;/li&gt;
&lt;li&gt;需要一个新的更高级别的服务框架来跟踪不同执行阶段的数千个批处理查询的执行情况，以确保最终完成所有查询。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;批处理执行框架&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#批处理执行框架&#34;&gt;#&lt;/a&gt; 批处理执行框架&lt;/h5&gt;
&lt;p&gt;批处理模式使用 MapReduce（MR）框架作为其执行平台。在抽象级别，查询计划中的每个计划片段（参见图 4）都可以映射到 MapReduce 阶段。处理管道中的每个阶段都将其输出存储到 Colossus 文件系统。该通信模型支持不同 MapReduce 阶段的异步执行，同时提供必要的容错性。当整个 MapReduce 阶段失败时，它们可以重新启动，因为它们的输入存储在 Colossus 上。由于 MapReduce 框架提供的固有容错性，MapReduce 阶段期间的故障是可以容忍的。&lt;/p&gt;
&lt;p&gt;在其最简化的形式中，可以将 F1 查询执行计划中的计划片段映射到 MapReduce 阶段。然而，F1 Query 以类似于 FlumeJava [19] 的 MSCR 融合优化的方式进行优化。在该优化中，叶节点被抽象为映射操作，而内部节点被抽象为 reduce 操作。然而，这种映射导致了 map-reduce-reduce 类型的处理，这并不完全对应于 MapReduce 框架。F1 查询批处理模式通过插入一个特殊的映射运算符（身份函数）来解决这个问题。通过这种方式，map-reduce-reduce 处理可以分为两个 MapReduce 阶段：mapreduce 和&lt;identity&gt;map-reduce。图 6 展示了从常规物理执行计划到批处理模式 MapReduce 计划的映射。如图所示，左侧的查询计划仅映射到三个 MapReduce 阶段，而不是默认映射，这将导致六个 MapReduce 阶段。我们尚未实现的进一步改进是使用像 Cloud Dataflow 这样的框架，它支持原生的 map-reduce-reduce 类型的处理。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/image-20230829145402362.png&#34; alt=&#34;image-20230829145402362&#34;&gt;&lt;/p&gt;
&lt;p&gt;在片段之间，F1 Query 的分布式交互模式通过 RPC 在网络上发送数据。相反，批处理模式将数据具体化到暂存文件中，将其读回，并输入到下一个计划片段中。这是通过用于计划片段执行器的公共 I/O 接口来实现的，其由两种模式实现。此外，在分布式交互模式中，查询执行计划中的每个节点都是同时活动的，从而允许通过流水线实现并行性。相比之下，在批处理模式中没有流水线：MapReduce 阶段只有在所有输入完全可用时才开始。批处理模式确实支持丛并行，即，独立的 MR 级可以并行执行。&lt;/p&gt;
&lt;p&gt;请注意，F1 Query 批处理模式以非常大的规模运行，并且为查询计划中的每个交换操作符带来了大量的数据具体化开销。因此，在可能的情况下减少计划中的交换运营商的数量是有益的，特别是当处理非常大的表时。避免交换运算符的一种方法是用查找连接替换散列连接。对于较小的输入对于广播散列连接来说太大，或者存在显著偏斜的连接，批处理模式可以将较小的输入具体化到称为排序字符串表（SSTables）的基于磁盘的查找表中。然后，它在与较大输入相同的片段中使用查找连接操作符来查找这些表，从而避免了对较大输入进行代价高昂的重新分区。查找使用分布式缓存层来减少磁盘 I/O。&lt;/p&gt;
&lt;h5 id=&#34;批处理服务框架&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#批处理服务框架&#34;&gt;#&lt;/a&gt; 批处理服务框架&lt;/h5&gt;
&lt;p&gt;F1Query 批处理模式服务框架协调所有批处理模式查询的执行。它负责注册要执行的传入查询，跨不同数据中心分发查询，以及调度和监视相关的 MapReduce 处理。图 7 显示了服务框架体系结构。当 F1 客户端发出以批处理模式运行的查询时，其中一个 F1 服务器将接收该查询。然后，它生成一个执行计划，并在 QueryRegistry 中注册查询，QueryRegistry 是一个全局分布式 Spanner 数据库，用于跟踪所有批处理模式查询的元数据。然后，服务的查询分发服务器组件将查询分配给数据中心，并根据负载平衡考虑因素和执行所需数据源的可用性选择数据中心。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/image-20230829145611972.png&#34; alt=&#34;image-20230829145611972&#34;&gt;&lt;/p&gt;
&lt;p&gt;然后，由在目标数据中心中运行的框架组件拾取查询。每个数据中心都有一个查询调度程序，它定期从查询注册表中检索新分配的查询。调度器创建查询执行任务的依赖关系图，并且当任务准备好执行并且资源可用时，调度器将任务发送到查询执行器。然后，查询执行器使用 MapReduce 的 worker 池来执行任务。&lt;/p&gt;
&lt;p&gt;服务框架是健壮的，在每个级别都具有弹性功能。所有组件都具有冗余性，包括复制和主选举的全局查询分发器，以及每个数据中心具有多个冗余实例的查询调度器。查询的所有执行状态都在查询注册表中维护，这允许所有组件有效地无状态和可替换。在数据中心内，失败的 MapReduce 阶段会重试多次。如果查询完全停止，例如由于数据中心中断，分发服务器将查询重新分配给备用数据中心，备用数据中心将从头开始执行。&lt;/p&gt;
&lt;h4 id=&#34;f1查询优化器&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#f1查询优化器&#34;&gt;#&lt;/a&gt; F1 查询优化器&lt;/h4&gt;
&lt;p&gt;查询优化器的开发非常复杂。F1Query 通过重用相同的逻辑来规划所有查询，而不管执行模式如何，从而缓解了这种情况。尽管交互式和批处理执行模式使用了明显不同的执行框架，但两者都使用相同的计划和相同的执行内核。通过这种方式，F1 查询优化器中实现的所有查询规划特性都自动适用于这两种执行模式。&lt;/p&gt;
&lt;p&gt;F1 查询优化器的高级结构如图 8 所示，它从 Cascades [35] 风格的优化中汲取了灵感。该基础设施与 Spark 的 Catalyst 计划者共享一些设计原则和术语 [11]，因为 F1 Query 和 Catalyst 团队成员之间的早期对话。第一步是调用 Google 的 SQL 解析器来解析和分析原始输入 SQL，并生成解析的抽象语法树（AST）。然后，优化器将每个这样的 AST 转换为关系代数计划。在关系代数上执行多个规则，直到达到定点条件，以产生启发式确定的最优关系代数计划。然后，优化器将最终的代数计划转换为包括所有数据源访问路径和执行算法的物理计划。优化器通过将物理计划转换为适合查询执行的最终数据结构来完成其工作，并将它们传递给查询协调器以供执行。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/image-20230829145814850.png&#34; alt=&#34;image-20230829145814850&#34;&gt;&lt;/p&gt;
&lt;p&gt;应该注意的是，F1 查询优化器主要基于启发式规则。当存在时，它在一定程度上使用统计数据属性。然而，由于数据源的多样性，典型的 F1 查询仅使用来自某些源的统计信息，这取决于预先收集什么是可行的。&lt;/p&gt;
&lt;h5 id=&#34;优化器基础架构&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#优化器基础架构&#34;&gt;#&lt;/a&gt; 优化器基础架构&lt;/h5&gt;
&lt;p&gt;优化器的所有阶段都基于一个公共的基础结构层，用于表示各种计划树以及对它们进行操作的转换。所有计划树结构都是不可变的：转换阶段必须构建新的运算符来更改查询计划。此属性支持探索性规划和子树重用。为了减轻来自数据结构的多次构造和析构的负面性能影响，优化器在存储器竞技场中构造所有数据结构，并在查询的关键路径之外析构它。&lt;/p&gt;
&lt;p&gt;优化器为表达式、逻辑计划和物理计划提供了单独的树层次结构。数百种树节点类型的样板代码仅由约 3K 行 Python 代码以及约 5K 行 Jinja2 模板生成，导致约 600K 行 C++。所生成的代码使得域特定语言（DSL）能够用于查询规划，并且包含用于计算每个树节点的散列的方法，以执行树相等比较，以及适合于将树存储在标准集合中并在测试框架中表示它们的其他助手。代码生成的使用为 F1 Query 工程师节省了大量时间，减少了开发过程中的错误，并使新功能能够在树层次结构中有效地推出。&lt;/p&gt;
&lt;p&gt;所有关系代数规则和计划转换阶段使用 C&lt;ins&gt; 嵌入式 DSL 检查和操作树，以进行树模式匹配和构建。由于代码生成和 C&lt;/ins&gt; 模板，树模式表达式的性能与优化的手写代码一样好。同时，它们比手工编写的代码更简洁，并且更清楚地表达了每次重写的意图。&lt;/p&gt;
&lt;h5 id=&#34;逻辑查询计划优化&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#逻辑查询计划优化&#34;&gt;#&lt;/a&gt; 逻辑查询计划优化&lt;/h5&gt;
&lt;p&gt;当 SQL 查询分析器接收到原始查询文本时，它会生成一个解析的抽象语法树（AST）。然后，F1 查询优化器将此 AST 转换为关系代数树。然后应用逻辑重写规则来应用启发式更新以改进查询计划。规则被组织成批，每个批运行一次或直到达到固定点。应用的规则包括过滤器下推、常量折叠、属性修剪、约束传播、外部连接缩小、排序消除、公共子计划重复数据删除和实体化视图重写。&lt;/p&gt;
&lt;p&gt;F1 查询中的数据源可能包括关系表列中的结构化协议缓冲区数据，并且所有规则都具有协议缓冲区的一流知识。例如，核心属性修剪规则将各个协议缓冲区字段的提取操作表达式递归地向下推到查询计划中尽可能远。如果这样的提取一直行进到查询计划的叶子，则通常可以将它们集成到扫描操作中，以减少从磁盘读取或通过网络传输的字节数。&lt;/p&gt;
&lt;h5 id=&#34;物理查询计划构造&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#物理查询计划构造&#34;&gt;#&lt;/a&gt; 物理查询计划构造&lt;/h5&gt;
&lt;p&gt;基于关系代数计划，优化器然后创建一个物理计划树，它表示实际的执行算法和数据源访问路径。物理计划构造逻辑封装在称为策略的模块中。每种策略都试图与关系代数运算符的一个特定组合进行匹配。然后，该策略产生物理运算符来实现匹配的逻辑运算符。例如，一种策略仅处理查找连接，检测具有适当索引的表的逻辑连接，然后在它们之间生成物理查找连接运算符。每个结果的物理操作符表示为跟踪多个数据属性的类，包括分布、排序、唯一性、估计基数和波动性（以及其他）。优化器使用这些属性来确定何时插入交换运算符，以便将输入元组重新分区为下一个运算符所需的新数据分布。优化器还使用物理计划属性来决定是以集中模式还是分布式模式运行查询。当任何扫描被认为对于中央查询来说太昂贵时，例如因为它是全表扫描，所以整个查询被规划为分布式查询。&lt;/p&gt;
&lt;h5 id=&#34;执行计划片段生成器&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#执行计划片段生成器&#34;&gt;#&lt;/a&gt; 执行计划片段生成器&lt;/h5&gt;
&lt;p&gt;查询优化器的最后阶段将物理查询计划转换为一系列适合直接执行的计划片段。该执行计划片段生成器将物理计划树节点转换成在每个交换操作符处具有片段边界的对应执行操作符。生成器还负责计算每个片段的最终并行度，从包含分布式表扫描的叶片段开始，并向上传播到查询计划的根。&lt;/p&gt;
&lt;h4 id=&#34;f1中的各种扩展性选项&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#f1中的各种扩展性选项&#34;&gt;#&lt;/a&gt; F1 中的各种扩展性选项&lt;/h4&gt;
&lt;p&gt;F1 查询可通过多种方式进行扩展：它支持自定义数据源以及用户定义标量函数（UDF）、聚合函数（UDA）和表值函数（TVF）。用户定义的函数可以使用任何类型的数据作为输入和输出，包括协议缓冲区。客户端可以用 SQL 语法表达用户定义的逻辑，为他们提供了一种从查询中抽象出公共概念的简单方法，并使它们更具可读性和可维护性。他们也可以使用 Lua 脚本来定义额外的功能，用于特定的查询和分析。对于 C++ 和 Java 等编译和托管语言，F1 Query 集成了称为 UDF 服务器的专用辅助进程，以帮助客户在 SQL 查询和其他系统之间重用公共业务逻辑。&lt;/p&gt;
&lt;p&gt;UDF 服务器是由 F1 Query 客户端单独拥有和部署的 RPC 服务。它们通常用 C++、Java 或 Go 语言编写，并在与调用它们的 F1 服务器和 worker 相同的数据中心中执行。每个客户端都保持对自己的 UDF 服务器发布周期和资源配置的完全控制。UDF 服务器公开了一个通用的 RPC 接口，该接口使 F1 服务器能够找到它们导出的函数的详细信息，并实际执行这些函数。要使用 UDF 服务器提供的扩展，F1 Query 客户端必须在它发送到 F1 服务器的查询 RPC 中提供 UDF 服务器池的地址。或者，F1 数据库的所有者可以配置默认 UDF 服务器，使其可用于在该数据库上下文中运行的所有查询。即使 F1 在查询执行期间将与 UDF 服务器通信，它们仍然是独立的进程，并将核心 F1 系统与自定义函数中的故障隔离开来。&lt;/p&gt;
&lt;p&gt;SQL 和 Lua 脚本化函数不使用 UDF 服务器，也没有一个用于它们定义的中央存储库。相反，客户端必须始终将其定义作为 RPC 的一部分提供给 F1 Query。客户端工具（如 F1 Query 命令行界面）从配置文件和其他显式加载的源以及立即命令中收集函数定义。它们随后将所有相关函数定义作为每个 RPC 的一部分传递给 F1 Query。F1 Query 确实提供了一种将多个 SQLUDF、UDA 和 TVF 分组到模块中的机制。客户团队使用模块来构建他们的定制业务逻辑，提高可维护性并促进重用。模块通过发送到 F1 Query 的查询 RPC 以与单个 UDF 相同的方式呈现给 F1 Query。&lt;/p&gt;
&lt;h5 id=&#34;标量函数&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#标量函数&#34;&gt;#&lt;/a&gt; 标量函数&lt;/h5&gt;
&lt;p&gt;F1 查询支持用 SQL、Lua 编写的标量 UDF，并通过 UDF 服务器编译代码。SQLUDF 允许用户将复杂表达式封装为可重用的库。它们在查询中使用的地方就地展开。对于像 Lua 这样的脚本语言，查询执行器维护一个沙盒解释器来在运行时评估脚本函数。例如，下面所示的 Lua UDF 将编码为字符串的日期值转换为表示相应 Unix 时间的无符号整数：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/image-20230829150332868.png&#34; alt=&#34;image-20230829150332868&#34;&gt;&lt;/p&gt;
&lt;p&gt;由 UDF 服务器导出的函数只能在投影执行运算符中计算。在解析每个查询时，系统为每个 UDF 生成一个函数表达式。然后优化器将所有此类表达式移动到投影中。在执行过程中，投影运算符缓冲输入行并计算其关联的 UDF 参数值，最多限制大小。然后，工作者将 RPC 分派到关联的 UDF 服务器。UDF 服务器延迟是通过管道化多个 RPC 来隐藏的。这允许相当高的延迟 UDF 实现，而不会影响查询延迟。&lt;/p&gt;
&lt;h5 id=&#34;聚合函数&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#聚合函数&#34;&gt;#&lt;/a&gt; 聚合函数&lt;/h5&gt;
&lt;p&gt;F1 Query 还支持用户定义的聚合函数，该函数将一个组中的多个输入行组合成一个结果。与标量函数一样，用户可以在 SQL 中定义 UDA，查询优化器在每个调用站点扩展定义。对于编译和托管语言，系统还支持在 UDF 服务器中托管 UDA。AUDF 基于服务器的 UDA 定义必须实现典型的 UDA 处理操作 Initialize、Accumulate 和 Finalize 。此外，它必须实现 Reaccumulate 操作，该操作用于联合收割机来自部分聚合的多个聚合缓冲区（参见图 5）。&lt;/p&gt;
&lt;p&gt;在执行期间，聚合运算符处理输入行，并在内存中缓冲每个 UDA 聚合值的聚合输入。当哈希表中所有这些缓冲输入的内存使用总和超过一定大小时，执行器将每个组的现有聚合值和新输入发送到 UDF 服务器。然后，UDF 服务器调用适当的 UDA 操作，为每个组生成一个新的聚合值。UDF 服务器是无状态的，允许每个 F1 服务器并行地将请求分发给许多 UDF 服务器进程。&lt;/p&gt;
&lt;h5 id=&#34;表值函数&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#表值函数&#34;&gt;#&lt;/a&gt; 表值函数&lt;/h5&gt;
&lt;p&gt;最后，F1Query 公开了表值函数（TVF），这是一个客户端构建自己的用户定义数据库执行操作符的框架。表值函数有多种用途，可帮助扩展 F1 查询的功能。突出的例子包括在 SQL 查询执行期间集成机器学习步骤，如模型训练，这允许用户使用数据，然后在一个步骤中运行高级预测。整个公司的开发团队还可以根据需要添加新的 TVF 数据源，而无需与核心 F1 Query 开发人员进行交互或重新启动正在运行的数据库服务器。&lt;/p&gt;
&lt;p&gt;TVF 可以接受整个表以及常量标量值作为输入，并使用这些输入返回一个新表作为输出。查询可以通过在 FROM 子句中调用 TVF、传入标量参数、数据库中的表或表子查询来调用 TVF。例如，这调用一个带有标量参数的 TVF 和一个数据库表来计算过去 3 天的广告点击活动：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/image-20230829150520606.png&#34; alt=&#34;image-20230829150520606&#34;&gt;&lt;/p&gt;
&lt;p&gt;与 UDF 和 UDA 一样，可以使用 SQL 定义 TVF。这样的 TVF 类似于参数化视图，其中参数可以是整个表。在查询优化之前，它们被扩展到查询计划中，这样优化器就可以完全优化 TVF。上面调用的 UDF 可以使用 SQL 定义如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/image-20230829150548557.png&#34; alt=&#34;image-20230829150548557&#34;&gt;&lt;/p&gt;
&lt;p&gt;请注意，此示例使用 ANY TABLE 指定函数可以接受任何表作为参数。在这种情况下，TVF 在分析时基于每个查询的实际输入表动态地计算输出模式，在此之后，输出模式在该查询的执行期间保持固定。还可以指定输入表必须具有特定的模式，在这种情况下，F1Query 在查询分析时强制执行此不变量。&lt;/p&gt;
&lt;p&gt;可以使用 UDF 服务器定义更复杂的 TVF。UDF 服务器使用函数签名公开 TVF 定义。该签名可以包括类似于 SQL TVF 示例中的通用参数。TVF 定义还提供了一个函数来计算特定调用的输出表模式。有趣的是，这个输出模式可能不仅依赖于输入表的列类型，还依赖于标量常量参数的值。因此，即使签名不包含通用参数，TVF 也使用该函数来计算输出模式。TVF 定义还公开了优化器的执行属性，例如输入表上的 TVF 是否可以通过按某些键对输入表进行分区，然后分别调用每个分区上的 TVF 实现来并行化。&lt;/p&gt;
&lt;p&gt;查询优化器选择两个运算符之一来评估网络上远程托管的 TVF。第一个运算符特别适用于 TVF 不包含输入表参数的情况。在这种情况下，它表示远程数据源，并且它的计划和执行与其他数据源一样，包括对分区扫描和查找联接的支持。带有输入表参数的函数由专门的 TVF 执行运算符处理。对于这两种类型的 TVF，优化器可以将过滤器、限制和聚合步骤下推到 TVF 本身中，这可以使用它们来减少工作。&lt;/p&gt;
&lt;p&gt;用于远程 TVF 评估的 RPC 协议使用持久双向流网络连接将输入行发送到 UDF 服务器并接收返回的输出行，如图 9 所示。对于远程数据源，优化器还向 UDF 服务器发送 RPC 调用，以检索 TVF 的分区描述，以便多个工作进程可以并行扫描数据源。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/image-20230829150944947.png&#34; alt=&#34;image-20230829150944947&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;f1的结构化数据处理&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#f1的结构化数据处理&#34;&gt;#&lt;/a&gt; F1 的结构化数据处理&lt;/h4&gt;
&lt;h5 id=&#34;强大性能&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#强大性能&#34;&gt;#&lt;/a&gt; 强大性能&lt;/h5&gt;
&lt;p&gt;F1 Query 将性能健壮性视为数据库查询处理中的一个关键问题，也是影响用户体验的重要第三个维度，超越效率和可伸缩性。健壮性要求性能在出现意外的输入大小、意外的选择性和其他因素时适度下降。在没有适度降级的情况下，用户可能会看到性能悬崖，即，算法或计划的成本函数中的不连续性。例如，一旦整个输入开始溢出到临时文件中，从内存快速排序到外部合并排序的转换可以将端到端排序运行时间增加两倍或更多。图 10 显示了一个实例，该实例显示了 F1 查询排序操作的性能不连续性，其中有一个悬崖（之前测量）和删除悬崖（之后测量）。悬崖会产生几个问题，包括不可预测的性能和用户体验差；优化器选择变得容易出错，因为小的基数估计误差可能被放大成大的成本计算误差；并且在并行查询执行中，计算节点之间的小负载不平衡可能转变为经过的运行时间中的大差异。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/image-20230829150824633.png&#34; alt=&#34;image-20230829150824633&#34;&gt;&lt;/p&gt;
&lt;p&gt;F1 Query 采用强大的算法来防止性能悬崖。其主要思想是，代替在优化时间或在执行时间使用二进制开关，执行操作符在操作模式之间递增地转换。例如，它的排序操作符只从内存中的工作区溢出所需的数据，以便为内存中的其他输入腾出空间。排序的另一个例子发生在过渡到多个合并步骤期间，其中一个额外的输入字节可以迫使所有输入记录经历两个合并步骤，而不是只有一个 [36]。F1Query 从其排序和聚合的实现中消除了这两个悬崖。悬崖避免或移除的成功示例包括 SmoothScan [16] 和散列连接中的动态降级 [52]。如果单行 “太多” 将停止执行并重新启动编译时优化器，则动态重新优化将引入巨大的悬崖。&lt;/p&gt;
&lt;h5 id=&#34;google协议缓冲区中的嵌套数据&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#google协议缓冲区中的嵌套数据&#34;&gt;#&lt;/a&gt; Google 协议缓冲区中的嵌套数据&lt;/h5&gt;
&lt;p&gt;在 Google 内部，Protocol Buffers [9] 作为一种数据交换和存储格式无处不在。协议缓冲区是一种结构化的数据格式，其记录类型称为消息，并支持数组值或重复字段。协议缓冲区既有人类可读的文本格式，又有紧凑、高效的二进制表示。它们是 F1 查询数据模型中的一级数据类型，并且其 SQL 方言具有用于查询和操作单个消息的扩展，例如字段用于字段访问，以及 NEW Point（3AS x，5AS y）用于创建新消息。F1 查询还支持相关子查询表达式和重复字段上的连接&lt;/p&gt;
&lt;p&gt;查询协议缓冲区提出了许多与 XML [18] 和 JSON [21] 等半结构化数据格式相同的挑战，对此已有大量的研究。然而，存在一些关键的差异。在 JSON 完全动态类型化并且通常以人类可读格式存储的情况下，协议缓冲区是静态类型化的并且通常以紧凑的二进制格式存储，从而实现更有效的解码。protocol buffers 的二进制编码有点类似于 MongoDB [2] 中使用的 JSON 对象的二进制编码，但它更有效，因为字段是静态类型的，并且由整数而不是字符串标识。此外，一些数据源垂直地将消息分解成列格式 [51]，其方式类似于 XML 数据库中文档的垂直分解 [29]。&lt;/p&gt;
&lt;p&gt;查询中引用的所有 protos 的确切结构和类型在查询规划时是已知的，优化器会从数据源扫描中删除所有未使用的字段。在列式数据源中，这减少了 I/O 并实现了对过滤器的高效逐列评估。对于使用行二进制格式的面向记录的数据源，F1 Query 使用高效的流解码器，该解码器对编码数据进行单次传递，并仅提取必要的字段，跳过不相关的数据。这仅通过每个协议缓冲区类型的固定定义以及快速识别和跳过的整数字段标识符来实现。&lt;/p&gt;
&lt;h4 id=&#34;总结&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#总结&#34;&gt;#&lt;/a&gt; 总结&lt;/h4&gt;
&lt;p&gt;在本文中，我们已经证明了构建一个查询处理系统是可能的，该系统涵盖了存储在任何数据源中的数据的大量数据处理和分析用例。通过在单个系统中组合对所有这些用例的支持，F1 Query 实现了显着的协同效益，而不是存在许多单独的系统用于不同的用例。对于查询解析、分析和优化等单独系统中常见的功能，没有重复的开发工作，确保有利于一个用例的改进自动有利于其他用例。最重要的是，拥有一个单一的系统为客户提供了一个一站式的数据查询需求，并消除了当客户遇到更专业的系统的支持用例的边界时发生的不连续性或 “悬崖”。我们相信，正是 F1 Query 的广泛适用性奠定了该产品在 Google 内部建立的庞大用户群的基础。&lt;/p&gt;
&lt;p&gt;F1 Query 继续进行积极的开发，以解决新的用例，并缩小与专用系统的性能差距。例如，F1 查询还不能与向量化的列式执行引擎（例如，Vectorwise [63]），因为其面向行的执行内核。向向量化执行内核的过渡是未来的工作。F1 Query 也不支持查询引擎的本地格式的数据的本地缓存，例如在无共享架构中自然会发现，因为所有数据源都是分散的和远程的。目前，F1 Query 依赖于数据源中的现有缓存或远程缓存层，如 TableCache 。为了支持内存或近内存分析，例如 PowerDrill [39] 提供的分析，F1 Query 需要支持单个工作者的本地缓存和本地感知工作调度，将工作定向到数据可能被缓存的服务器。使用远程数据源也使得收集用于查询优化的统计信息变得更加困难，但我们正在努力使它们可用，以便 F1 Query 可以使用基于成本的优化规则。虽然 F1 Query 对扩展有很好的支持，但我们正在研究改进 F1 扩展方式的技术，例如，通过仅在几台服务器上运行中等规模的分布式查询，从而降低 Exchange 操作的成本和延迟。&lt;/p&gt;
 ]]></description>
        </item>
    </channel>
</rss>
